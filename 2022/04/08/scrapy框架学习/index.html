<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"syst1m.top","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.12.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="创建项目1scrapy startproject tutorial  创建任务1scrapy genspider first www.baidu.com  会生成一个first文件">
<meta property="og:type" content="article">
<meta property="og:title" content="scrapy框架学习">
<meta property="og:url" content="https://syst1m.top/2022/04/08/scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Syst1m">
<meta property="og:description" content="创建项目1scrapy startproject tutorial  创建任务1scrapy genspider first www.baidu.com  会生成一个first文件">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/chencicici/images/main/202205151340834.png?token=ARYCSAUIENI4WQ2X26Z4EQ3CQCJIM">
<meta property="og:image" content="https://raw.githubusercontent.com/chencicici/images/main/202205151341626.png?token=ARYCSAWOAVW6ADT6LYL7NY3CQCJMU">
<meta property="og:image" content="https://raw.githubusercontent.com/chencicici/images/main/202205151342799.png?token=ARYCSAQF2H5ORVGMPC5ZA6TCQCJQG">
<meta property="og:image" content="https://raw.githubusercontent.com/chencicici/images/main/202205151342947.png?token=ARYCSATQHIG5XPI3W7VHBBTCQCJR2">
<meta property="article:published_time" content="2022-04-08T10:57:11.000Z">
<meta property="article:modified_time" content="2022-05-15T05:43:14.000Z">
<meta property="article:author" content="Syst1m">
<meta property="article:tag" content="教程">
<meta property="article:tag" content="爬虫">
<meta property="article:tag" content="scrapy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/chencicici/images/main/202205151340834.png?token=ARYCSAUIENI4WQ2X26Z4EQ3CQCJIM">


<link rel="canonical" href="https://syst1m.top/2022/04/08/scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://syst1m.top/2022/04/08/scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/","path":"2022/04/08/scrapy框架学习/","title":"scrapy框架学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>scrapy框架学习 | Syst1m</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Syst1m" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Syst1m</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="nav-number">1.</span> <span class="nav-text">创建项目</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E4%BB%BB%E5%8A%A1"><span class="nav-number">2.</span> <span class="nav-text">创建任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">3.</span> <span class="nav-text">修改配置文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F"><span class="nav-number">4.</span> <span class="nav-text">运行程序</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="nav-number">5.</span> <span class="nav-text">数据解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="nav-number">6.</span> <span class="nav-text">持久化存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%9F%BA%E4%BA%8E%E7%BB%88%E7%AB%AF%E6%8C%87%E4%BB%A4%E7%9A%84%E5%AD%98%E5%82%A8"><span class="nav-number">6.1.</span> <span class="nav-text">1.基于终端指令的存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%9F%BA%E4%BA%8E%E7%AE%A1%E9%81%93%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="nav-number">6.2.</span> <span class="nav-text">2.基于管道的持久化存储</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8%E8%AF%B7%E6%B1%82%E5%8F%91%E9%80%81"><span class="nav-number">7.</span> <span class="nav-text">手动请求发送</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="nav-number">8.</span> <span class="nav-text">五大核心组件工作流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%B7%E6%B1%82%E4%BC%A0%E5%8F%82%E7%9A%84%E6%B7%B1%E5%BA%A6%E7%88%AC%E5%8F%96-4567kan-com"><span class="nav-number">9.</span> <span class="nav-text">请求传参的深度爬取-4567kan.com</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-number">10.</span> <span class="nav-text">中间件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD-%E7%88%AC%E5%8F%96jdlingyu-com%E5%9B%BE%E7%89%87"><span class="nav-number">11.</span> <span class="nav-text">大文件下载-爬取jdlingyu.com图片</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CrawlSpider-%E6%B7%B1%E5%BA%A6%E7%88%AC%E5%8F%96"><span class="nav-number">12.</span> <span class="nav-text">CrawlSpider 深度爬取</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Syst1m"
      src="/images/IMG_0010.JPG">
  <!--
  <p class="site-author-name" itemprop="name">Syst1m</p>
  -->
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">73</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">74</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/chencicici" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;chencicici" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.uodrad.top/" title="https:&#x2F;&#x2F;www.uodrad.top&#x2F;" rel="noopener" target="_blank">uodrad</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/Cl0ud/" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;Cl0ud&#x2F;" rel="noopener" target="_blank">春告鳥</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://076w.cn/" title="http:&#x2F;&#x2F;076w.cn&#x2F;" rel="noopener" target="_blank">076w</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://quan9i.top/" title="https:&#x2F;&#x2F;quan9i.top" rel="noopener" target="_blank">quan9i</a>
        </li>
    </ul>
  </div>


       <!--网易云插件-->
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1933503036&auto=0&height=66">
      </iframe>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/chencicici" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://syst1m.top/2022/04/08/scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_0010.JPG">
      <meta itemprop="name" content="Syst1m">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Syst1m">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="scrapy框架学习 | Syst1m">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          scrapy框架学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-04-08 18:57:11" itemprop="dateCreated datePublished" datetime="2022-04-08T18:57:11+08:00">2022-04-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-05-15 13:43:14" itemprop="dateModified" datetime="2022-05-15T13:43:14+08:00">2022-05-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject tutorial</span><br></pre></td></tr></table></figure>

<h2 id="创建任务"><a href="#创建任务" class="headerlink" title="创建任务"></a>创建任务</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider first www.baidu.com</span><br></pre></td></tr></table></figure>

<p>会生成一个first文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FirstSpider</span>(scrapy.Spider):</span><br><span class="line">    <span class="comment"># 唯一标识符</span></span><br><span class="line">    name = <span class="string">&#x27;first&#x27;</span></span><br><span class="line">    <span class="comment"># 允许的域名</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.baidu.com&#x27;</span>]</span><br><span class="line">    <span class="comment"># 起始的url,默认发送get请求</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.baidu.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据解析</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>只输出ERROR级别的日志</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只输出ERROR级别的日志</span></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br></pre></td></tr></table></figure>

<p>不遵从robots协议</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span>  </span><br></pre></td></tr></table></figure>

<p>指定ua</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT = <span class="string">&#x27;tutorial (+http://www.yourdomain.com)&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl first</span><br></pre></td></tr></table></figure>

<p>会输出一个response对象</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="number">200</span> https:<span class="regexp">//</span>www.baidu.com/&gt;</span><br></pre></td></tr></table></figure>

<h2 id="数据解析"><a href="#数据解析" class="headerlink" title="数据解析"></a>数据解析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FirstSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;first&#x27;</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://ishuo.cn/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 返回一个selector对象</span></span><br><span class="line">        title_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;list&quot;]/ul/li/div[1]/text()&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> title_list:</span><br><span class="line">            <span class="built_in">print</span>(title)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到返回了一个selector对象,我们想要的数据在data属性里</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chenci@MacBook-Pro tutorial %scrapy crawl first</span><br><span class="line">&lt;Selector xpath=<span class="string">&#x27;//*[@id=&quot;list&quot;]/ul/li/div[1]/text()&#x27;</span> data=<span class="string">&#x27;如果你得罪了老板，失去的只是一份工作；如果你得罪了客户，失去的不过是一份订...&#x27;</span>&gt;</span><br><span class="line">&lt;Selector xpath=<span class="string">&#x27;//*[@id=&quot;list&quot;]/ul/li/div[1]/text()&#x27;</span> data=<span class="string">&#x27;有位非常漂亮的女同事，有天起晚了没有时间化妆便急忙冲到公司。结果那天她被记...&#x27;</span>&gt;</span><br><span class="line">&lt;Selector xpath=<span class="string">&#x27;//*[@id=&quot;list&quot;]/ul/li/div[1]/text()&#x27;</span> data=<span class="string">&#x27;悟空和唐僧一起上某卫视非诚勿扰,悟空上台,24盏灯全灭。理由:1.没房没车...&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>

<p>从data属性中取出我们想要的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FirstSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;first&#x27;</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://ishuo.cn/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 返回一个selector对象</span></span><br><span class="line">        title_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;list&quot;]/ul/li/div[1]/text()&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> title_list:</span><br><span class="line">            <span class="comment"># 取出数据</span></span><br><span class="line">            title = title.extract()  <span class="comment"># extract_first()取第一个</span></span><br><span class="line">            <span class="built_in">print</span>(title)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="持久化存储"><a href="#持久化存储" class="headerlink" title="持久化存储"></a>持久化存储</h2><h3 id="1-基于终端指令的存储"><a href="#1-基于终端指令的存储" class="headerlink" title="1.基于终端指令的存储"></a>1.基于终端指令的存储</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FirstSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;first&#x27;</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://ishuo.cn/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        data_all = []</span><br><span class="line">        <span class="comment"># 返回一个selector对象</span></span><br><span class="line">        title_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;list&quot;]/ul/li/div[1]/text()&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> title_list:</span><br><span class="line">            <span class="comment"># 取出数据</span></span><br><span class="line">            title = title.extract()  <span class="comment"># extract_first()取第一个</span></span><br><span class="line">            <span class="comment"># 构造字典</span></span><br><span class="line">            dic = &#123;</span><br><span class="line">                <span class="string">&#x27;title&#x27;</span>: title</span><br><span class="line">            &#125;</span><br><span class="line">            data_all.append(dic)</span><br><span class="line">        <span class="comment"># 返回一个列表</span></span><br><span class="line">        <span class="keyword">return</span> data_all</span><br></pre></td></tr></table></figure>

<p>执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chenci@MacBook-Pro tutorial %scrapy crawl first -o test.csv</span><br></pre></td></tr></table></figure>

<h3 id="2-基于管道的持久化存储"><a href="#2-基于管道的持久化存储" class="headerlink" title="2.基于管道的持久化存储"></a>2.基于管道的持久化存储</h3><p>开启管道</p>
<p>settings.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;tutorial.pipelines.TutorialPipeline&#x27;</span>: <span class="number">300</span>,  <span class="comment"># 300表示优先级,越小优先级越高</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在items.py中定义相关属性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TutorialItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    <span class="comment"># Field定义好的属性当做万能属性</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将first.py提取出的数据提交给管道</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> TutorialItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FirstSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;first&#x27;</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://ishuo.cn/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 返回一个selector对象</span></span><br><span class="line">        title_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;list&quot;]/ul/li/div[1]/text()&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> title_list:</span><br><span class="line">            <span class="comment"># 取出数据</span></span><br><span class="line">            title = title.extract()  <span class="comment"># extract_first()取第一个</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 实例化一个item对象,将解析到的数据存入到该对象</span></span><br><span class="line">            item = TutorialItem()</span><br><span class="line">            <span class="comment"># 通过字典的方式调用</span></span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line">            <span class="comment"># 将item对象提交给管道</span></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在pipelines.py中重写父类方法,存储到本地</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TutorialPipeline</span>:</span><br><span class="line">    <span class="comment"># 重写父类方法</span></span><br><span class="line">    f = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我是open_spider,只会在爬虫开始的时候执行一次&#x27;</span>)</span><br><span class="line">        self.f = <span class="built_in">open</span>(<span class="string">&#x27;./text1.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我是close_spider,只会在爬虫开始的时候执行一次&#x27;</span>)</span><br><span class="line">        self.f.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 该方法是用来接收item对象的,一次只能接收一个item,说明该方法会被多次调用</span></span><br><span class="line">    <span class="comment"># 参数item就是接收的item对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 存储到本地文件</span></span><br><span class="line">        self.f.write(item[<span class="string">&#x27;title&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>基于管道实现数据的备份</p>
<p>pipelines.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MysqlPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    conn = <span class="literal">None</span></span><br><span class="line">    cursor = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写父类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="comment"># 数据库连接对象</span></span><br><span class="line">        self.conn = pymysql.Connect(host=<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">3306</span>, user=<span class="string">&#x27;root&#x27;</span>, password=<span class="string">&#x27;123456&#x27;</span>, charset=<span class="string">&#x27;utf8&#x27;</span>,</span><br><span class="line">                                    db=<span class="string">&#x27;spider&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line">        sql = <span class="string">&#x27;insert into duanzi values(&quot;%s&quot;)&#x27;</span> % item[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">        <span class="comment"># 事务处理</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.cursor.execute(sql)</span><br><span class="line">            self.conn.commit()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line">            self.conn.rollback()</span><br><span class="line">        <span class="comment"># 返回item会给下一个管道使用,如果不返回,下一个管道将接收不到</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写父类,关闭连接</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.cursor.close()</span><br><span class="line">        self.conn.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在settings.py增加一个管道</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="comment"># 爬虫文件中的item只会提交给优先级最高的那一个管道类</span></span><br><span class="line">    <span class="string">&#x27;tutorial.pipelines.TutorialPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;tutorial.pipelines.MysqlPipeline&#x27;</span>: <span class="number">301</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="手动请求发送"><a href="#手动请求发送" class="headerlink" title="手动请求发送"></a>手动请求发送</h2><p>新建工程</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chenci@MacBook-Pro scrapy %scrapy startproject HandReq</span><br><span class="line">chenci@MacBook-Pro scrapy %<span class="built_in">cd</span> HandReq </span><br><span class="line">chenci@MacBook-Pro HandReq %scrapy genspider duanzi www.xxx.com</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> HandReq.items <span class="keyword">import</span> HandreqItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DuanziSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;duanzi&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://duanzixing.com/page/1/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通用的url模板</span></span><br><span class="line">    url = <span class="string">&#x27;https://duanzixing.com/page/%d/&#x27;</span></span><br><span class="line">    page_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        title_list = response.xpath(<span class="string">&#x27;/html/body/section/div/div/article[1]/header/h2/a/text()&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> title_list:</span><br><span class="line">            title = title.extract()</span><br><span class="line">            item = HandreqItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.page_num &lt; <span class="number">5</span>:</span><br><span class="line">            <span class="comment"># 构造页码</span></span><br><span class="line">            new_url = <span class="built_in">format</span>(self.url % self.page_num)</span><br><span class="line">            self.page_num += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 对新的url发起请求,递归回调自己</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=new_url, callback=self.parse)</span><br><span class="line">            <span class="comment"># scrapy.FormRequest(url,callback,formdata) 发送post请求</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="五大核心组件工作流程"><a href="#五大核心组件工作流程" class="headerlink" title="五大核心组件工作流程"></a>五大核心组件工作流程</h2><p><img src="https://raw.githubusercontent.com/chencicici/images/main/202205151340834.png?token=ARYCSAUIENI4WQ2X26Z4EQ3CQCJIM"></p>
<p>引擎(Scrapy)</p>
<pre><code>用来处理整个系统的数据流处理, 触发事务(框架核心)
</code></pre>
<p>调度器(Scheduler)</p>
<pre><code>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
</code></pre>
<p>下载器(Downloader)</p>
<pre><code>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
</code></pre>
<p>爬虫(Spiders)</p>
<pre><code>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
</code></pre>
<p>项目管道(Pipeline)</p>
<pre><code>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。
</code></pre>
<h2 id="请求传参的深度爬取-4567kan-com"><a href="#请求传参的深度爬取-4567kan-com" class="headerlink" title="请求传参的深度爬取-4567kan.com"></a>请求传参的深度爬取-4567kan.com</h2><p>文件目录</p>
<p><img src="https://raw.githubusercontent.com/chencicici/images/main/202205151341626.png?token=ARYCSAWOAVW6ADT6LYL7NY3CQCJMU"></p>
<pre><code>meta是一个字典,可以将meta传给callback
    scrapy.Request(url, callback, meta)

callback取出字典
    item = response.meta[&#39;item&#39;]
</code></pre>
<p>move.py 项目文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> move_4567kan.items <span class="keyword">import</span> Move4567KanItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MoveSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;move&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.4567kan.com/frim/index1-1.html&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造页码</span></span><br><span class="line">    url = <span class="string">&#x27;https://www.4567kan.com/frim/index1-%d.html&#x27;</span></span><br><span class="line">    page_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 抓取url和title</span></span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;/html/body/div[2]/div/div[3]/div/div[2]/ul/li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            url = <span class="string">&#x27;https://www.4567kan.com&#x27;</span> + li.xpath(<span class="string">&#x27;./div/a/@href&#x27;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            title = li.xpath(<span class="string">&#x27;./div/a/@title&#x27;</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 传递给item</span></span><br><span class="line">            item = Move4567KanItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对详情页发起请求,回调get_details函数</span></span><br><span class="line">            <span class="comment"># meta请求传参,以字典形式,传给get_details函数,因为item只能是唯一</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.get_details, meta=&#123;<span class="string">&#x27;item&#x27;</span>: item&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 爬取多页</span></span><br><span class="line">        <span class="keyword">if</span> self.page_num &lt; <span class="number">5</span>:</span><br><span class="line">            <span class="comment"># 构造页码</span></span><br><span class="line">            new_url = <span class="built_in">format</span>(self.url % self.page_num)</span><br><span class="line">            self.page_num += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 对新的url发起请求,递归回调自己</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=new_url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自定义函数去抓取详情</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_details</span>(<span class="params">self, response</span>):</span><br><span class="line">        details = response.xpath(<span class="string">&#x27;//*[@class=&quot;detail-content&quot;]/text()&#x27;</span>).extract()</span><br><span class="line">        <span class="comment"># 判断,没有返回None</span></span><br><span class="line">        <span class="keyword">if</span> details:</span><br><span class="line">            details = details[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            details = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 接受item</span></span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        item[<span class="string">&#x27;details&#x27;</span>] = details</span><br><span class="line">        <span class="comment"># 提交给管道</span></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>items.py 定义两个字段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Move4567KanItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    details = scrapy.Field()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>pipelines.py 打印输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Move4567KanPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="中间件"><a href="#中间件" class="headerlink" title="中间件"></a>中间件</h2><p>作用</p>
<pre><code>拦截请求和响应
</code></pre>
<p>爬虫中间件</p>
<pre><code>略
</code></pre>
<p>下载中间件(推荐)</p>
<pre><code>拦截请求:    
    1.篡改请求url
    2.伪装请求头信息:
        UA
        Cookie
    3.设置请求代理

拦截响应:
    篡改响应数据
</code></pre>
<p>改写中间件文件 middlewares.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> is_item, ItemAdapter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MiddleDownloaderMiddleware</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拦截所有请求</span></span><br><span class="line">    <span class="comment"># request就是拦截到的请求,spider就是爬虫类实例化的对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我是process_request()&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拦截所有响应对象</span></span><br><span class="line">    <span class="comment"># request就是response响应对象对应的请求对象,response就是拦截到的响应对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我是process_response()&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拦截异常请求</span></span><br><span class="line">    <span class="comment"># request就是拦截到的异常请求的请求对象</span></span><br><span class="line">    <span class="comment"># 作用:修正异常请求,将其 重新发送</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我是process_exception()&#x27;</span>)</span><br><span class="line">        <span class="comment"># pass</span></span><br></pre></td></tr></table></figure>

<p>编写爬虫文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MidSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;mid&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.baidu.com&#x27;</span>, <span class="string">&#x27;https://www.sogou.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="built_in">print</span>(response)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在配置文件setting.py中启用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;middle.middlewares.MiddleDownloaderMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>启动工程</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">chenci@MacBook-Pro middle %scrapy crawl mid</span><br><span class="line">我是process_request()</span><br><span class="line">我是process_request()</span><br><span class="line">我是process_response()</span><br><span class="line">我是process_exception()</span><br><span class="line">我是process_response()</span><br><span class="line">我是process_exception()</span><br></pre></td></tr></table></figure>

<p>process_exception()方法设置代理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拦截异常请求</span></span><br><span class="line"><span class="comment"># request就是拦截到的异常请求的请求对象</span></span><br><span class="line"><span class="comment"># 作用:修正异常请求,将其 重新发送</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">    <span class="comment"># 请求的ip被禁,该请求就会变成一个异常请求,加入代理</span></span><br><span class="line">    request.meta[<span class="string">&#x27;proxy_&#x27;</span>] = <span class="string">&#x27;https://ip:port&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;我是process_exception()&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将异常的请求修正后重新发送</span></span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line">    <span class="comment"># 可能会造成死循环,因为如果加入代理后依旧发生异常,会再次加入代理去请求</span></span><br></pre></td></tr></table></figure>

<p>process_request()方法设置headers</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">    <span class="comment"># 设置请求头,但一般不这么写,可以在setting.py中设置全局</span></span><br><span class="line">    request.headers[<span class="string">&#x27;User-Agent&#x27;</span>] = <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">    request.headers[<span class="string">&#x27;Cookie&#x27;</span>] = <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;我是process_request()&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>process_response()方法篡改响应数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拦截所有响应对象</span></span><br><span class="line"><span class="comment"># request就是response响应对象对应的请求对象,response就是拦截到的响应对象</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">    <span class="comment"># 篡改响应数据</span></span><br><span class="line">    response.text = <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;我是process_response()&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>

<h2 id="大文件下载-爬取jdlingyu-com图片"><a href="#大文件下载-爬取jdlingyu-com图片" class="headerlink" title="大文件下载-爬取jdlingyu.com图片"></a>大文件下载-爬取jdlingyu.com图片</h2><p>文件目录</p>
<p><img src="https://raw.githubusercontent.com/chencicici/images/main/202205151342799.png?token=ARYCSAQF2H5ORVGMPC5ZA6TCQCJQG"></p>
<p>img.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> imgdownload.items <span class="keyword">import</span> ImgdownloadItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImgSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;img&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.jdlingyu.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;/html/body/div[1]/div[2]/div[1]/div/div[6]/div/div[1]/div/div[2]/ul/li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> li_list:</span><br><span class="line">            url = a.xpath(<span class="string">&#x27;./div/div[2]/h2/a/@href&#x27;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            title = a.xpath(<span class="string">&#x27;./div/div[2]/h2/a/text()&#x27;</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 传递给itme</span></span><br><span class="line">            item = ImgdownloadItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 回调并传递参数</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.get_img_url, meta=&#123;<span class="string">&#x27;item&#x27;</span>: item&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对每个图集的url发起请求</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_img_url</span>(<span class="params">self, response</span>):</span><br><span class="line">        page = <span class="number">0</span></span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        <span class="comment"># 抓取每张图片的下载链接</span></span><br><span class="line">        img_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;primary-home&quot;]/article/div[2]/img&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> scr <span class="keyword">in</span> img_list:</span><br><span class="line">            img_url = scr.xpath(<span class="string">&#x27;./@src&#x27;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            page += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 传递给item</span></span><br><span class="line">            item[<span class="string">&#x27;img_url&#x27;</span>] = img_url</span><br><span class="line">            item[<span class="string">&#x27;page&#x27;</span>] = page</span><br><span class="line">            <span class="comment"># 提交给管道</span></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>setting.py增加配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT = <span class="string">&#x27;ua&#x27;</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br><span class="line"><span class="comment"># 图片存放目录</span></span><br><span class="line">IMAGES_STORE = <span class="string">&#x27;./imgs&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>items.py增加字段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImgdownloadItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    img_url = scrapy.Field()</span><br><span class="line">    page = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>pipelines.py增加管道类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认管道类无法请求图片数据</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImgdownloadPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接受图片地址和title,然后对其进行请求存储到本地</span></span><br><span class="line"><span class="comment"># 提供了数据下载功能,也可以下载视频和音频</span></span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 继承ImagesPipeline类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">img_download</span>(<span class="title class_ inherited__">ImagesPipeline</span>):</span><br><span class="line">    <span class="comment"># 重写三个父类方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_media_requests</span>(<span class="params">self, item, info</span>):</span><br><span class="line">        <span class="comment"># 下载,并传参,如果传递整个item,最后只会下载一张图片,原因未知</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=item[<span class="string">&#x27;img_url&#x27;</span>], meta=&#123;<span class="string">&#x27;title&#x27;</span>: item[<span class="string">&#x27;title&#x27;</span>], <span class="string">&#x27;page&#x27;</span>: item[<span class="string">&#x27;page&#x27;</span>]&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回图片保存路径</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span>, *, item=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 拼接路径</span></span><br><span class="line">        title = request.meta[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">        page = request.meta[<span class="string">&#x27;page&#x27;</span>]</span><br><span class="line">        path = <span class="string">f&#x27;<span class="subst">&#123;title&#125;</span>/<span class="subst">&#123;page&#125;</span>.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回路径</span></span><br><span class="line">        <span class="keyword">return</span> path</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将item返回给下一个即将被执行的管道类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">item_completed</span>(<span class="params">self, results, item, info</span>):</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>setting.py增加管道类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="comment">#&#x27;imgdownload.pipelines.ImgdownloadPipeline&#x27;: 300,</span></span><br><span class="line">   <span class="string">&#x27;imgdownload.pipelines.img_download&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行效果</p>
<p><img src="https://raw.githubusercontent.com/chencicici/images/main/202205151342947.png?token=ARYCSATQHIG5XPI3W7VHBBTCQCJR2"></p>
<h2 id="CrawlSpider-深度爬取"><a href="#CrawlSpider-深度爬取" class="headerlink" title="CrawlSpider 深度爬取"></a>CrawlSpider 深度爬取</h2><p>是什么</p>
<pre><code>是Spider的一个子类,也就是爬虫文件的父类
</code></pre>
<p>作用:用作于全站数据的爬取</p>
<pre><code>将一个页面下所有的页码进行爬取
</code></pre>
<p>基本使用</p>
<pre><code>1.创建一个工程
2.创建一个基于CrawlSpider类的爬虫文件
    crapy genspider -t crawl main www.xxx.com
3.执行工程
</code></pre>
<p>编写工程文件main.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MainSpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;main&#x27;</span></span><br><span class="line">    <span class="comment">#allowed_domains = [&#x27;https://www.mn52.com/&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.mn52.com/fj/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 链接提取器,根据allow里的正则来提取url</span></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># 对提取的url发起请求,然后回调解析</span></span><br><span class="line">        <span class="comment"># 如果allow为空 将抓取此页面下的链接</span></span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;list_8_\d.html&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="built_in">print</span>(response)</span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>执行工程</p>
<pre><code>可以看到抓取了所有页码的url
</code></pre>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">chenci@MacBook-Pro crawl %scrapy crawl main</span><br><span class="line">&lt;200 https://www.mn52.com/fj/list_8_2.html&gt;</span><br><span class="line">&lt;200 https://www.mn52.com/fj/list_8_3.html&gt;</span><br><span class="line">&lt;200 https://www.mn52.com/fj/list_8_4.html&gt;</span><br><span class="line">&lt;200 https://www.mn52.com/fj/list_8_8.html&gt;</span><br><span class="line">&lt;200 https://www.mn52.com/fj/list_8_5.html&gt;</span><br><span class="line">&lt;200 https://www.mn52.com/fj/list_8_7.html&gt;</span><br><span class="line">&lt;200 https://www.mn52.com/fj/list_8_9.html&gt;</span><br><span class="line">&lt;200 https://www.mn52.com/fj/list_8_6.html&gt;</span><br></pre></td></tr></table></figure>



    </div>

    
    
    
      


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Syst1m
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://syst1m.top/2022/04/08/scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/" title="scrapy框架学习">https://syst1m.top/2022/04/08/scrapy框架学习/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%95%99%E7%A8%8B/" rel="tag"># 教程</a>
              <a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"># 爬虫</a>
              <a href="/tags/scrapy/" rel="tag"># scrapy</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/03/01/ubuntu%E4%B8%8Bhadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/" rel="prev" title="ubuntu下大数据集群搭建">
                  <i class="fa fa-chevron-left"></i> ubuntu下大数据集群搭建
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/04/14/%E5%BC%82%E6%AD%A5%E7%88%AC%E5%8F%96%E6%9F%90%E6%B6%A9%E6%83%85%E7%BD%91%E7%AB%99%E5%9B%BE%E7%89%87/" rel="next" title="异步协程爬取福利姬">
                  异步协程爬取福利姬 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Syst1m</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>


    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.0/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
