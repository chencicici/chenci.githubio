[{"title":"异步协程爬取福利姬涩图","url":"/2022/04/14/异步爬取某涩情网站图片/","content":"\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\n-------------------------------------------------\n   File Name：     协程4-实战爬取tuao8.com\n   Author :       chenci\n   date：          2022/3/25\n-------------------------------------------------\n\"\"\"\nimport aiofiles\nimport requests\nfrom lxml import etree\nimport asyncio\nimport aiohttp\nfrom fake_useragent import UserAgent\nimport os\nimport time\n\ndef create_dir_not_exist(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\n# 抓取每个条目的图集\ndef get_photos_url():\n    resp = requests.get(url='https://www.tuao8.xyz/category-2_1.html', headers=header)\n    text = etree.HTML(resp.text)\n    href_url_list = text.xpath('//*[@id=\"container\"]/main/article/div/a/@href')\n    return href_url_list\n\n\n# 去请求每个图集.返回源码\nasync def get_photos(photo_list):\n    # 限制并发熟路默认100,0为无限制\n    conn = aiohttp.TCPConnector(limit=10)\n    # 发送请求\n    async with aiohttp.ClientSession(connector=conn) as session:\n        async with await session.get(url=photo_list, headers=header) as resp:\n            page_text = await resp.text()\n            await get_photos_title_page(page_text, photo_list)\n\n\n# 从每个源码里筛选出标题和最大页码,url\nasync def get_photos_title_page(text, url):\n    tasks = []\n    html = etree.HTML(text)\n    title = html.xpath('//*[@id=\"container\"]/main/article/h1/text()')[0]\n    max_page = int(html.xpath('//*[@id=\"dm-fy\"]/li[last()-1]/a/text()')[0])\n    create_dir_not_exist(f'./imgs/tuzo_xc/{title}')\n    task = asyncio.create_task(get_download_url(url=url, title=title, max_page=max_page))\n    tasks.append(task)\n    await asyncio.wait(tasks)\n\n\n# 获取每一页的url并从源码中筛选出每张图片的下载链接\nasync def get_download_url(url, title, max_page):\n    tasks = []\n    for i in range(1, max_page):\n        urls = f'{url}?page={i}'\n        conn = aiohttp.TCPConnector(limit=10)\n        async with aiohttp.ClientSession(connector=conn) as session:\n            async with await session.get(url=urls, headers=header) as resp:\n                page_text = await resp.text()\n                html = etree.HTML(page_text)\n                image_url = html.xpath('//*[@class=\"entry\"]//img/@src')[0]\n                task = asyncio.create_task(download_image(image_url, title, i))\n                tasks.append(task)\n    await asyncio.wait(tasks)\n\n\n# 下载\nasync def download_image(image_url, title, i):\n    conn = aiohttp.TCPConnector(limit=30)\n    async with aiohttp.ClientSession(connector=conn) as session:\n        try:\n            async with await session.get(url=image_url, headers=header) as resp:\n                print(image_url)\n                async with aiofiles.open(f'./imgs/{title}/{i}.jpg', 'wb') as f:\n                    print(f'正在下载{title}  第{i}张')\n                    await f.write(await resp.read())\n        except:\n            pass\n        print('下载完成')\n\n\nasync def main():\n    href_url_list = get_photos_url()\n    tasks = []\n    for url in href_url_list:\n        task = asyncio.create_task(get_photos(photo_list=url))\n        tasks.append(task)\n    await asyncio.wait(tasks)\n\n\nif __name__ == '__main__':\n    start = time.time()\n    ua = UserAgent()\n    header = {\n        'Referer': 'https://www.tuao8.xyz/category-2_2.html',\n        'user-agent': ua.random\n    }\n    asyncio.run(main())\n    end = time.time()\n    print('全部下载完成!耗时:', int(end - start), '秒')\n\n\n```\n![1](img.png)\n","tags":["requests","异步"],"categories":["爬虫"]},{"title":"scrapy框架学习","url":"/2022/04/08/scrapy框架学习/","content":"\n## 创建项目\n\n```bash\nscrapy startproject tutorial\n```\n\n## 创建任务\n\n```bash\nscrapy genspider first www.baidu.com\n```\n\n会生成一个first文件\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    # 唯一标识符\n    name = 'first'\n    # 允许的域名\n    allowed_domains = ['www.baidu.com']\n    # 起始的url,默认发送get请求\n    start_urls = ['https://www.baidu.com/']\n\n    # 数据解析\n    def parse(self, response):\n        pass\n```\n\n## 修改配置文件\n\n只输出ERROR级别的日志\n\n```python\n# 只输出ERROR级别的日志\nLOG_LEVEL = 'ERROR'\n```\n\n不遵从robots协议\n\n```python\nROBOTSTXT_OBEY = False  \n```\n\n指定ua\n\n```python\nUSER_AGENT = 'tutorial (+http://www.yourdomain.com)'\n```\n\n## 运行程序\n\n```bash\nscrapy crawl first\n```\n\n会输出一个response对象\n\n```\n<200 https://www.baidu.com/>\n```\n\n## 数据解析\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            print(title)\n\n```\n\n可以看到返回了一个selector对象,我们想要的数据在data属性里\n\n```bash\nchenci@MacBook-Pro tutorial %scrapy crawl first\n<Selector xpath='//*[@id=\"list\"]/ul/li/div[1]/text()' data='如果你得罪了老板，失去的只是一份工作；如果你得罪了客户，失去的不过是一份订...'>\n<Selector xpath='//*[@id=\"list\"]/ul/li/div[1]/text()' data='有位非常漂亮的女同事，有天起晚了没有时间化妆便急忙冲到公司。结果那天她被记...'>\n<Selector xpath='//*[@id=\"list\"]/ul/li/div[1]/text()' data='悟空和唐僧一起上某卫视非诚勿扰,悟空上台,24盏灯全灭。理由:1.没房没车...'>\n```\n\n从data属性中取出我们想要的数据\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            # 取出数据\n            title = title.extract()  # extract_first()取第一个\n            print(title)\n\n```\n\n## 持久化存储\n\n### 1.基于终端指令的存储\n\n```python\nimport scrapy\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        data_all = []\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            # 取出数据\n            title = title.extract()  # extract_first()取第一个\n            # 构造字典\n            dic = {\n                'title': title\n            }\n            data_all.append(dic)\n        # 返回一个列表\n        return data_all\n```\n\n执行\n\n```bash\nchenci@MacBook-Pro tutorial %scrapy crawl first -o test.csv\n```\n\n### 2.基于管道的持久化存储\n\n开启管道\n\nsettings.py\n\n```python\nITEM_PIPELINES = {\n    'tutorial.pipelines.TutorialPipeline': 300,  # 300表示优先级,越小优先级越高\n}\n```\n\n在items.py中定义相关属性\n\n```python\nimport scrapy\n\n\nclass TutorialItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    # Field定义好的属性当做万能属性\n    title = scrapy.Field()\n\n```\n\n将first.py提取出的数据提交给管道\n\n```python\nimport scrapy\nfrom tutorial.items import TutorialItem\n\n\nclass FirstSpider(scrapy.Spider):\n    name = 'first'\n    start_urls = ['https://ishuo.cn/']\n\n    def parse(self, response):\n        # 返回一个selector对象\n        title_list = response.xpath('//*[@id=\"list\"]/ul/li/div[1]/text()')\n        for title in title_list:\n            # 取出数据\n            title = title.extract()  # extract_first()取第一个\n\n            # 实例化一个item对象,将解析到的数据存入到该对象\n            item = TutorialItem()\n            # 通过字典的方式调用\n            item['title'] = title\n            # 将item对象提交给管道\n            yield item\n\n```\n\n在pipelines.py中重写父类方法,存储到本地\n\n```python\nclass TutorialPipeline:\n    # 重写父类方法\n    f = None\n\n    def open_spider(self, spider):\n        print('我是open_spider,只会在爬虫开始的时候执行一次')\n        self.f = open('./text1.txt', 'w', encoding='utf-8')\n\n    def close_spider(self, spider):\n        print('我是close_spider,只会在爬虫开始的时候执行一次')\n        self.f.close()\n\n    # 该方法是用来接收item对象的,一次只能接收一个item,说明该方法会被多次调用\n    # 参数item就是接收的item对象\n    def process_item(self, item, spider):\n        # 存储到本地文件\n        self.f.write(item['title'] + '\\n')\n        return item\n```\n\n基于管道实现数据的备份\n\npipelines.py\n\n```python\nimport pymysql\n\n\nclass MysqlPipeline(object):\n    conn = None\n    cursor = None\n\n    # 重写父类\n    def open_spider(self, spider):\n        # 数据库连接对象\n        self.conn = pymysql.Connect(host='localhost', port=3306, user='root', password='123456', charset='utf8',\n                                    db='spider')\n\n    def process_item(self, item, spider):\n        self.cursor = self.conn.cursor()\n        sql = 'insert into duanzi values(\"%s\")' % item['title']\n        # 事务处理\n        try:\n            self.cursor.execute(sql)\n            self.conn.commit()\n        except Exception as e:\n            print(e)\n            self.conn.rollback()\n        # 返回item会给下一个管道使用,如果不返回,下一个管道将接收不到\n        return item\n\n    # 重写父类,关闭连接\n    def close_spider(self, spider):\n        self.cursor.close()\n        self.conn.close()\n\n```\n\n在settings.py增加一个管道\n\n```python\nITEM_PIPELINES = {\n    # 爬虫文件中的item只会提交给优先级最高的那一个管道类\n    'tutorial.pipelines.TutorialPipeline': 300,\n    'tutorial.pipelines.MysqlPipeline': 301,\n}\n```\n\n## 手动请求发送\n\n新建工程\n\n```bash\nchenci@MacBook-Pro scrapy %scrapy startproject HandReq\nchenci@MacBook-Pro scrapy %cd HandReq \nchenci@MacBook-Pro HandReq %scrapy genspider duanzi www.xxx.com\n```\n\n```python\nimport scrapy\nfrom HandReq.items import HandreqItem\n\n\nclass DuanziSpider(scrapy.Spider):\n    name = 'duanzi'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://duanzixing.com/page/1/']\n\n    # 通用的url模板\n    url = 'https://duanzixing.com/page/%d/'\n    page_num = 2\n\n    def parse(self, response):\n        title_list = response.xpath('/html/body/section/div/div/article[1]/header/h2/a/text()')\n        for title in title_list:\n            title = title.extract()\n            item = HandreqItem()\n            item['title'] = title\n            yield item\n\n        if self.page_num < 5:\n            # 构造页码\n            new_url = format(self.url % self.page_num)\n            self.page_num += 1\n            # 对新的url发起请求,递归回调自己\n            yield scrapy.Request(url=new_url, callback=self.parse)\n            # scrapy.FormRequest(url,callback,formdata) 发送post请求\n\n```\n\n## 五大核心组件工作流程\n\n![1](img.png)\n\n引擎(Scrapy)\n\n    用来处理整个系统的数据流处理, 触发事务(框架核心)\n\n调度器(Scheduler)\n\n    用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址\n\n下载器(Downloader)\n\n    用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)\n\n爬虫(Spiders)\n\n    爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面\n\n项目管道(Pipeline)\n\n    负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。\n\n## 请求传参的深度爬取\n\n通过请求传参,深度爬取4567kan.com\n\n    meta是一个字典,可以将meta传给callback\n        scrapy.Request(url, callback, meta)\n\n    callback取出字典\n        item = response.meta['item']\n\nmove.py 项目文件\n\n```python\nimport scrapy\nfrom move_4567kan.items import Move4567KanItem\n\n\nclass MoveSpider(scrapy.Spider):\n    name = 'move'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://www.4567kan.com/frim/index1-1.html']\n\n    # 构造页码\n    url = 'https://www.4567kan.com/frim/index1-%d.html'\n    page_num = 2\n\n    def parse(self, response):\n        # 抓取url和title\n        li_list = response.xpath('/html/body/div[2]/div/div[3]/div/div[2]/ul/li')\n        for li in li_list:\n            url = 'https://www.4567kan.com' + li.xpath('./div/a/@href').extract()[0]\n            title = li.xpath('./div/a/@title').extract()[0]\n\n            # 传递给item\n            item = Move4567KanItem()\n            item['title'] = title\n\n            # 对详情页发起请求,回调get_details函数\n            # meta请求传参,以字典形式,传给get_details函数,因为item只能是唯一\n            yield scrapy.Request(url=url, callback=self.get_details, meta={'item': item})\n\n        # 爬取多页\n        if self.page_num < 5:\n            # 构造页码\n            new_url = format(self.url % self.page_num)\n            self.page_num += 1\n            # 对新的url发起请求,递归回调自己\n            yield scrapy.Request(url=new_url, callback=self.parse)\n\n    # 自定义函数去抓取详情\n    def get_details(self, response):\n        details = response.xpath('//*[@class=\"detail-content\"]/text()').extract()\n        # 判断,没有返回None\n        if details:\n            details = details[0]\n        else:\n            details = None\n        # 接受item\n        item = response.meta['item']\n        item['details'] = details\n        # 提交给管道\n        yield item\n\n```\n\nitems.py 定义两个字段\n\n```python\nimport scrapy\n\n\nclass Move4567KanItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = scrapy.Field()\n    details = scrapy.Field()\n\n```\n\npipelines.py 打印输出\n\n```python\nclass Move4567KanPipeline:\n    def process_item(self, item, spider):\n        print(item)\n        return item\n\n```\n\n## 中间件\n\n作用\n\n    拦截请求和响应\n\n爬虫中间件\n\n    略\n\n下载中间件(推荐)\n\n    拦截请求:    \n        1.篡改请求url\n        2.伪装请求头信息:\n            UA\n            Cookie\n        3.设置请求代理\n\n    拦截响应:\n        篡改响应数据\n\n改写中间件文件 middlewares.py\n\n```python\n\nfrom scrapy import signals\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass MiddleDownloaderMiddleware:\n\n    # 拦截所有请求\n    # request就是拦截到的请求,spider就是爬虫类实例化的对象\n    def process_request(self, request, spider):\n        print('我是process_request()')\n        return None\n\n    # 拦截所有响应对象\n    # request就是response响应对象对应的请求对象,response就是拦截到的响应对象\n    def process_response(self, request, response, spider):\n        print('我是process_response()')\n        return response\n\n    # 拦截异常请求\n    # request就是拦截到的异常请求的请求对象\n    # 作用:修正异常请求,将其 重新发送\n    def process_exception(self, request, exception, spider):\n        print('我是process_exception()')\n        # pass\n```\n\n编写爬虫文件\n\n```python\nimport scrapy\n\n\nclass MidSpider(scrapy.Spider):\n    name = 'mid'\n    # allowed_domains = ['www.xxx.com']\n    start_urls = ['https://www.baidu.com', 'https://www.sogou.com']\n\n    def parse(self, response):\n        print(response)\n\n```\n\n在配置文件setting.py中启用\n\n```python\nROBOTSTXT_OBEY = True\n\nDOWNLOADER_MIDDLEWARES = {\n    'middle.middlewares.MiddleDownloaderMiddleware': 543,\n}\n```\n\n启动工程\n\n```bash\nchenci@MacBook-Pro middle %scrapy crawl mid\n我是process_request()\n我是process_request()\n我是process_response()\n我是process_exception()\n我是process_response()\n我是process_exception()\n```\n\nprocess_exception()方法设置代理\n\n```python\n# 拦截异常请求\n# request就是拦截到的异常请求的请求对象\n# 作用:修正异常请求,将其 重新发送\ndef process_exception(self, request, exception, spider):\n    # 请求的ip被禁,该请求就会变成一个异常请求,加入代理\n    request.meta['proxy_'] = 'https://ip:port'\n    print('我是process_exception()')\n    # 将异常的请求修正后重新发送\n    return request\n    # 可能会造成死循环,因为如果加入代理后依旧发生异常,会再次加入代理去请求\n```\n\nprocess_request()方法设置headers\n```python\ndef process_request(self, request, spider):\n    # 设置请求头,但一般不这么写,可以在setting.py中设置全局\n    request.headers['User-Agent'] = 'xxx'\n    request.headers['Cookie'] = 'xxx'\n    print('我是process_request()')\n    return None\n```\n\nprocess_response()方法篡改响应数据\n```python\n# 拦截所有响应对象\n# request就是response响应对象对应的请求对象,response就是拦截到的响应对象\ndef process_response(self, request, response, spider):\n    # 篡改响应数据\n    response.text = 'xxx'\n    print('我是process_response()')\n    return response\n```\n","tags":["爬虫","scrapy","教程"],"categories":["爬虫"]},{"title":"ubuntu下hadoop集群搭建","url":"/2022/03/01/ubuntu下hadoop集群搭建/","content":"### 一.配置ip(三个节点)\n自ubuntu17之后多了一种配置方式更加高效,也就是netplan\n\n1.1编辑配置文件\n```bash\nroot@master:/etc/netplan# gedit /etc/netplan/01-network-manager-all.yaml\n```\n配置内容如下,`注意缩进`\n```yaml\nnetwork:\n  version: 2\n  renderer: NetworkManager\n  ethernets:\n     ens33:\n       dhcp4:  no\n       dhcp6:  no\n       addresses:  [192.168.10.101/24]\n       gateway4:  192.168.10.1\n       nameservers:\n         addresses: [8.8.8.8,192.168.10.1]\n```\n1.2使配置生效\n```bash\nroot@master:/etc/netplan# netplan apply\n```\n如果没有报错则配置成功\n\n### 二.配置主机名和主机名映射(三个节点)\n1.1配置主机名并查看\n```bash\n重启后生效\nroot@master:/etc/netplan# hostnamectl set-hostname master\nroot@master:/etc/netplan# hostname\n```\n1.2配置主机名映射\n```bash\nroot@master:/etc/netplan# gedit /etc/hosts\n```\n添加以下内容\n```bash\n192.168.10.101 master\n192.168.10.102 slave1\n192.168.10.103 slave2\n```\n1.3ping测试\n```bash\n有以下回显证明配置成功\nroot@master:/etc/netplan# ping slave2\nPING slave2 (192.168.10.103) 56(84) bytes of data.\n64 bytes from slave2 (192.168.10.103): icmp_seq=1 ttl=64 time=0.891 ms\n64 bytes from slave2 (192.168.10.103): icmp_seq=2 ttl=64 time=0.369 ms\n64 bytes from slave2 (192.168.10.103): icmp_seq=3 ttl=64 time=0.455 ms\n```\n1.4将hosts文件分发给子节点\n```bash\nroot@master:/etc/netplan# scp /etc/hosts root@slave1:/etc/\n输入yes再输入密码\n```\n\n### 三.配置ssh免密登录(三个节点)\n因为Ubuntu并不自带ssh服务所以要安装ssh并配置允许root远程登录\n```bash\n下载\nsudo apt-get install openssh-server\n启动\nsudo service ssh start\n配置\nsudo vim /etc/ssh/sshd_config\n添加一条\nPermitRootLogin yes\n```\n1.生成密钥\n```bash\nroot@master:~# ssh-keygen -t rsa\n一直回车\n```\n2.将密钥写入authorized.keys文件\n```bash\nroot@master:~# cd .ssh/\nroot@master:~/.ssh# cat id_rsa.pub >> authorized_keys\n\n```\n3.在另外两个子节点执行以上操作,并将authorized.keys的内容复制进master主机的authorized.keys文件末尾,成功后如下\n```bash\nroot@master:~/.ssh# cat authorized.keys \nssh-dss AAAAB3NzaC1kc3MAAACBAIzJrAXCuK15C+mq3TkdFFJUJiuY9rMo6L6LoU+naCEKJNKfRDXXAXDcRC2TJK5JqnWHuexfOusYZS/kpRU4JO1S4VGzq446r5QM19c7xH3TkE2A2W2Z9AA/7G+UHzqyHWQ6gDRIsqqsF6MlJUtOO7x3XtNUVYrtIzvUeqTbXrbJAAAAFQCsjTDCWxn2PU5WobBN/xYTxS9vdwAAAIBcM2X2tlkwnmpNcm3a1Cf4addU395AfJfhOwdqacHSCdiaNSlx7kVkd8T1Hk+gvF0KzP4KbjqiGWsGEiaYdlU4Ujrei+VplG8moa4GcCA/wUzpAioeULCP+0+870/+NwFUt7XKhYk9llUrh56LWev5c5YC3aNQ0GzElBxjUj8v4gAAAIBpUWTTkmdeL7ploxSCR56Js0pMFJiGvKP6tMkc3UL5Vwl5RDqJt+eFd31SDVJVVEK3vX06wujOlDbHwdIfpE48y2dN7nRn5bK3ccg1yo7Cq7Vtj4TlODYTkPYxXaR2e8dqW9bg8anXvaCI7AylRwPYNnQIgcjPeC4qJsRuMq4Mag== root@master\nssh-dss AAAAB3NzaC1kc3MAAACBAMxF+Q5Kg1DluBqo0vZKPlE0uB2+1cDTn/f2xN0ug5mYa3WDpC36p8P2iQ4IrZEp7BqFEiQSstbZd+Im4qpaBRlHnWZhym5oOqY2a4JVsrAtyTObYFM/+/eEtQ/0Bl6UxeRKkWWPuZwbtYREEnbJ2VwLzvIJEBDVkZcccY58TO8LAAAAFQC41GJzzSEGbZLDCu2Fgzo3iml/ZQAAAIBpWqD1HHm5gTyp/6h+hCEDMP1cOOl11e+f4ZO+vhpYm+AXqpEbmMr2UTSBlc93PdJRxiIAIKidWmcLaaSuLDYWoeDDcFGCclz9bCoXZmeOVoAe096jyNFPZGorb7mqnif3oRI5hkqsmph2AX/9n90taaLUF5VrgJVEAOPLkjZ+IAAAAIEAsc7MCMYn7phJIACMypSeeWkmjUisRxVEp6u6WWHQ3GsImNkjR7UmFVxnpYOikexsPsbhlXahTIas7SQiPNRsgxi2nDBwauEvkRHQID5LbjFiIp97xbrSg8T0H23MXlBbI/MycFcyuxBIUOL5zSrz8CcUG6uQtLDMGAEVkCHORCU= root@slave1\nssh-dss AAAAB3NzaC1kc3MAAACBANwhno/+fLpWNOg1NOrBQ+qs7XWLZeu+Xxl/g5eJOD9+qaQKTWLOYfgyez38cpqjZ9r39tKRR5HQ7RVlM0tJicGgz+jCdtRoQKs6W5mc3SCmW+u+ILMxxTqdUHUKsNq4NauoVcSduq4ot8HKpi2GBGWE1MCNgCaSnH6TB8tvl49lAAAAFQCnfx5p+/KbSsrlSFo9BYuAhEuI7QAAAIA4lsxJjI3bn/FQsSjzcjIyRLiut432/i/QngE7Y9UwQGXKY9x8z7EksXDpdswo2M2cBSZsrelSnoiUYHjusSfMTptzdT8WUWCutCd7Kn1zU4fPJCM4gTNuECjHaWU/t7BVJXHGkB6eWErcHxnm6iILVLCFf9wm8oPMjRJmLLQGhQAAAIEAkA+YrcoTQfuZbS8ACHN3zkvg1/gAmx26owiZsMrSaV1rbrJ6WgWCX+Ux9CHIkKK4MZrJrXVQpoal5/PEPw0OCZepCHOGVLNcrhyhKNov1EzSC664Mb0l+9bHh+zXjv/X0yrMB1bY16eNMBCnx0YsJ5vuXZtZRg9ms6dEh5eA/LY= root@slave2\n```\n4.分发给另外两台子节点\n```bash\nroot@master:~/.ssh# scp ./authorized.keys root@slave1:/root/.ssh/\nroot@master:~/.ssh# scp ./authorized.keys root@slave2:/root/.ssh/\n```\n5.测试免密登录\n```bash\nssh master\nssh slave1\nssh slave2\n```\n### 四.安装jdk\n1.解压\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# tar -zxvf jdk-8u11-linux-x64.tar.gz\n```\n2.分发给其余子节点\n```bash\ncp -r /root/software/jdk/jdk1.8.0_11/ root@slave1:/root/software/jdk/\ncp -r /root/software/jdk/jdk1.8.0_11/ root@slave2:/root/software/jdk/\n```\n3.配置环境变量\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# gedit /root/.bashrc \n```\n配置如下\n```bash\n#JAVA_HOME\nexport JAVA_HOME=/root/software/jdk/jdk1.8.0_11\nexport PATH=$JAVA_HOME/bin:$PATH\n```\n分发给其他节点,也可以直接配置\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# scp -r /root/.bashrc root@slave1:/root/\nroot@master:~/software/jdk/jdk1.8.0_11# scp -r /root/.bashrc root@slave2:/root/\n```\n4.刷新环境变量\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# source /root/.bashrc \n```\n5.测试\n如下回显则表示成功\n```bash\nroot@master:~/software/jdk/jdk1.8.0_11# java -version\njava version \"1.8.0_11\"\nJava(TM) SE Runtime Environment (build 1.8.0_11-b12)\nJava HotSpot(TM) 64-Bit Server VM (build 25.11-b03, mixed mode)\n```\n### 五.安装hadoop\n1.解压\n```bash\nroot@master:~/software/hadoop# tar -zxvf hadoop-2.7.3.tar.gz\n```\n2.配置环境变量\n```bash\nroot@master:~/software/hadoop# gedit /root/.bashrc \n```\n配置如下\n```bash\n#HADOOP_HOME\nexport HADOOP_HOME=/root/software/hadoop/hadoop-2.7.3\nexport PATH=$HADOOP_HOME/bin:$HADOOP/sbin:$PATH\n```\n分发给子节点\n```bash\nroot@master:~/software/hadoop# scp -r /root/.bashrc root@slave1:/root/\nroot@master:~/software/hadoop# scp -r /root/.bashrc root@slave2:/root/\n```\n刷新环境变量\n```bash\nroot@master:~/software/hadoop# source /root/.bashrc \n```\n3.创建hadoopdata目录\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3# mkdir hadoopdata\n```\n4.配置hadoop-env.sh文件\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# cd etc/hadoop/\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit hadoop-env.sh \n```\n```bash\n找到\nexport JAVA_HOME=${JAVA_HOME}\n修改为\nexport JAVA_HOME=/root/software/jdk/jdk1.8.0_11\n```\n5.配置yarn-env.sh\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit yarn-env.sh \n```\n```bash\n找到\n#export JAVA_HOME=/home/y/libexec/jdk1.6.0/\n修改为\nexport JAVA_HOME=/root/software/jdk/jdk1.8.0_11\n```\n6.配置核心组件core-site.xml \n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit core-site.xml \n```\n```xml\n<configuration>\n<property>\n<name>fs.defaultFS</name>\n<value>hdfs://master:9000</value>\n</property>\n<property>\n<name>hadoop.tmp.dir</name>\n<value>/root/software/hadoop/hadoop-2.7.3/hadoopdata</value>\n</property>\n</configuration>\n```\n7.配置配置文件系统hdfs-site.xml\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit hdfs-site.xml \n```\n```xml\n<configuration>\n\t<property>\n\t\t<name>dfs.replication</name>\n\t\t<value>2</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.rpc-address</name>\n\t\t<value>master:50071</value>\n\t</property>\n</configuration>\n```\n8.配置文件系统yarn-site.xm\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit yarn-site.xml\n```\n```xml\n<configuration>\n<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n\t<property>\n                <name>yarn.resourcemanager.address</name>\n                <value>master:18040</value>\n        </property>\n\t<property>                <name>yarn.resourcemanager.scheduler.address</name>\n                <value>master:18030</value>\n        </property>\n\t<property>\n                <name>yarn.resourcemanager.resource-tracker.address</name>\n                <value>master:18025</value>\n        </property>\n\t<property>                <name>yarn.resourcemanager.admin.address</name>\n                <value>master:18141</value>\n        </property>\n\t<property>                <name>yarn.resourcemanager.webapp.address</name>\n                <value>master:18088</value>\n        </property>\n\n</configuration>\n```\n9.配置计算框架mapred-site.xml\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# cp mapred-site.xml.template mapred-site.xml\n\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit mapred-site.xml\n```\n```xml\n<configuration>\n<property>\n<name>mapreduce.framework.name</name>\n<value>yarn</value>\n</property>\n</configuration>\n```\n10.配置slaves文件\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# gedit slaves \n```\n```bash\nmaster\nslave1\nslave2\n```\n11.分发给子节点\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# scp -r /root/software/hadoop/hadoop-2.7.3/ root@slave2:/root/software/hadoop/\n```\n12.格式化namanode\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/etc/hadoop# hdfs namenode -format\n```\n13.启动hadoop\n```bash\n进入sbin目录下执行\n\nroot@master:~/software/hadoop/hadoop-2.7.3/sbin# ./start-all.sh \n\n执行命令后，提示出入yes/no时，输入yes。\n```\n14.测试\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/sbin# jps\n```\n有以下进程表示搭建成功!\n```bash\nroot@master:~/software/hadoop/hadoop-2.7.3/sbin# jps\n4848 SecondaryNameNode\n4999 ResourceManager\n4489 NameNode\n4650 DataNode\n5423 Jps\n5135 NodeManager\n```\n15.web端查看\n```bash\n在Master上启动Firefox浏览器，在浏览器地址栏中输入输入http://master:50070/,有如下回显表示成功\n```\n\n![1](QQ20220301-220428.png)\n```bash\n在Master上启动Firefox浏览器，在浏览器地址栏中输入输入http://master:18088/，检查 Yarn是否正常，页面如下图所示。\n```\n![2](QQ20220301-220440.png)\n### 六.flume安装与配置\n\n1.解压\n```bash\ntar -zxvf apache-flume-1.7.0-bin.tar.gz \n```\n2.配置环境变量\n```bash\n#FLUME_HOME\nexport FLUME_HOME=/root/software/flume-1.7.0\nexport PATH=$FLUME_HOME/bin:$PATH\n```\n3.复制配置文件\n```bash\ncp flume-env.sh.template flume-env.sh\n```\n修改\n```bash\n# export JAVA_HOME=/usr/lib/jvm/java-6-sun\nexport JAVA_HOME=/root/software/jdk1.8.0_11\n```\n4.配置配置文件\n```bash\nsource: 数据的入口,规定了数据收集的入口规范\nchannel: 管道,存储数据用的\nskin: 数据的出口,规定了数据收集的出口规范\nagent: 一个任务,包含了source,channel,skin\n\n```\n\n```bash\ncp flume-conf.properties.template flume-conf.properties\n```\n修改为\n```bash\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n5.启动\n```bash\n./bin/flume-ng agent --conf conf --conf-file conf/flume-conf.properties --name a1 -Dflume.root.logger=INFO,console\n```\n6.nc测试\n```bash\nnc localhost 44444\n```\n7.案例一\n监听文件内容变动，将新增加的内容输出到控制台。\n新建配置文件 exec-memory-logger.properties,其内容如下：\n```bash\n#指定agent的sources,sinks,channels\na1.sources = s1  \na1.sinks = k1  \na1.channels = c1  \n   \n#配置sources属性\na1.sources.s1.type = exec\na1.sources.s1.command = tail -F /tmp/log.txt\na1.sources.s1.bash = /bin/bash -c\n\n#将sources与channels进行绑定\na1.sources.s1.channels = c1\n   \n#配置sink \na1.sinks.k1.type = logger\n\n#将sinks与channels进行绑定  \na1.sinks.k1.channel = c1  \n   \n#配置channel类型\na1.channels.c1.type = memory\n```\n8.案例二\n监听指定端口,将这个向这个端口写入的数据输出到控制台\n```bash\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = 192.168.32.130\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transctionCapacity = 100\n\n#Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\n9.案例三\n监听指定目录，将目录下新增加的文件存储到 HDFS。\n新建配置文件spooling-memory-hdfs.properties\n```bash\n#指定agent的sources,sinks,channels\na1.sources = s1  \na1.sinks = k1  \na1.channels = c1  \n   \n#配置sources属性\na1.sources.s1.type =spooldir  \na1.sources.s1.spoolDir =/tmp/logs\na1.sources.s1.basenameHeader = true\na1.sources.s1.basenameHeaderKey = fileName \n#将sources与channels进行绑定  \na1.sources.s1.channels =c1 \n\n   \n#配置sink \na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/\na1.sinks.k1.hdfs.filePrefix = %{fileName}\n#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\na1.sinks.k1.hdfs.fileType = DataStream  \na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#将sinks与channels进行绑定  \na1.sinks.k1.channel = c1\n   \n#配置channel类型\na1.channels.c1.type = memory\n```\n\n10.案例四\n将本服务器收集到的数据发送到另外一台服务器。\n新建配置 netcat-memory-avro.properties，监听文件内容变化，然后将新的文件内容通过 avro sink 发送到 hadoop001 这台服务器的 8888 端口：\n```bash\n#指定agent的sources,sinks,channels\na1.sources = s1\na1.sinks = k1\na1.channels = c1\n\n#配置sources属性\na1.sources.s1.type = exec\na1.sources.s1.command = tail -F /tmp/log.txt\na1.sources.s1.bash = /bin/bash -c\na1.sources.s1.channels = c1\n\n#配置sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = hadoop001\na1.sinks.k1.port = 8888\na1.sinks.k1.batch-size = 1\na1.sinks.k1.channel = c1\n\n#配置channel类型\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n```\n配置日志聚合Flume\n使用 avro source 监听 hadoop001 服务器的 8888 端口，将获取到内容输出到控制台\n```bash\n#指定agent的sources,sinks,channels\na2.sources = s2\na2.sinks = k2\na2.channels = c2\n\n#配置sources属性\na2.sources.s2.type = avro\na2.sources.s2.bind = hadoop001\na2.sources.s2.port = 8888\n\n#将sources与channels进行绑定\na2.sources.s2.channels = c2\n\n#配置sink\na2.sinks.k2.type = logger\n\n#将sinks与channels进行绑定\na2.sinks.k2.channel = c2\n\n#配置channel类型\na2.channels.c2.type = memory\na2.channels.c2.capacity = 1000\na2.channels.c2.transactionCapacity = 100\n\n```\n这里建议先启动a2，原因是 avro.source 会先与端口进行绑定，这样 avro sink 连接时才不会报无法连接的异常。但是即使不按顺序启动也是没关系的，sink 会一直重试，直至建立好连接。\n\n### 七.Zookeeper安装配置\n1.解压并配置环境变量\n```bash\n#ZOOKEEPER_HOME\nexport ZOOKEEPER_HOME=/root/software/zookeeper-3.4.5-cdh5.6.0\nexport PATH=$ZOOKEEPER_HOME/bin:$PATH\n```\n\n2.新建一个目录用来存放数据\n```bash\nmkdir /root/software/zookeeper-3.4.5-cdh5.6.0/zk_data\n```\n3.编辑配置文件\n复制一份配置文件,并替换内容\n```bash\ncp zoo_sample.cfg zoo.cfg\n```\n```bash\ndataDir=/root/software/zookeeper-3.4.5-cdh5.6.0/zk_data\n```\n4.启动\n```bash\n ./zkServer.sh start\n```\n\n### 八.kafka安装配置与使用\n1.解压并配置环境变量\n```bash\n#KAFKA_HOME\nexport KAFKA_HOME=/root/software/kafka_2.11-2.0.0\nexport PATH=$KAFKA_HOME/bin:$PATH\n```\n2.创建日志文件夹\n```bash\nmkdir /root/software/kafka_2.11-2.0.0/kafka-logs\n```\n3.config文件夹中修改配置文件以下几项\n```bash\ngedit server.properties \n```\n```bash\nlog.dirs=/root/software/kafka_2.11-2.0.0/kafka-logs\n\nlisteners=PLAINTEXT://localhost:9092\n```\n4.启动kafka\n启动kafka之前要先启动zookeeper\n```bash\n kafka-server-start.sh ./config/server.properties\n```\n5.创建topic主题\n```bash\nkafka-topics.sh --zookeeper localhost: 2181/kafka --create --topic topic-demo --replication-factor 1 --partitions 1\n```\n6.查看\n```bash\n kafka-topics.sh --list --zookeeper localhost:2181\n```\n7.生产消息\n```bash\n kafka-console-producer.sh --broker-list localhost:9092 --topic topic-demo\n```\n8.消费消息\n```bash\nkafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo\n--beginning 可选参数,代表从头消费\n```\n9.查看所有topic的信息\n```bash\nkafka-topics.sh --zookeeper localhost: 2181 --describe \n--topic topic-demo 可选参数,表示指定topic\n```\n10.单节点多broker\n\n* 修改配合文件中的id,端口,日志文件夹\n* 启动\n```bash\nkafka-server-start.sh --deamon ./config/server.properties &\nkafka-server-start.sh --deamon ./config/server2.properties &\nkafka-server-start.sh --deamon ./config/server3.properties &\n```\n* 多副本\n```bash\nkafka-topics.sh --zookeeper localhost: 2181/kafka --create --topic my-topic-demo --replication-factor 3 --partitions 1\n```\n\n### 九.安装scala\n1.解压并配置环境变量\n```bash\nroot@ubuntu:~/software/scala-2.11.0# tar -zxvf scala-2.11.0.tgz \nroot@ubuntu:~/software/scala-2.11.0# gedit /root/.bashrc \n```\n```bash\n#SCALA_HOME\nexport SCALA_HOME=/root/software/scala-2.11.0\nexport PATH=$SCALA_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software/scala-2.11.0# source /root/.bashrc \n```\n3.测试\n```bash\nroot@ubuntu:~/software/scala-2.11.0# scala\nWelcome to Scala version 2.11.0 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_11).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> \n```\n\n### 十.安装maven\n1.解压并配置环境变量\n```bash\nroot@ubuntu:~/software# tar -zxvf apache-maven-3.8.5-bin.tar.gz\nroot@ubuntu:~/software# mv apache-maven-3.8.5 maven-3.8.5\nroot@ubuntu:~/software# gedit /root/.bashrc \n```\n```bash\n#MAVEN_HOME\nexport MAVEN_HOME=/root/software/maven-3.8.5\nexport PATH=$MAVEN_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software/scala-2.11.0# source /root/.bashrc \n```\n3.测试\n```bash\nroot@ubuntu:~/software/maven-3.8.5# mvn -v\nApache Maven 3.8.5 (3599d3414f046de2324203b78ddcf9b5e4388aa0)\nMaven home: /root/software/maven-3.8.5\nJava version: 1.8.0_11, vendor: Oracle Corporation, runtime: /root/software/jdk1.8.0_11/jre\nDefault locale: en_US, platform encoding: UTF-8\nOS name: \"linux\", version: \"5.4.0-100-generic\", arch: \"amd64\", family: \"unix\"\n```\n4.修改jar包存放位置\n```bash\nroot@ubuntu:~/software/maven-3.8.5# mkdir maven-repos\nroot@ubuntu:~/software/maven-3.8.5# gedit conf/settings.xml \n```\n添加一行\n```xml\n<localRepository>/root/software/maven-3.8.5/maven-repos</localRepository>\n```\n\n### 十一.Hbase安装\n1.解压并配置环境变量\n```bash\nroot@master:~/software# tar -zxvf hbase-1.2.0-bin.tar.gz \nroot@ubuntu:~/software# gedit /root/.bashrc \n```\n```bash\n#HBASE_HOME\nexport HBASE_HOME=/root/software/hbase-1.2.0\nexport PATH=$HBASE_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software# source /root/.bashrc \n```\n3.编辑配置文件\n```bash\nroot@master:~/software/hbase-1.2.0/conf# gedit hbase-env.sh\n```\n修改\n```bash\n#export JAVA_HOME=/usr/java/jdk1.6.0/\nexport JAVA_HOME=/root/software/jdk1.8.0_11\n```\n修改\n```bash\n# export HBASE_MANAGES_ZK=true\nexport HBASE_MANAGES_ZK=false\n```\n添加\n```bash\nroot@master:~/software/hbase-1.2.0/conf# gedit hbase-site.xml \n```\n```xml\n<configuration>\n        <property>\n                <name>hbase.cluster.distributed</name>\n        </property>\n        <property>\n                <name>hbase.rootdir</name>\n                <value>hdfs://master:9000/hbase</value>\n        </property>\n        <property>\n                <name>hbase.zookeeper.quorum</name>\n                <value>master</value>\n        </property>\n        <property>\n                <name>hbase.master.info.port</name>\n                <value>60010</value>\n        </property>\n</configuration>\n```\n修改\n```bash\nroot@master:~/software/hbase-1.2.0/conf# gedit regionservers \n```\n为\n```bash\nmaster\n```\n4.启动hbase\n首先要先启动zeekeeper\n```bash\nroot@master:~/software# zkServer.sh start\nJMX enabled by default\nUsing config: /root/software/zookeeper-3.4.5-cdh5.6.0/bin/../conf/zoo.cfg\nStarting zookeeper ... STARTED\n```\n```bash\nroot@master:~/software# start-hbase.sh \nstarting master, logging to /root/software/hbase-1.2.0/logs/hbase-root-master-master.out\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\nmaster: starting regionserver, logging to /root/software/hbase-1.2.0/bin/../logs/hbase-root-regionserver-master.out\nmaster: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nmaster: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\n```\n```bash\nroot@master:~/software/hbase-1.2.0/bin# jps\n2992 SecondaryNameNode\n4514 QuorumPeerMain\n3282 NodeManager\n6196 HRegionServer\n3143 ResourceManager\n6026 HMaster\n6330 Jps\n2636 NameNode\n2796 DataNode\n```\n访问\n```url\nhttp://master:60010\n```\n\n6.测试\n```bash\nroot@master:~/software/hbase-1.2.0/bin# hbase shell\n\nhbase(main):001:0> version\n1.2.0, r25b281972df2f5b15c426c8963cbf77dd853a5ad, Thu Feb 18 23:01:49 CST 2016\n```\n\n### 十二.Spark安装\n1.解压并配置环境变量\n```bash\nroot@master:~/software# tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz \nroot@ubuntu:~/software# gedit /root/.bashrc \n```\n```bash\n#SPARK_HOME\nexport SPARK_HOME=/root/software/spark-2.1.1-bin-hadoop2.7\nexport PATH=$SPARK_HOME/bin:$PATH\n```\n2.刷新环境变量\n```bash\nroot@ubuntu:~/software# source /root/.bashrc \n```\n3.测试\n```bash\nroot@master:~/software/spark-2.1.1-bin-hadoop2.7# spark-shell --version\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n      /_/\n                        \nUsing Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_11\nBranch \nCompiled by user jenkins on 2017-04-25T23:51:10Z\nRevision \nUrl \nType --help for more information.\n\n```\n### 十三.flume对接kafka\n一般flume采集的方式有两种\n1.Exec类型的Source\n可以将命令产生的输出作为源，如：\n```bashh\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/log.txt //此处输入命令\n```\n2.Spooling Directory类型的 Source\n将指定的文件加入到“自动搜集 ”目录中。flume会持续监听这个目录，把文件当做source来处理。注意：一旦文件被放到“自动收集”目录中后，便不能修改，如果修改，flume会报错。此外，也不能有重名的文件，如果有，flume也会报错。\n```bash\na1.sources.r1.type = spooldir\na1.sources.r1.spoolDir = /home/work/data\n```\n#### 1.flume采集某日志文件到kafka自定义topic\n1.1 创建flume配置文件 flume-kafka-file.conf\n```bash\n# 定义这个agent中各组件的名字\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n \n# 描述和配置source组件：r1\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/log.txt\n \n# 描述和配置sink组件：k1\na1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k1.kafka.topic = topic-test\na1.sinks.k1.kafka.bootstrap.servers = localhost:9092\na1.sinks.k1.kafka.flumeBatchSize = 20\na1.sinks.k1.kafka.producer.acks = 1\na1.sinks.k1.kafka.producer.linger.ms = 1\na1.sinks.ki.kafka.producer.compression.type = snappy\n \n# 描述和配置channel组件，此处使用是内存缓存的方式\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n \n# 描述和配置source  channel   sink之间的连接关系\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n1.2 启动zookeeper和kafka\n```bash\n./zkServer.sh start\nJMX enabled by default\nUsing config: /root/software/zookeeper-3.4.5-cdh5.6.0/bin/../conf/zoo.cfg\nStarting zookeeper ... already running as process 5452.\n```\n```bash\nkafka-server-start.sh ./config/server.properties\n```\n1.3 创建topic\n\ntopic:指定topic name\n\npartitions:指定分区数，这个参数需要根据broker数和数据量决定，正常情况下，每个broker上两个partition最好\n\nreplication-factor:副本数，建议设置为2\n```bash\nkafka-topics.sh --zookeeper localhost: 2181/kafka --create --topic topic-test2 --replication-factor 1 --partitions 1\n```\n1.4 启动kafka去消费topic\n```bash\nkafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-test2\n--from-beginning 可选参数,代表从头消费\n```\n1.5 启动flume\n```bash\n./bin/flume-ng agent -n a1 -c ./conf/ -f ./conf/flume-kafka-port.conf -Dflume.root.logger=INFO,console\n```\n1.6 向日志文件/tmp/log.txt写入一些数据\n```bash\necho '123' >> /tmp/log.txt\n```\n就可以在消费者窗口看到输出\n\n\n\n#### 2.flume采集端口数据到kafka自定义topic\n\n2.1 新建配置文件 flume-kafka-port.conf\n```bash\n#指定agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n \n# 描述和配置source组件：r1\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 55555 \n# 描述和配置sink组件：k1\na1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k1.kafka.topic = topic-test2\na1.sinks.k1.kafka.bootstrap.servers = localhost:9092\na1.sinks.k1.kafka.flumeBatchSize = 20\na1.sinks.k1.kafka.producer.acks = 1\na1.sinks.k1.kafka.producer.linger.ms = 1\na1.sinks.ki.kafka.producer.compression.type = snappy\n \n# 描述和配置channel组件，此处使用是内存缓存的方式\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n \n# 描述和配置source  channel   sink之间的连接关系\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\n2.2所有操作与上文一致\n略\n\n2.3 向端口发送数据\n```bash\nroot@ubuntu:~# nc localhost 55555\nOK\nls\nOK\nls\nOK\nls\nOK\nls\nOK\nls\n```\n\n在消费者端口可以看到\n```bash\nls\nls\nls\nls\nls\n```","tags":["hadoop","kafka","flume"],"categories":["环境搭建"]}]